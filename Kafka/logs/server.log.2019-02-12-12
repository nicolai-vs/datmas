[2019-02-12 13:07:38,935] INFO [GroupMetadataManager brokerId=0] Removed 0 expired offsets in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-02-12 13:10:49,190] INFO Topic creation Map(connect-test-0 -> ArrayBuffer(0)) (kafka.zk.AdminZkClient)
[2019-02-12 13:10:49,201] INFO [KafkaApi-0] Auto creation of topic connect-test with 1 partitions and replication factor 1 is successful (kafka.server.KafkaApis)
[2019-02-12 13:10:49,235] INFO [ReplicaFetcherManager on broker 0] Removed fetcher for partitions Set(connect-test-0) (kafka.server.ReplicaFetcherManager)
[2019-02-12 13:10:49,248] INFO [Log partition=connect-test-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-12 13:10:49,248] INFO Topic creation Map(__consumer_offsets-22 -> ArrayBuffer(0), __consumer_offsets-30 -> ArrayBuffer(0), __consumer_offsets-8 -> ArrayBuffer(0), __consumer_offsets-21 -> ArrayBuffer(0), __consumer_offsets-4 -> ArrayBuffer(0), __consumer_offsets-27 -> ArrayBuffer(0), __consumer_offsets-7 -> ArrayBuffer(0), __consumer_offsets-9 -> ArrayBuffer(0), __consumer_offsets-46 -> ArrayBuffer(0), __consumer_offsets-25 -> ArrayBuffer(0), __consumer_offsets-35 -> ArrayBuffer(0), __consumer_offsets-41 -> ArrayBuffer(0), __consumer_offsets-33 -> ArrayBuffer(0), __consumer_offsets-23 -> ArrayBuffer(0), __consumer_offsets-49 -> ArrayBuffer(0), __consumer_offsets-47 -> ArrayBuffer(0), __consumer_offsets-16 -> ArrayBuffer(0), __consumer_offsets-28 -> ArrayBuffer(0), __consumer_offsets-31 -> ArrayBuffer(0), __consumer_offsets-36 -> ArrayBuffer(0), __consumer_offsets-42 -> ArrayBuffer(0), __consumer_offsets-3 -> ArrayBuffer(0), __consumer_offsets-18 -> ArrayBuffer(0), __consumer_offsets-37 -> ArrayBuffer(0), __consumer_offsets-15 -> ArrayBuffer(0), __consumer_offsets-24 -> ArrayBuffer(0), __consumer_offsets-38 -> ArrayBuffer(0), __consumer_offsets-17 -> ArrayBuffer(0), __consumer_offsets-48 -> ArrayBuffer(0), __consumer_offsets-19 -> ArrayBuffer(0), __consumer_offsets-11 -> ArrayBuffer(0), __consumer_offsets-13 -> ArrayBuffer(0), __consumer_offsets-2 -> ArrayBuffer(0), __consumer_offsets-43 -> ArrayBuffer(0), __consumer_offsets-6 -> ArrayBuffer(0), __consumer_offsets-14 -> ArrayBuffer(0), __consumer_offsets-20 -> ArrayBuffer(0), __consumer_offsets-0 -> ArrayBuffer(0), __consumer_offsets-44 -> ArrayBuffer(0), __consumer_offsets-39 -> ArrayBuffer(0), __consumer_offsets-12 -> ArrayBuffer(0), __consumer_offsets-45 -> ArrayBuffer(0), __consumer_offsets-1 -> ArrayBuffer(0), __consumer_offsets-5 -> ArrayBuffer(0), __consumer_offsets-26 -> ArrayBuffer(0), __consumer_offsets-29 -> ArrayBuffer(0), __consumer_offsets-34 -> ArrayBuffer(0), __consumer_offsets-10 -> ArrayBuffer(0), __consumer_offsets-32 -> ArrayBuffer(0), __consumer_offsets-40 -> ArrayBuffer(0)) (kafka.zk.AdminZkClient)
[2019-02-12 13:10:49,250] INFO [Log partition=connect-test-0, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 7 ms (kafka.log.Log)
[2019-02-12 13:10:49,252] INFO Created log for partition connect-test-0 in /tmp/kafka-logs with properties {compression.type -> producer, message.format.version -> 2.1-IV2, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 1073741824, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-02-12 13:10:49,254] INFO [Partition connect-test-0 broker=0] No checkpointed highwatermark is found for partition connect-test-0 (kafka.cluster.Partition)
[2019-02-12 13:10:49,255] INFO Replica loaded for partition connect-test-0 with initial high watermark 0 (kafka.cluster.Replica)
[2019-02-12 13:10:49,256] INFO [Partition connect-test-0 broker=0] connect-test-0 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-02-12 13:10:49,264] INFO [KafkaApi-0] Auto creation of topic __consumer_offsets with 50 partitions and replication factor 1 is successful (kafka.server.KafkaApis)
[2019-02-12 13:10:49,578] INFO [ReplicaFetcherManager on broker 0] Removed fetcher for partitions Set(__consumer_offsets-22, __consumer_offsets-30, __consumer_offsets-8, __consumer_offsets-21, __consumer_offsets-4, __consumer_offsets-27, __consumer_offsets-7, __consumer_offsets-9, __consumer_offsets-46, __consumer_offsets-25, __consumer_offsets-35, __consumer_offsets-41, __consumer_offsets-33, __consumer_offsets-23, __consumer_offsets-49, __consumer_offsets-47, __consumer_offsets-16, __consumer_offsets-28, __consumer_offsets-31, __consumer_offsets-36, __consumer_offsets-42, __consumer_offsets-3, __consumer_offsets-18, __consumer_offsets-37, __consumer_offsets-15, __consumer_offsets-24, __consumer_offsets-38, __consumer_offsets-17, __consumer_offsets-48, __consumer_offsets-19, __consumer_offsets-11, __consumer_offsets-13, __consumer_offsets-2, __consumer_offsets-43, __consumer_offsets-6, __consumer_offsets-14, __consumer_offsets-20, __consumer_offsets-0, __consumer_offsets-44, __consumer_offsets-39, __consumer_offsets-12, __consumer_offsets-45, __consumer_offsets-1, __consumer_offsets-5, __consumer_offsets-26, __consumer_offsets-29, __consumer_offsets-34, __consumer_offsets-10, __consumer_offsets-32, __consumer_offsets-40) (kafka.server.ReplicaFetcherManager)
[2019-02-12 13:10:49,603] INFO [Log partition=__consumer_offsets-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-12 13:10:49,605] INFO [Log partition=__consumer_offsets-0, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 21 ms (kafka.log.Log)
[2019-02-12 13:10:49,606] INFO Created log for partition __consumer_offsets-0 in /tmp/kafka-logs with properties {compression.type -> producer, message.format.version -> 2.1-IV2, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-02-12 13:10:49,608] INFO [Partition __consumer_offsets-0 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-0 (kafka.cluster.Partition)
[2019-02-12 13:10:49,609] INFO Replica loaded for partition __consumer_offsets-0 with initial high watermark 0 (kafka.cluster.Replica)
[2019-02-12 13:10:49,610] INFO [Partition __consumer_offsets-0 broker=0] __consumer_offsets-0 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-02-12 13:10:49,625] INFO [Log partition=__consumer_offsets-29, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-12 13:10:49,627] INFO [Log partition=__consumer_offsets-29, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 7 ms (kafka.log.Log)
[2019-02-12 13:10:49,629] INFO Created log for partition __consumer_offsets-29 in /tmp/kafka-logs with properties {compression.type -> producer, message.format.version -> 2.1-IV2, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-02-12 13:10:49,631] INFO [Partition __consumer_offsets-29 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-29 (kafka.cluster.Partition)
[2019-02-12 13:10:49,632] INFO Replica loaded for partition __consumer_offsets-29 with initial high watermark 0 (kafka.cluster.Replica)
[2019-02-12 13:10:49,633] INFO [Partition __consumer_offsets-29 broker=0] __consumer_offsets-29 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-02-12 13:10:49,648] INFO [Log partition=__consumer_offsets-48, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-12 13:10:49,650] INFO [Log partition=__consumer_offsets-48, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 8 ms (kafka.log.Log)
[2019-02-12 13:10:49,651] INFO Created log for partition __consumer_offsets-48 in /tmp/kafka-logs with properties {compression.type -> producer, message.format.version -> 2.1-IV2, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-02-12 13:10:49,652] INFO [Partition __consumer_offsets-48 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-48 (kafka.cluster.Partition)
[2019-02-12 13:10:49,653] INFO Replica loaded for partition __consumer_offsets-48 with initial high watermark 0 (kafka.cluster.Replica)
[2019-02-12 13:10:49,654] INFO [Partition __consumer_offsets-48 broker=0] __consumer_offsets-48 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-02-12 13:10:49,678] INFO [Log partition=__consumer_offsets-10, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-12 13:10:49,680] INFO [Log partition=__consumer_offsets-10, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 6 ms (kafka.log.Log)
[2019-02-12 13:10:49,681] INFO Created log for partition __consumer_offsets-10 in /tmp/kafka-logs with properties {compression.type -> producer, message.format.version -> 2.1-IV2, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-02-12 13:10:49,682] INFO [Partition __consumer_offsets-10 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-10 (kafka.cluster.Partition)
[2019-02-12 13:10:49,683] INFO Replica loaded for partition __consumer_offsets-10 with initial high watermark 0 (kafka.cluster.Replica)
[2019-02-12 13:10:49,684] INFO [Partition __consumer_offsets-10 broker=0] __consumer_offsets-10 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-02-12 13:10:49,701] INFO [Log partition=__consumer_offsets-45, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-12 13:10:49,703] INFO [Log partition=__consumer_offsets-45, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 5 ms (kafka.log.Log)
[2019-02-12 13:10:49,704] INFO Created log for partition __consumer_offsets-45 in /tmp/kafka-logs with properties {compression.type -> producer, message.format.version -> 2.1-IV2, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-02-12 13:10:49,706] INFO [Partition __consumer_offsets-45 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-45 (kafka.cluster.Partition)
[2019-02-12 13:10:49,706] INFO Replica loaded for partition __consumer_offsets-45 with initial high watermark 0 (kafka.cluster.Replica)
[2019-02-12 13:10:49,707] INFO [Partition __consumer_offsets-45 broker=0] __consumer_offsets-45 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-02-12 13:10:49,721] INFO [Log partition=__consumer_offsets-26, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-12 13:10:49,723] INFO [Log partition=__consumer_offsets-26, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 6 ms (kafka.log.Log)
[2019-02-12 13:10:49,724] INFO Created log for partition __consumer_offsets-26 in /tmp/kafka-logs with properties {compression.type -> producer, message.format.version -> 2.1-IV2, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-02-12 13:10:49,726] INFO [Partition __consumer_offsets-26 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-26 (kafka.cluster.Partition)
[2019-02-12 13:10:49,727] INFO Replica loaded for partition __consumer_offsets-26 with initial high watermark 0 (kafka.cluster.Replica)
[2019-02-12 13:10:49,728] INFO [Partition __consumer_offsets-26 broker=0] __consumer_offsets-26 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-02-12 13:10:49,752] INFO [Log partition=__consumer_offsets-7, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-12 13:10:49,754] INFO [Log partition=__consumer_offsets-7, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 6 ms (kafka.log.Log)
[2019-02-12 13:10:49,755] INFO Created log for partition __consumer_offsets-7 in /tmp/kafka-logs with properties {compression.type -> producer, message.format.version -> 2.1-IV2, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-02-12 13:10:49,757] INFO [Partition __consumer_offsets-7 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-7 (kafka.cluster.Partition)
[2019-02-12 13:10:49,757] INFO Replica loaded for partition __consumer_offsets-7 with initial high watermark 0 (kafka.cluster.Replica)
[2019-02-12 13:10:49,758] INFO [Partition __consumer_offsets-7 broker=0] __consumer_offsets-7 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-02-12 13:10:49,777] INFO [Log partition=__consumer_offsets-42, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-12 13:10:49,780] INFO [Log partition=__consumer_offsets-42, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 7 ms (kafka.log.Log)
[2019-02-12 13:10:49,781] INFO Created log for partition __consumer_offsets-42 in /tmp/kafka-logs with properties {compression.type -> producer, message.format.version -> 2.1-IV2, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-02-12 13:10:49,782] INFO [Partition __consumer_offsets-42 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-42 (kafka.cluster.Partition)
[2019-02-12 13:10:49,783] INFO Replica loaded for partition __consumer_offsets-42 with initial high watermark 0 (kafka.cluster.Replica)
[2019-02-12 13:10:49,784] INFO [Partition __consumer_offsets-42 broker=0] __consumer_offsets-42 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-02-12 13:10:49,799] INFO [Log partition=__consumer_offsets-4, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-12 13:10:49,800] INFO [Log partition=__consumer_offsets-4, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 6 ms (kafka.log.Log)
[2019-02-12 13:10:49,802] INFO Created log for partition __consumer_offsets-4 in /tmp/kafka-logs with properties {compression.type -> producer, message.format.version -> 2.1-IV2, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-02-12 13:10:49,803] INFO [Partition __consumer_offsets-4 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-4 (kafka.cluster.Partition)
[2019-02-12 13:10:49,804] INFO Replica loaded for partition __consumer_offsets-4 with initial high watermark 0 (kafka.cluster.Replica)
[2019-02-12 13:10:49,805] INFO [Partition __consumer_offsets-4 broker=0] __consumer_offsets-4 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-02-12 13:10:49,824] INFO [Log partition=__consumer_offsets-23, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-12 13:10:49,826] INFO [Log partition=__consumer_offsets-23, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 6 ms (kafka.log.Log)
[2019-02-12 13:10:49,828] INFO Created log for partition __consumer_offsets-23 in /tmp/kafka-logs with properties {compression.type -> producer, message.format.version -> 2.1-IV2, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-02-12 13:10:49,829] INFO [Partition __consumer_offsets-23 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-23 (kafka.cluster.Partition)
[2019-02-12 13:10:49,830] INFO Replica loaded for partition __consumer_offsets-23 with initial high watermark 0 (kafka.cluster.Replica)
[2019-02-12 13:10:49,831] INFO [Partition __consumer_offsets-23 broker=0] __consumer_offsets-23 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-02-12 13:10:49,856] INFO [Log partition=__consumer_offsets-1, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-12 13:10:49,858] INFO [Log partition=__consumer_offsets-1, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 6 ms (kafka.log.Log)
[2019-02-12 13:10:49,859] INFO Created log for partition __consumer_offsets-1 in /tmp/kafka-logs with properties {compression.type -> producer, message.format.version -> 2.1-IV2, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-02-12 13:10:49,862] INFO [Partition __consumer_offsets-1 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-1 (kafka.cluster.Partition)
[2019-02-12 13:10:49,862] INFO Replica loaded for partition __consumer_offsets-1 with initial high watermark 0 (kafka.cluster.Replica)
[2019-02-12 13:10:49,864] INFO [Partition __consumer_offsets-1 broker=0] __consumer_offsets-1 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-02-12 13:10:49,877] INFO [Log partition=__consumer_offsets-20, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-12 13:10:49,878] INFO [Log partition=__consumer_offsets-20, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 5 ms (kafka.log.Log)
[2019-02-12 13:10:49,880] INFO Created log for partition __consumer_offsets-20 in /tmp/kafka-logs with properties {compression.type -> producer, message.format.version -> 2.1-IV2, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-02-12 13:10:49,881] INFO [Partition __consumer_offsets-20 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-20 (kafka.cluster.Partition)
[2019-02-12 13:10:49,882] INFO Replica loaded for partition __consumer_offsets-20 with initial high watermark 0 (kafka.cluster.Replica)
[2019-02-12 13:10:49,883] INFO [Partition __consumer_offsets-20 broker=0] __consumer_offsets-20 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-02-12 13:10:49,896] INFO [Log partition=__consumer_offsets-39, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-12 13:10:49,898] INFO [Log partition=__consumer_offsets-39, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 6 ms (kafka.log.Log)
[2019-02-12 13:10:49,899] INFO Created log for partition __consumer_offsets-39 in /tmp/kafka-logs with properties {compression.type -> producer, message.format.version -> 2.1-IV2, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-02-12 13:10:49,900] INFO [Partition __consumer_offsets-39 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-39 (kafka.cluster.Partition)
[2019-02-12 13:10:49,901] INFO Replica loaded for partition __consumer_offsets-39 with initial high watermark 0 (kafka.cluster.Replica)
[2019-02-12 13:10:49,902] INFO [Partition __consumer_offsets-39 broker=0] __consumer_offsets-39 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-02-12 13:10:49,925] INFO [Log partition=__consumer_offsets-17, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-12 13:10:49,927] INFO [Log partition=__consumer_offsets-17, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 6 ms (kafka.log.Log)
[2019-02-12 13:10:49,928] INFO Created log for partition __consumer_offsets-17 in /tmp/kafka-logs with properties {compression.type -> producer, message.format.version -> 2.1-IV2, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-02-12 13:10:49,930] INFO [Partition __consumer_offsets-17 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-17 (kafka.cluster.Partition)
[2019-02-12 13:10:49,931] INFO Replica loaded for partition __consumer_offsets-17 with initial high watermark 0 (kafka.cluster.Replica)
[2019-02-12 13:10:49,932] INFO [Partition __consumer_offsets-17 broker=0] __consumer_offsets-17 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-02-12 13:10:49,951] INFO [Log partition=__consumer_offsets-36, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-12 13:10:49,952] INFO [Log partition=__consumer_offsets-36, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 5 ms (kafka.log.Log)
[2019-02-12 13:10:49,953] INFO Created log for partition __consumer_offsets-36 in /tmp/kafka-logs with properties {compression.type -> producer, message.format.version -> 2.1-IV2, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-02-12 13:10:49,954] INFO [Partition __consumer_offsets-36 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-36 (kafka.cluster.Partition)
[2019-02-12 13:10:49,955] INFO Replica loaded for partition __consumer_offsets-36 with initial high watermark 0 (kafka.cluster.Replica)
[2019-02-12 13:10:49,956] INFO [Partition __consumer_offsets-36 broker=0] __consumer_offsets-36 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-02-12 13:10:49,968] INFO [Log partition=__consumer_offsets-14, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-12 13:10:49,970] INFO [Log partition=__consumer_offsets-14, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 6 ms (kafka.log.Log)
[2019-02-12 13:10:49,971] INFO Created log for partition __consumer_offsets-14 in /tmp/kafka-logs with properties {compression.type -> producer, message.format.version -> 2.1-IV2, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-02-12 13:10:49,972] INFO [Partition __consumer_offsets-14 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-14 (kafka.cluster.Partition)
[2019-02-12 13:10:49,973] INFO Replica loaded for partition __consumer_offsets-14 with initial high watermark 0 (kafka.cluster.Replica)
[2019-02-12 13:10:49,974] INFO [Partition __consumer_offsets-14 broker=0] __consumer_offsets-14 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-02-12 13:10:49,992] INFO [Log partition=__consumer_offsets-33, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-12 13:10:49,993] INFO [Log partition=__consumer_offsets-33, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 5 ms (kafka.log.Log)
[2019-02-12 13:10:49,995] INFO Created log for partition __consumer_offsets-33 in /tmp/kafka-logs with properties {compression.type -> producer, message.format.version -> 2.1-IV2, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-02-12 13:10:49,996] INFO [Partition __consumer_offsets-33 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-33 (kafka.cluster.Partition)
[2019-02-12 13:10:49,997] INFO Replica loaded for partition __consumer_offsets-33 with initial high watermark 0 (kafka.cluster.Replica)
[2019-02-12 13:10:49,998] INFO [Partition __consumer_offsets-33 broker=0] __consumer_offsets-33 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-02-12 13:10:50,023] INFO [Log partition=__consumer_offsets-49, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-12 13:10:50,025] INFO [Log partition=__consumer_offsets-49, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 6 ms (kafka.log.Log)
[2019-02-12 13:10:50,026] INFO Created log for partition __consumer_offsets-49 in /tmp/kafka-logs with properties {compression.type -> producer, message.format.version -> 2.1-IV2, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-02-12 13:10:50,027] INFO [Partition __consumer_offsets-49 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-49 (kafka.cluster.Partition)
[2019-02-12 13:10:50,028] INFO Replica loaded for partition __consumer_offsets-49 with initial high watermark 0 (kafka.cluster.Replica)
[2019-02-12 13:10:50,029] INFO [Partition __consumer_offsets-49 broker=0] __consumer_offsets-49 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-02-12 13:10:50,044] INFO [Log partition=__consumer_offsets-11, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-12 13:10:50,046] INFO [Log partition=__consumer_offsets-11, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 8 ms (kafka.log.Log)
[2019-02-12 13:10:50,047] INFO Created log for partition __consumer_offsets-11 in /tmp/kafka-logs with properties {compression.type -> producer, message.format.version -> 2.1-IV2, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-02-12 13:10:50,048] INFO [Partition __consumer_offsets-11 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-11 (kafka.cluster.Partition)
[2019-02-12 13:10:50,050] INFO Replica loaded for partition __consumer_offsets-11 with initial high watermark 0 (kafka.cluster.Replica)
[2019-02-12 13:10:50,051] INFO [Partition __consumer_offsets-11 broker=0] __consumer_offsets-11 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-02-12 13:10:50,064] INFO [Log partition=__consumer_offsets-30, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-12 13:10:50,066] INFO [Log partition=__consumer_offsets-30, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 6 ms (kafka.log.Log)
[2019-02-12 13:10:50,067] INFO Created log for partition __consumer_offsets-30 in /tmp/kafka-logs with properties {compression.type -> producer, message.format.version -> 2.1-IV2, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-02-12 13:10:50,068] INFO [Partition __consumer_offsets-30 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-30 (kafka.cluster.Partition)
[2019-02-12 13:10:50,069] INFO Replica loaded for partition __consumer_offsets-30 with initial high watermark 0 (kafka.cluster.Replica)
[2019-02-12 13:10:50,070] INFO [Partition __consumer_offsets-30 broker=0] __consumer_offsets-30 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-02-12 13:10:50,094] INFO [Log partition=__consumer_offsets-46, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-12 13:10:50,096] INFO [Log partition=__consumer_offsets-46, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 5 ms (kafka.log.Log)
[2019-02-12 13:10:50,098] INFO Created log for partition __consumer_offsets-46 in /tmp/kafka-logs with properties {compression.type -> producer, message.format.version -> 2.1-IV2, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-02-12 13:10:50,099] INFO [Partition __consumer_offsets-46 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-46 (kafka.cluster.Partition)
[2019-02-12 13:10:50,099] INFO Replica loaded for partition __consumer_offsets-46 with initial high watermark 0 (kafka.cluster.Replica)
[2019-02-12 13:10:50,100] INFO [Partition __consumer_offsets-46 broker=0] __consumer_offsets-46 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-02-12 13:10:50,119] INFO [Log partition=__consumer_offsets-27, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-12 13:10:50,121] INFO [Log partition=__consumer_offsets-27, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 5 ms (kafka.log.Log)
[2019-02-12 13:10:50,122] INFO Created log for partition __consumer_offsets-27 in /tmp/kafka-logs with properties {compression.type -> producer, message.format.version -> 2.1-IV2, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-02-12 13:10:50,123] INFO [Partition __consumer_offsets-27 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-27 (kafka.cluster.Partition)
[2019-02-12 13:10:50,124] INFO Replica loaded for partition __consumer_offsets-27 with initial high watermark 0 (kafka.cluster.Replica)
[2019-02-12 13:10:50,125] INFO [Partition __consumer_offsets-27 broker=0] __consumer_offsets-27 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-02-12 13:10:50,137] INFO [Log partition=__consumer_offsets-8, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-12 13:10:50,139] INFO [Log partition=__consumer_offsets-8, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 5 ms (kafka.log.Log)
[2019-02-12 13:10:50,141] INFO Created log for partition __consumer_offsets-8 in /tmp/kafka-logs with properties {compression.type -> producer, message.format.version -> 2.1-IV2, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-02-12 13:10:50,142] INFO [Partition __consumer_offsets-8 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-8 (kafka.cluster.Partition)
[2019-02-12 13:10:50,143] INFO Replica loaded for partition __consumer_offsets-8 with initial high watermark 0 (kafka.cluster.Replica)
[2019-02-12 13:10:50,144] INFO [Partition __consumer_offsets-8 broker=0] __consumer_offsets-8 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-02-12 13:10:50,167] INFO [Log partition=__consumer_offsets-24, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-12 13:10:50,169] INFO [Log partition=__consumer_offsets-24, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 8 ms (kafka.log.Log)
[2019-02-12 13:10:50,170] INFO Created log for partition __consumer_offsets-24 in /tmp/kafka-logs with properties {compression.type -> producer, message.format.version -> 2.1-IV2, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-02-12 13:10:50,172] INFO [Partition __consumer_offsets-24 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-24 (kafka.cluster.Partition)
[2019-02-12 13:10:50,172] INFO Replica loaded for partition __consumer_offsets-24 with initial high watermark 0 (kafka.cluster.Replica)
[2019-02-12 13:10:50,173] INFO [Partition __consumer_offsets-24 broker=0] __consumer_offsets-24 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-02-12 13:10:50,202] INFO [Log partition=__consumer_offsets-43, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-12 13:10:50,203] INFO [Log partition=__consumer_offsets-43, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 6 ms (kafka.log.Log)
[2019-02-12 13:10:50,205] INFO Created log for partition __consumer_offsets-43 in /tmp/kafka-logs with properties {compression.type -> producer, message.format.version -> 2.1-IV2, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-02-12 13:10:50,206] INFO [Partition __consumer_offsets-43 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-43 (kafka.cluster.Partition)
[2019-02-12 13:10:50,207] INFO Replica loaded for partition __consumer_offsets-43 with initial high watermark 0 (kafka.cluster.Replica)
[2019-02-12 13:10:50,208] INFO [Partition __consumer_offsets-43 broker=0] __consumer_offsets-43 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-02-12 13:10:50,223] INFO [Log partition=__consumer_offsets-5, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-12 13:10:50,225] INFO [Log partition=__consumer_offsets-5, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 7 ms (kafka.log.Log)
[2019-02-12 13:10:50,226] INFO Created log for partition __consumer_offsets-5 in /tmp/kafka-logs with properties {compression.type -> producer, message.format.version -> 2.1-IV2, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-02-12 13:10:50,228] INFO [Partition __consumer_offsets-5 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-5 (kafka.cluster.Partition)
[2019-02-12 13:10:50,229] INFO Replica loaded for partition __consumer_offsets-5 with initial high watermark 0 (kafka.cluster.Replica)
[2019-02-12 13:10:50,230] INFO [Partition __consumer_offsets-5 broker=0] __consumer_offsets-5 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-02-12 13:10:50,246] INFO [Log partition=__consumer_offsets-21, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-12 13:10:50,248] INFO [Log partition=__consumer_offsets-21, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 8 ms (kafka.log.Log)
[2019-02-12 13:10:50,251] INFO Created log for partition __consumer_offsets-21 in /tmp/kafka-logs with properties {compression.type -> producer, message.format.version -> 2.1-IV2, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-02-12 13:10:50,253] INFO [Partition __consumer_offsets-21 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-21 (kafka.cluster.Partition)
[2019-02-12 13:10:50,253] INFO Replica loaded for partition __consumer_offsets-21 with initial high watermark 0 (kafka.cluster.Replica)
[2019-02-12 13:10:50,254] INFO [Partition __consumer_offsets-21 broker=0] __consumer_offsets-21 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-02-12 13:10:50,285] INFO [Log partition=__consumer_offsets-2, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-12 13:10:50,287] INFO [Log partition=__consumer_offsets-2, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 6 ms (kafka.log.Log)
[2019-02-12 13:10:50,289] INFO Created log for partition __consumer_offsets-2 in /tmp/kafka-logs with properties {compression.type -> producer, message.format.version -> 2.1-IV2, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-02-12 13:10:50,290] INFO [Partition __consumer_offsets-2 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-2 (kafka.cluster.Partition)
[2019-02-12 13:10:50,292] INFO Replica loaded for partition __consumer_offsets-2 with initial high watermark 0 (kafka.cluster.Replica)
[2019-02-12 13:10:50,292] INFO [Partition __consumer_offsets-2 broker=0] __consumer_offsets-2 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-02-12 13:10:50,313] INFO [Log partition=__consumer_offsets-40, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-12 13:10:50,315] INFO [Log partition=__consumer_offsets-40, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 7 ms (kafka.log.Log)
[2019-02-12 13:10:50,316] INFO Created log for partition __consumer_offsets-40 in /tmp/kafka-logs with properties {compression.type -> producer, message.format.version -> 2.1-IV2, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-02-12 13:10:50,317] INFO [Partition __consumer_offsets-40 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-40 (kafka.cluster.Partition)
[2019-02-12 13:10:50,318] INFO Replica loaded for partition __consumer_offsets-40 with initial high watermark 0 (kafka.cluster.Replica)
[2019-02-12 13:10:50,319] INFO [Partition __consumer_offsets-40 broker=0] __consumer_offsets-40 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-02-12 13:10:50,335] INFO [Log partition=__consumer_offsets-37, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-12 13:10:50,336] INFO [Log partition=__consumer_offsets-37, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 7 ms (kafka.log.Log)
[2019-02-12 13:10:50,338] INFO Created log for partition __consumer_offsets-37 in /tmp/kafka-logs with properties {compression.type -> producer, message.format.version -> 2.1-IV2, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-02-12 13:10:50,339] INFO [Partition __consumer_offsets-37 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-37 (kafka.cluster.Partition)
[2019-02-12 13:10:50,340] INFO Replica loaded for partition __consumer_offsets-37 with initial high watermark 0 (kafka.cluster.Replica)
[2019-02-12 13:10:50,341] INFO [Partition __consumer_offsets-37 broker=0] __consumer_offsets-37 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-02-12 13:10:50,360] INFO [Log partition=__consumer_offsets-18, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-12 13:10:50,362] INFO [Log partition=__consumer_offsets-18, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 6 ms (kafka.log.Log)
[2019-02-12 13:10:50,364] INFO Created log for partition __consumer_offsets-18 in /tmp/kafka-logs with properties {compression.type -> producer, message.format.version -> 2.1-IV2, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-02-12 13:10:50,365] INFO [Partition __consumer_offsets-18 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-18 (kafka.cluster.Partition)
[2019-02-12 13:10:50,366] INFO Replica loaded for partition __consumer_offsets-18 with initial high watermark 0 (kafka.cluster.Replica)
[2019-02-12 13:10:50,367] INFO [Partition __consumer_offsets-18 broker=0] __consumer_offsets-18 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-02-12 13:10:50,392] INFO [Log partition=__consumer_offsets-34, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-12 13:10:50,394] INFO [Log partition=__consumer_offsets-34, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 6 ms (kafka.log.Log)
[2019-02-12 13:10:50,395] INFO Created log for partition __consumer_offsets-34 in /tmp/kafka-logs with properties {compression.type -> producer, message.format.version -> 2.1-IV2, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-02-12 13:10:50,396] INFO [Partition __consumer_offsets-34 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-34 (kafka.cluster.Partition)
[2019-02-12 13:10:50,397] INFO Replica loaded for partition __consumer_offsets-34 with initial high watermark 0 (kafka.cluster.Replica)
[2019-02-12 13:10:50,398] INFO [Partition __consumer_offsets-34 broker=0] __consumer_offsets-34 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-02-12 13:10:50,410] INFO [Log partition=__consumer_offsets-15, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-12 13:10:50,411] INFO [Log partition=__consumer_offsets-15, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 6 ms (kafka.log.Log)
[2019-02-12 13:10:50,412] INFO Created log for partition __consumer_offsets-15 in /tmp/kafka-logs with properties {compression.type -> producer, message.format.version -> 2.1-IV2, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-02-12 13:10:50,413] INFO [Partition __consumer_offsets-15 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-15 (kafka.cluster.Partition)
[2019-02-12 13:10:50,414] INFO Replica loaded for partition __consumer_offsets-15 with initial high watermark 0 (kafka.cluster.Replica)
[2019-02-12 13:10:50,415] INFO [Partition __consumer_offsets-15 broker=0] __consumer_offsets-15 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-02-12 13:10:50,427] INFO [Log partition=__consumer_offsets-12, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-12 13:10:50,428] INFO [Log partition=__consumer_offsets-12, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 5 ms (kafka.log.Log)
[2019-02-12 13:10:50,430] INFO Created log for partition __consumer_offsets-12 in /tmp/kafka-logs with properties {compression.type -> producer, message.format.version -> 2.1-IV2, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-02-12 13:10:50,431] INFO [Partition __consumer_offsets-12 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-12 (kafka.cluster.Partition)
[2019-02-12 13:10:50,432] INFO Replica loaded for partition __consumer_offsets-12 with initial high watermark 0 (kafka.cluster.Replica)
[2019-02-12 13:10:50,433] INFO [Partition __consumer_offsets-12 broker=0] __consumer_offsets-12 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-02-12 13:10:50,458] INFO [Log partition=__consumer_offsets-31, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-12 13:10:50,460] INFO [Log partition=__consumer_offsets-31, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 6 ms (kafka.log.Log)
[2019-02-12 13:10:50,462] INFO Created log for partition __consumer_offsets-31 in /tmp/kafka-logs with properties {compression.type -> producer, message.format.version -> 2.1-IV2, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-02-12 13:10:50,464] INFO [Partition __consumer_offsets-31 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-31 (kafka.cluster.Partition)
[2019-02-12 13:10:50,464] INFO Replica loaded for partition __consumer_offsets-31 with initial high watermark 0 (kafka.cluster.Replica)
[2019-02-12 13:10:50,472] INFO [Partition __consumer_offsets-31 broker=0] __consumer_offsets-31 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-02-12 13:10:50,487] INFO [Log partition=__consumer_offsets-9, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-12 13:10:50,489] INFO [Log partition=__consumer_offsets-9, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 6 ms (kafka.log.Log)
[2019-02-12 13:10:50,490] INFO Created log for partition __consumer_offsets-9 in /tmp/kafka-logs with properties {compression.type -> producer, message.format.version -> 2.1-IV2, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-02-12 13:10:50,493] INFO [Partition __consumer_offsets-9 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-9 (kafka.cluster.Partition)
[2019-02-12 13:10:50,493] INFO Replica loaded for partition __consumer_offsets-9 with initial high watermark 0 (kafka.cluster.Replica)
[2019-02-12 13:10:50,494] INFO [Partition __consumer_offsets-9 broker=0] __consumer_offsets-9 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-02-12 13:10:50,520] INFO [Log partition=__consumer_offsets-47, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-12 13:10:50,522] INFO [Log partition=__consumer_offsets-47, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 8 ms (kafka.log.Log)
[2019-02-12 13:10:50,524] INFO Created log for partition __consumer_offsets-47 in /tmp/kafka-logs with properties {compression.type -> producer, message.format.version -> 2.1-IV2, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-02-12 13:10:50,527] INFO [Partition __consumer_offsets-47 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-47 (kafka.cluster.Partition)
[2019-02-12 13:10:50,529] INFO Replica loaded for partition __consumer_offsets-47 with initial high watermark 0 (kafka.cluster.Replica)
[2019-02-12 13:10:50,530] INFO [Partition __consumer_offsets-47 broker=0] __consumer_offsets-47 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-02-12 13:10:50,556] INFO [Log partition=__consumer_offsets-19, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-12 13:10:50,557] INFO [Log partition=__consumer_offsets-19, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 5 ms (kafka.log.Log)
[2019-02-12 13:10:50,559] INFO Created log for partition __consumer_offsets-19 in /tmp/kafka-logs with properties {compression.type -> producer, message.format.version -> 2.1-IV2, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-02-12 13:10:50,561] INFO [Partition __consumer_offsets-19 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-19 (kafka.cluster.Partition)
[2019-02-12 13:10:50,561] INFO Replica loaded for partition __consumer_offsets-19 with initial high watermark 0 (kafka.cluster.Replica)
[2019-02-12 13:10:50,562] INFO [Partition __consumer_offsets-19 broker=0] __consumer_offsets-19 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-02-12 13:10:50,581] INFO [Log partition=__consumer_offsets-28, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-12 13:10:50,583] INFO [Log partition=__consumer_offsets-28, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 6 ms (kafka.log.Log)
[2019-02-12 13:10:50,584] INFO Created log for partition __consumer_offsets-28 in /tmp/kafka-logs with properties {compression.type -> producer, message.format.version -> 2.1-IV2, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-02-12 13:10:50,586] INFO [Partition __consumer_offsets-28 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-28 (kafka.cluster.Partition)
[2019-02-12 13:10:50,588] INFO Replica loaded for partition __consumer_offsets-28 with initial high watermark 0 (kafka.cluster.Replica)
[2019-02-12 13:10:50,589] INFO [Partition __consumer_offsets-28 broker=0] __consumer_offsets-28 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-02-12 13:10:50,602] INFO [Log partition=__consumer_offsets-38, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-12 13:10:50,603] INFO [Log partition=__consumer_offsets-38, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 6 ms (kafka.log.Log)
[2019-02-12 13:10:50,605] INFO Created log for partition __consumer_offsets-38 in /tmp/kafka-logs with properties {compression.type -> producer, message.format.version -> 2.1-IV2, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-02-12 13:10:50,607] INFO [Partition __consumer_offsets-38 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-38 (kafka.cluster.Partition)
[2019-02-12 13:10:50,608] INFO Replica loaded for partition __consumer_offsets-38 with initial high watermark 0 (kafka.cluster.Replica)
[2019-02-12 13:10:50,608] INFO [Partition __consumer_offsets-38 broker=0] __consumer_offsets-38 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-02-12 13:10:50,635] INFO [Log partition=__consumer_offsets-35, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-12 13:10:50,636] INFO [Log partition=__consumer_offsets-35, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 5 ms (kafka.log.Log)
[2019-02-12 13:10:50,637] INFO Created log for partition __consumer_offsets-35 in /tmp/kafka-logs with properties {compression.type -> producer, message.format.version -> 2.1-IV2, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-02-12 13:10:50,639] INFO [Partition __consumer_offsets-35 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-35 (kafka.cluster.Partition)
[2019-02-12 13:10:50,640] INFO Replica loaded for partition __consumer_offsets-35 with initial high watermark 0 (kafka.cluster.Replica)
[2019-02-12 13:10:50,641] INFO [Partition __consumer_offsets-35 broker=0] __consumer_offsets-35 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-02-12 13:10:50,660] INFO [Log partition=__consumer_offsets-44, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-12 13:10:50,662] INFO [Log partition=__consumer_offsets-44, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 6 ms (kafka.log.Log)
[2019-02-12 13:10:50,663] INFO Created log for partition __consumer_offsets-44 in /tmp/kafka-logs with properties {compression.type -> producer, message.format.version -> 2.1-IV2, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-02-12 13:10:50,666] INFO [Partition __consumer_offsets-44 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-44 (kafka.cluster.Partition)
[2019-02-12 13:10:50,666] INFO Replica loaded for partition __consumer_offsets-44 with initial high watermark 0 (kafka.cluster.Replica)
[2019-02-12 13:10:50,667] INFO [Partition __consumer_offsets-44 broker=0] __consumer_offsets-44 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-02-12 13:10:50,680] INFO [Log partition=__consumer_offsets-6, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-12 13:10:50,682] INFO [Log partition=__consumer_offsets-6, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 6 ms (kafka.log.Log)
[2019-02-12 13:10:50,683] INFO Created log for partition __consumer_offsets-6 in /tmp/kafka-logs with properties {compression.type -> producer, message.format.version -> 2.1-IV2, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-02-12 13:10:50,686] INFO [Partition __consumer_offsets-6 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-6 (kafka.cluster.Partition)
[2019-02-12 13:10:50,687] INFO Replica loaded for partition __consumer_offsets-6 with initial high watermark 0 (kafka.cluster.Replica)
[2019-02-12 13:10:50,688] INFO [Partition __consumer_offsets-6 broker=0] __consumer_offsets-6 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-02-12 13:10:50,700] INFO [Log partition=__consumer_offsets-25, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-12 13:10:50,702] INFO [Log partition=__consumer_offsets-25, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 5 ms (kafka.log.Log)
[2019-02-12 13:10:50,703] INFO Created log for partition __consumer_offsets-25 in /tmp/kafka-logs with properties {compression.type -> producer, message.format.version -> 2.1-IV2, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-02-12 13:10:50,704] INFO [Partition __consumer_offsets-25 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-25 (kafka.cluster.Partition)
[2019-02-12 13:10:50,705] INFO Replica loaded for partition __consumer_offsets-25 with initial high watermark 0 (kafka.cluster.Replica)
[2019-02-12 13:10:50,706] INFO [Partition __consumer_offsets-25 broker=0] __consumer_offsets-25 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-02-12 13:10:50,733] INFO [Log partition=__consumer_offsets-16, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-12 13:10:50,735] INFO [Log partition=__consumer_offsets-16, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 6 ms (kafka.log.Log)
[2019-02-12 13:10:50,737] INFO Created log for partition __consumer_offsets-16 in /tmp/kafka-logs with properties {compression.type -> producer, message.format.version -> 2.1-IV2, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-02-12 13:10:50,738] INFO [Partition __consumer_offsets-16 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-16 (kafka.cluster.Partition)
[2019-02-12 13:10:50,739] INFO Replica loaded for partition __consumer_offsets-16 with initial high watermark 0 (kafka.cluster.Replica)
[2019-02-12 13:10:50,740] INFO [Partition __consumer_offsets-16 broker=0] __consumer_offsets-16 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-02-12 13:10:50,752] INFO [Log partition=__consumer_offsets-22, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-12 13:10:50,754] INFO [Log partition=__consumer_offsets-22, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 6 ms (kafka.log.Log)
[2019-02-12 13:10:50,755] INFO Created log for partition __consumer_offsets-22 in /tmp/kafka-logs with properties {compression.type -> producer, message.format.version -> 2.1-IV2, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-02-12 13:10:50,757] INFO [Partition __consumer_offsets-22 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-22 (kafka.cluster.Partition)
[2019-02-12 13:10:50,760] INFO Replica loaded for partition __consumer_offsets-22 with initial high watermark 0 (kafka.cluster.Replica)
[2019-02-12 13:10:50,761] INFO [Partition __consumer_offsets-22 broker=0] __consumer_offsets-22 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-02-12 13:10:50,775] INFO [Log partition=__consumer_offsets-41, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-12 13:10:50,777] INFO [Log partition=__consumer_offsets-41, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 8 ms (kafka.log.Log)
[2019-02-12 13:10:50,778] INFO Created log for partition __consumer_offsets-41 in /tmp/kafka-logs with properties {compression.type -> producer, message.format.version -> 2.1-IV2, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-02-12 13:10:50,779] INFO [Partition __consumer_offsets-41 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-41 (kafka.cluster.Partition)
[2019-02-12 13:10:50,780] INFO Replica loaded for partition __consumer_offsets-41 with initial high watermark 0 (kafka.cluster.Replica)
[2019-02-12 13:10:50,781] INFO [Partition __consumer_offsets-41 broker=0] __consumer_offsets-41 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-02-12 13:10:50,805] INFO [Log partition=__consumer_offsets-32, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-12 13:10:50,807] INFO [Log partition=__consumer_offsets-32, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 7 ms (kafka.log.Log)
[2019-02-12 13:10:50,808] INFO Created log for partition __consumer_offsets-32 in /tmp/kafka-logs with properties {compression.type -> producer, message.format.version -> 2.1-IV2, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-02-12 13:10:50,809] INFO [Partition __consumer_offsets-32 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-32 (kafka.cluster.Partition)
[2019-02-12 13:10:50,810] INFO Replica loaded for partition __consumer_offsets-32 with initial high watermark 0 (kafka.cluster.Replica)
[2019-02-12 13:10:50,811] INFO [Partition __consumer_offsets-32 broker=0] __consumer_offsets-32 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-02-12 13:10:50,831] INFO [Log partition=__consumer_offsets-3, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-12 13:10:50,832] INFO [Log partition=__consumer_offsets-3, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 7 ms (kafka.log.Log)
[2019-02-12 13:10:50,834] INFO Created log for partition __consumer_offsets-3 in /tmp/kafka-logs with properties {compression.type -> producer, message.format.version -> 2.1-IV2, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-02-12 13:10:50,835] INFO [Partition __consumer_offsets-3 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-3 (kafka.cluster.Partition)
[2019-02-12 13:10:50,836] INFO Replica loaded for partition __consumer_offsets-3 with initial high watermark 0 (kafka.cluster.Replica)
[2019-02-12 13:10:50,837] INFO [Partition __consumer_offsets-3 broker=0] __consumer_offsets-3 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-02-12 13:10:50,851] INFO [Log partition=__consumer_offsets-13, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-12 13:10:50,853] INFO [Log partition=__consumer_offsets-13, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 7 ms (kafka.log.Log)
[2019-02-12 13:10:50,854] INFO Created log for partition __consumer_offsets-13 in /tmp/kafka-logs with properties {compression.type -> producer, message.format.version -> 2.1-IV2, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-02-12 13:10:50,855] INFO [Partition __consumer_offsets-13 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-13 (kafka.cluster.Partition)
[2019-02-12 13:10:50,856] INFO Replica loaded for partition __consumer_offsets-13 with initial high watermark 0 (kafka.cluster.Replica)
[2019-02-12 13:10:50,857] INFO [Partition __consumer_offsets-13 broker=0] __consumer_offsets-13 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-02-12 13:10:50,871] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-22 (kafka.coordinator.group.GroupMetadataManager)
[2019-02-12 13:10:50,872] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-25 (kafka.coordinator.group.GroupMetadataManager)
[2019-02-12 13:10:50,873] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-28 (kafka.coordinator.group.GroupMetadataManager)
[2019-02-12 13:10:50,874] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-31 (kafka.coordinator.group.GroupMetadataManager)
[2019-02-12 13:10:50,875] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-34 (kafka.coordinator.group.GroupMetadataManager)
[2019-02-12 13:10:50,876] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-37 (kafka.coordinator.group.GroupMetadataManager)
[2019-02-12 13:10:50,877] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-40 (kafka.coordinator.group.GroupMetadataManager)
[2019-02-12 13:10:50,878] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-43 (kafka.coordinator.group.GroupMetadataManager)
[2019-02-12 13:10:50,879] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-46 (kafka.coordinator.group.GroupMetadataManager)
[2019-02-12 13:10:50,879] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-49 (kafka.coordinator.group.GroupMetadataManager)
[2019-02-12 13:10:50,881] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-41 (kafka.coordinator.group.GroupMetadataManager)
[2019-02-12 13:10:50,882] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-44 (kafka.coordinator.group.GroupMetadataManager)
[2019-02-12 13:10:50,882] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-22 in 9 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-02-12 13:10:50,885] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-47 (kafka.coordinator.group.GroupMetadataManager)
[2019-02-12 13:10:50,887] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-1 (kafka.coordinator.group.GroupMetadataManager)
[2019-02-12 13:10:50,887] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-25 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-02-12 13:10:50,888] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-4 (kafka.coordinator.group.GroupMetadataManager)
[2019-02-12 13:10:50,889] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-28 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-02-12 13:10:50,889] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-7 (kafka.coordinator.group.GroupMetadataManager)
[2019-02-12 13:10:50,890] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-31 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-02-12 13:10:50,891] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-10 (kafka.coordinator.group.GroupMetadataManager)
[2019-02-12 13:10:50,892] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-34 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-02-12 13:10:50,892] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-13 (kafka.coordinator.group.GroupMetadataManager)
[2019-02-12 13:10:50,893] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-37 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-02-12 13:10:50,894] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-16 (kafka.coordinator.group.GroupMetadataManager)
[2019-02-12 13:10:50,896] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-19 (kafka.coordinator.group.GroupMetadataManager)
[2019-02-12 13:10:50,895] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-40 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-02-12 13:10:50,897] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-2 (kafka.coordinator.group.GroupMetadataManager)
[2019-02-12 13:10:50,898] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-43 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-02-12 13:10:50,900] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-5 (kafka.coordinator.group.GroupMetadataManager)
[2019-02-12 13:10:50,901] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-46 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-02-12 13:10:50,902] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-8 (kafka.coordinator.group.GroupMetadataManager)
[2019-02-12 13:10:50,903] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-49 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-02-12 13:10:50,903] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-11 (kafka.coordinator.group.GroupMetadataManager)
[2019-02-12 13:10:50,905] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-41 in 1 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-02-12 13:10:50,905] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-14 (kafka.coordinator.group.GroupMetadataManager)
[2019-02-12 13:10:50,906] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-44 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-02-12 13:10:50,907] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-17 (kafka.coordinator.group.GroupMetadataManager)
[2019-02-12 13:10:50,908] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-47 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-02-12 13:10:50,909] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-20 (kafka.coordinator.group.GroupMetadataManager)
[2019-02-12 13:10:50,909] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-1 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-02-12 13:10:50,910] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-23 (kafka.coordinator.group.GroupMetadataManager)
[2019-02-12 13:10:50,911] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-4 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-02-12 13:10:50,912] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-26 (kafka.coordinator.group.GroupMetadataManager)
[2019-02-12 13:10:50,914] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-29 (kafka.coordinator.group.GroupMetadataManager)
[2019-02-12 13:10:50,913] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-7 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-02-12 13:10:50,916] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-32 (kafka.coordinator.group.GroupMetadataManager)
[2019-02-12 13:10:50,917] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-10 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-02-12 13:10:50,918] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-35 (kafka.coordinator.group.GroupMetadataManager)
[2019-02-12 13:10:50,919] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-13 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-02-12 13:10:50,919] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-38 (kafka.coordinator.group.GroupMetadataManager)
[2019-02-12 13:10:50,920] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-16 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-02-12 13:10:50,921] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-0 (kafka.coordinator.group.GroupMetadataManager)
[2019-02-12 13:10:50,922] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-19 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-02-12 13:10:50,922] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-3 (kafka.coordinator.group.GroupMetadataManager)
[2019-02-12 13:10:50,923] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-2 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-02-12 13:10:50,924] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-6 (kafka.coordinator.group.GroupMetadataManager)
[2019-02-12 13:10:50,925] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-5 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-02-12 13:10:50,926] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-9 (kafka.coordinator.group.GroupMetadataManager)
[2019-02-12 13:10:50,926] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-8 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-02-12 13:10:50,927] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-12 (kafka.coordinator.group.GroupMetadataManager)
[2019-02-12 13:10:50,928] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-11 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-02-12 13:10:50,929] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-15 (kafka.coordinator.group.GroupMetadataManager)
[2019-02-12 13:10:50,931] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-14 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-02-12 13:10:50,932] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-18 (kafka.coordinator.group.GroupMetadataManager)
[2019-02-12 13:10:50,933] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-17 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-02-12 13:10:50,933] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-21 (kafka.coordinator.group.GroupMetadataManager)
[2019-02-12 13:10:50,934] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-20 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-02-12 13:10:50,935] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-24 (kafka.coordinator.group.GroupMetadataManager)
[2019-02-12 13:10:50,936] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-23 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-02-12 13:10:50,936] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-27 (kafka.coordinator.group.GroupMetadataManager)
[2019-02-12 13:10:50,937] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-26 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-02-12 13:10:50,938] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-30 (kafka.coordinator.group.GroupMetadataManager)
[2019-02-12 13:10:50,939] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-29 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-02-12 13:10:50,939] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-33 (kafka.coordinator.group.GroupMetadataManager)
[2019-02-12 13:10:50,940] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-32 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-02-12 13:10:50,941] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-36 (kafka.coordinator.group.GroupMetadataManager)
[2019-02-12 13:10:50,942] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-35 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-02-12 13:10:50,942] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-39 (kafka.coordinator.group.GroupMetadataManager)
[2019-02-12 13:10:50,943] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-38 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-02-12 13:10:50,944] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-42 (kafka.coordinator.group.GroupMetadataManager)
[2019-02-12 13:10:50,945] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-0 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-02-12 13:10:50,947] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-45 (kafka.coordinator.group.GroupMetadataManager)
[2019-02-12 13:10:50,948] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-3 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-02-12 13:10:50,949] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-48 (kafka.coordinator.group.GroupMetadataManager)
[2019-02-12 13:10:50,950] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-6 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-02-12 13:10:50,951] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-9 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-02-12 13:10:50,953] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-12 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-02-12 13:10:50,955] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-15 in 1 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-02-12 13:10:50,955] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-18 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-02-12 13:10:50,956] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-21 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-02-12 13:10:50,957] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-24 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-02-12 13:10:50,958] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-27 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-02-12 13:10:50,959] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-30 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-02-12 13:10:50,960] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-33 in 1 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-02-12 13:10:50,960] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-36 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-02-12 13:10:50,961] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-39 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-02-12 13:10:50,962] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-42 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-02-12 13:10:50,963] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-45 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-02-12 13:10:50,963] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-48 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-02-12 13:10:51,007] INFO [GroupCoordinator 0]: Preparing to rebalance group connect-local-file-sink in state PreparingRebalance with old generation 0 (__consumer_offsets-7) (reason: Adding new member consumer-1-e910eef0-39eb-4808-a5c9-371bf7171974) (kafka.coordinator.group.GroupCoordinator)
[2019-02-12 13:10:51,023] INFO [GroupCoordinator 0]: Stabilized group connect-local-file-sink generation 1 (__consumer_offsets-7) (kafka.coordinator.group.GroupCoordinator)
[2019-02-12 13:10:51,034] INFO [GroupCoordinator 0]: Assignment received from leader for group connect-local-file-sink for generation 1 (kafka.coordinator.group.GroupCoordinator)
[2019-02-12 13:17:38,955] INFO [GroupMetadataManager brokerId=0] Removed 0 expired offsets in 20 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-02-12 13:27:38,936] INFO [GroupMetadataManager brokerId=0] Removed 0 expired offsets in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-02-12 13:30:07,270] WARN Client session timed out, have not heard from server in 4001ms for sessionid 0x1000001b4720000 (org.apache.zookeeper.ClientCnxn)
[2019-02-12 13:30:07,275] INFO Client session timed out, have not heard from server in 4001ms for sessionid 0x1000001b4720000, closing socket connection and attempting reconnect (org.apache.zookeeper.ClientCnxn)
[2019-02-12 13:30:07,394] INFO [ZooKeeperClient] Waiting until connected. (kafka.zookeeper.ZooKeeperClient)
[2019-02-12 13:30:07,403] INFO [ZooKeeperClient] Connected. (kafka.zookeeper.ZooKeeperClient)
[2019-02-12 13:30:08,740] INFO Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
[2019-02-12 13:30:08,747] INFO Socket connection established to localhost/127.0.0.1:2181, initiating session (org.apache.zookeeper.ClientCnxn)
[2019-02-12 13:30:14,747] WARN Client session timed out, have not heard from server in 6001ms for sessionid 0x1000001b4720000 (org.apache.zookeeper.ClientCnxn)
[2019-02-12 13:30:14,753] INFO Client session timed out, have not heard from server in 6001ms for sessionid 0x1000001b4720000, closing socket connection and attempting reconnect (org.apache.zookeeper.ClientCnxn)
[2019-02-12 13:30:14,860] INFO [ZooKeeperClient] Waiting until connected. (kafka.zookeeper.ZooKeeperClient)
[2019-02-12 13:30:16,742] INFO Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
[2019-02-12 13:30:16,744] INFO Socket connection established to localhost/127.0.0.1:2181, initiating session (org.apache.zookeeper.ClientCnxn)
[2019-02-12 13:30:22,745] WARN Client session timed out, have not heard from server in 6001ms for sessionid 0x1000001b4720000 (org.apache.zookeeper.ClientCnxn)
[2019-02-12 13:30:22,750] INFO Client session timed out, have not heard from server in 6001ms for sessionid 0x1000001b4720000, closing socket connection and attempting reconnect (org.apache.zookeeper.ClientCnxn)
[2019-02-12 13:30:24,621] INFO Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
[2019-02-12 13:30:24,627] INFO Socket connection established to localhost/127.0.0.1:2181, initiating session (org.apache.zookeeper.ClientCnxn)
[2019-02-12 13:30:30,628] WARN Client session timed out, have not heard from server in 6002ms for sessionid 0x1000001b4720000 (org.apache.zookeeper.ClientCnxn)
[2019-02-12 13:30:30,634] INFO Client session timed out, have not heard from server in 6002ms for sessionid 0x1000001b4720000, closing socket connection and attempting reconnect (org.apache.zookeeper.ClientCnxn)
[2019-02-12 13:30:31,855] INFO Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
[2019-02-12 13:30:31,863] INFO Socket connection established to localhost/127.0.0.1:2181, initiating session (org.apache.zookeeper.ClientCnxn)
[2019-02-12 13:30:37,864] WARN Client session timed out, have not heard from server in 6001ms for sessionid 0x1000001b4720000 (org.apache.zookeeper.ClientCnxn)
[2019-02-12 13:30:37,870] INFO Client session timed out, have not heard from server in 6001ms for sessionid 0x1000001b4720000, closing socket connection and attempting reconnect (org.apache.zookeeper.ClientCnxn)
[2019-02-12 13:30:37,976] INFO [ZooKeeperClient] Waiting until connected. (kafka.zookeeper.ZooKeeperClient)
[2019-02-12 13:30:39,697] INFO Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
[2019-02-12 13:30:39,703] INFO Socket connection established to localhost/127.0.0.1:2181, initiating session (org.apache.zookeeper.ClientCnxn)
[2019-02-12 13:30:45,704] WARN Client session timed out, have not heard from server in 6000ms for sessionid 0x1000001b4720000 (org.apache.zookeeper.ClientCnxn)
[2019-02-12 13:30:45,709] INFO Client session timed out, have not heard from server in 6000ms for sessionid 0x1000001b4720000, closing socket connection and attempting reconnect (org.apache.zookeeper.ClientCnxn)
[2019-02-12 13:30:46,943] INFO Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
[2019-02-12 13:30:46,949] INFO Socket connection established to localhost/127.0.0.1:2181, initiating session (org.apache.zookeeper.ClientCnxn)
[2019-02-12 13:30:52,951] WARN Client session timed out, have not heard from server in 6001ms for sessionid 0x1000001b4720000 (org.apache.zookeeper.ClientCnxn)
[2019-02-12 13:30:52,955] INFO Client session timed out, have not heard from server in 6001ms for sessionid 0x1000001b4720000, closing socket connection and attempting reconnect (org.apache.zookeeper.ClientCnxn)
[2019-02-12 13:30:54,938] INFO Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
[2019-02-12 13:30:54,954] INFO Socket connection established to localhost/127.0.0.1:2181, initiating session (org.apache.zookeeper.ClientCnxn)
[2019-02-12 13:31:00,954] WARN Client session timed out, have not heard from server in 6001ms for sessionid 0x1000001b4720000 (org.apache.zookeeper.ClientCnxn)
[2019-02-12 13:31:00,973] INFO Client session timed out, have not heard from server in 6001ms for sessionid 0x1000001b4720000, closing socket connection and attempting reconnect (org.apache.zookeeper.ClientCnxn)
[2019-02-12 13:31:02,620] INFO Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
[2019-02-12 13:31:02,628] INFO Socket connection established to localhost/127.0.0.1:2181, initiating session (org.apache.zookeeper.ClientCnxn)
[2019-02-12 13:31:08,630] WARN Client session timed out, have not heard from server in 6002ms for sessionid 0x1000001b4720000 (org.apache.zookeeper.ClientCnxn)
[2019-02-12 13:31:08,636] INFO Client session timed out, have not heard from server in 6002ms for sessionid 0x1000001b4720000, closing socket connection and attempting reconnect (org.apache.zookeeper.ClientCnxn)
[2019-02-12 13:31:10,660] INFO Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
[2019-02-12 13:31:10,664] INFO Socket connection established to localhost/127.0.0.1:2181, initiating session (org.apache.zookeeper.ClientCnxn)
[2019-02-12 13:31:16,668] WARN Client session timed out, have not heard from server in 6003ms for sessionid 0x1000001b4720000 (org.apache.zookeeper.ClientCnxn)
[2019-02-12 13:31:16,675] INFO Client session timed out, have not heard from server in 6003ms for sessionid 0x1000001b4720000, closing socket connection and attempting reconnect (org.apache.zookeeper.ClientCnxn)
[2019-02-12 13:31:18,622] INFO Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
[2019-02-12 13:31:18,628] INFO Socket connection established to localhost/127.0.0.1:2181, initiating session (org.apache.zookeeper.ClientCnxn)
[2019-02-12 13:31:24,629] WARN Client session timed out, have not heard from server in 6001ms for sessionid 0x1000001b4720000 (org.apache.zookeeper.ClientCnxn)
[2019-02-12 13:31:24,633] INFO Client session timed out, have not heard from server in 6001ms for sessionid 0x1000001b4720000, closing socket connection and attempting reconnect (org.apache.zookeeper.ClientCnxn)
[2019-02-12 13:31:26,696] INFO Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
[2019-02-12 13:31:26,699] INFO Socket connection established to localhost/127.0.0.1:2181, initiating session (org.apache.zookeeper.ClientCnxn)
[2019-02-12 13:31:32,699] WARN Client session timed out, have not heard from server in 6001ms for sessionid 0x1000001b4720000 (org.apache.zookeeper.ClientCnxn)
[2019-02-12 13:31:32,701] INFO Client session timed out, have not heard from server in 6001ms for sessionid 0x1000001b4720000, closing socket connection and attempting reconnect (org.apache.zookeeper.ClientCnxn)
[2019-02-12 13:31:34,789] INFO Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
[2019-02-12 13:31:34,791] INFO Socket connection established to localhost/127.0.0.1:2181, initiating session (org.apache.zookeeper.ClientCnxn)
[2019-02-12 13:31:40,791] WARN Client session timed out, have not heard from server in 6001ms for sessionid 0x1000001b4720000 (org.apache.zookeeper.ClientCnxn)
[2019-02-12 13:31:40,798] INFO Client session timed out, have not heard from server in 6001ms for sessionid 0x1000001b4720000, closing socket connection and attempting reconnect (org.apache.zookeeper.ClientCnxn)
[2019-02-12 13:31:42,519] INFO Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
[2019-02-12 13:31:42,526] INFO Socket connection established to localhost/127.0.0.1:2181, initiating session (org.apache.zookeeper.ClientCnxn)
[2019-02-12 13:31:48,527] WARN Client session timed out, have not heard from server in 6002ms for sessionid 0x1000001b4720000 (org.apache.zookeeper.ClientCnxn)
[2019-02-12 13:31:48,530] INFO Client session timed out, have not heard from server in 6002ms for sessionid 0x1000001b4720000, closing socket connection and attempting reconnect (org.apache.zookeeper.ClientCnxn)
[2019-02-12 13:31:50,611] INFO Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
[2019-02-12 13:31:50,618] INFO Socket connection established to localhost/127.0.0.1:2181, initiating session (org.apache.zookeeper.ClientCnxn)
[2019-02-12 13:31:56,619] WARN Client session timed out, have not heard from server in 6001ms for sessionid 0x1000001b4720000 (org.apache.zookeeper.ClientCnxn)
[2019-02-12 13:31:56,625] INFO Client session timed out, have not heard from server in 6001ms for sessionid 0x1000001b4720000, closing socket connection and attempting reconnect (org.apache.zookeeper.ClientCnxn)
[2019-02-12 13:31:57,841] INFO Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
[2019-02-12 13:31:57,847] INFO Socket connection established to localhost/127.0.0.1:2181, initiating session (org.apache.zookeeper.ClientCnxn)
[2019-02-12 13:32:03,848] WARN Client session timed out, have not heard from server in 6001ms for sessionid 0x1000001b4720000 (org.apache.zookeeper.ClientCnxn)
[2019-02-12 13:32:03,853] INFO Client session timed out, have not heard from server in 6001ms for sessionid 0x1000001b4720000, closing socket connection and attempting reconnect (org.apache.zookeeper.ClientCnxn)
[2019-02-12 13:32:05,422] INFO Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
[2019-02-12 13:32:05,430] INFO Socket connection established to localhost/127.0.0.1:2181, initiating session (org.apache.zookeeper.ClientCnxn)
[2019-02-12 13:32:11,430] WARN Client session timed out, have not heard from server in 6001ms for sessionid 0x1000001b4720000 (org.apache.zookeeper.ClientCnxn)
[2019-02-12 13:32:11,435] INFO Client session timed out, have not heard from server in 6001ms for sessionid 0x1000001b4720000, closing socket connection and attempting reconnect (org.apache.zookeeper.ClientCnxn)
[2019-02-12 13:32:12,546] INFO Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
[2019-02-12 13:32:12,553] INFO Socket connection established to localhost/127.0.0.1:2181, initiating session (org.apache.zookeeper.ClientCnxn)
[2019-02-12 13:32:16,075] INFO Terminating process due to signal SIGINT (org.apache.kafka.common.utils.LoggingSignalHandler)
[2019-02-12 13:32:16,097] INFO [KafkaServer id=0] shutting down (kafka.server.KafkaServer)
[2019-02-12 13:32:16,107] INFO [KafkaServer id=0] Starting controlled shutdown (kafka.server.KafkaServer)
[2019-02-12 13:32:18,553] WARN Client session timed out, have not heard from server in 6000ms for sessionid 0x1000001b4720000 (org.apache.zookeeper.ClientCnxn)
[2019-02-12 13:32:19,261] INFO Client session timed out, have not heard from server in 6000ms for sessionid 0x1000001b4720000, closing socket connection and attempting reconnect (org.apache.zookeeper.ClientCnxn)
[2019-02-12 13:32:19,372] INFO [ZooKeeperClient] Waiting until connected. (kafka.zookeeper.ZooKeeperClient)
[2019-02-12 13:32:20,139] INFO Terminating process due to signal SIGINT (org.apache.kafka.common.utils.LoggingSignalHandler)
[2019-02-12 13:32:20,534] INFO Terminating process due to signal SIGINT (org.apache.kafka.common.utils.LoggingSignalHandler)
[2019-02-12 13:32:20,652] INFO Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
[2019-02-12 13:32:20,659] INFO Socket connection established to localhost/127.0.0.1:2181, initiating session (org.apache.zookeeper.ClientCnxn)
[2019-02-12 13:32:20,723] INFO Terminating process due to signal SIGINT (org.apache.kafka.common.utils.LoggingSignalHandler)
[2019-02-12 13:32:23,037] INFO [GroupCoordinator 0]: Preparing to rebalance group connect-local-file-sink in state PreparingRebalance with old generation 1 (__consumer_offsets-7) (reason: removing member consumer-1-e910eef0-39eb-4808-a5c9-371bf7171974 on LeaveGroup) (kafka.coordinator.group.GroupCoordinator)
[2019-02-12 13:32:23,042] INFO [GroupCoordinator 0]: Group connect-local-file-sink with generation 2 is now empty (__consumer_offsets-7) (kafka.coordinator.group.GroupCoordinator)
[2019-02-12 13:32:26,660] WARN Client session timed out, have not heard from server in 6001ms for sessionid 0x1000001b4720000 (org.apache.zookeeper.ClientCnxn)
[2019-02-12 13:32:26,665] INFO Client session timed out, have not heard from server in 6001ms for sessionid 0x1000001b4720000, closing socket connection and attempting reconnect (org.apache.zookeeper.ClientCnxn)
[2019-02-12 13:32:27,818] INFO Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
[2019-02-12 13:32:27,828] INFO Socket connection established to localhost/127.0.0.1:2181, initiating session (org.apache.zookeeper.ClientCnxn)
[2019-02-12 13:32:33,828] WARN Client session timed out, have not heard from server in 6001ms for sessionid 0x1000001b4720000 (org.apache.zookeeper.ClientCnxn)
[2019-02-12 13:32:33,833] INFO Client session timed out, have not heard from server in 6001ms for sessionid 0x1000001b4720000, closing socket connection and attempting reconnect (org.apache.zookeeper.ClientCnxn)
[2019-02-12 13:32:34,952] INFO Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
[2019-02-12 13:32:34,953] INFO Socket connection established to localhost/127.0.0.1:2181, initiating session (org.apache.zookeeper.ClientCnxn)
[2019-02-12 13:32:40,954] WARN Client session timed out, have not heard from server in 6000ms for sessionid 0x1000001b4720000 (org.apache.zookeeper.ClientCnxn)
[2019-02-12 13:32:40,958] INFO Client session timed out, have not heard from server in 6000ms for sessionid 0x1000001b4720000, closing socket connection and attempting reconnect (org.apache.zookeeper.ClientCnxn)
[2019-02-12 13:32:42,148] INFO Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
[2019-02-12 13:32:42,155] INFO Socket connection established to localhost/127.0.0.1:2181, initiating session (org.apache.zookeeper.ClientCnxn)
[2019-02-12 13:32:48,157] WARN Client session timed out, have not heard from server in 6001ms for sessionid 0x1000001b4720000 (org.apache.zookeeper.ClientCnxn)
[2019-02-12 13:32:48,159] INFO Client session timed out, have not heard from server in 6001ms for sessionid 0x1000001b4720000, closing socket connection and attempting reconnect (org.apache.zookeeper.ClientCnxn)
[2019-02-12 13:32:49,911] INFO Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
[2019-02-12 13:32:49,922] INFO Socket connection established to localhost/127.0.0.1:2181, initiating session (org.apache.zookeeper.ClientCnxn)
[2019-02-12 13:32:55,922] WARN Client session timed out, have not heard from server in 6001ms for sessionid 0x1000001b4720000 (org.apache.zookeeper.ClientCnxn)
[2019-02-12 13:32:55,928] INFO Client session timed out, have not heard from server in 6001ms for sessionid 0x1000001b4720000, closing socket connection and attempting reconnect (org.apache.zookeeper.ClientCnxn)
[2019-02-12 13:32:57,277] INFO Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
[2019-02-12 13:32:57,284] INFO Socket connection established to localhost/127.0.0.1:2181, initiating session (org.apache.zookeeper.ClientCnxn)
[2019-02-12 13:32:59,527] INFO Terminating process due to signal SIGINT (org.apache.kafka.common.utils.LoggingSignalHandler)
[2019-02-12 13:32:59,827] INFO Terminating process due to signal SIGINT (org.apache.kafka.common.utils.LoggingSignalHandler)
[2019-02-12 13:33:00,054] INFO Terminating process due to signal SIGINT (org.apache.kafka.common.utils.LoggingSignalHandler)
[2019-02-12 13:33:00,301] INFO Terminating process due to signal SIGINT (org.apache.kafka.common.utils.LoggingSignalHandler)
[2019-02-12 13:33:00,475] INFO Terminating process due to signal SIGINT (org.apache.kafka.common.utils.LoggingSignalHandler)
[2019-02-12 13:33:00,630] INFO Terminating process due to signal SIGINT (org.apache.kafka.common.utils.LoggingSignalHandler)
[2019-02-12 13:33:01,118] INFO Terminating process due to signal SIGINT (org.apache.kafka.common.utils.LoggingSignalHandler)
[2019-02-12 13:33:01,154] INFO Terminating process due to signal SIGINT (org.apache.kafka.common.utils.LoggingSignalHandler)
[2019-02-12 13:33:01,182] INFO Terminating process due to signal SIGINT (org.apache.kafka.common.utils.LoggingSignalHandler)
[2019-02-12 13:33:01,212] INFO Terminating process due to signal SIGINT (org.apache.kafka.common.utils.LoggingSignalHandler)
[2019-02-12 13:33:01,241] INFO Terminating process due to signal SIGINT (org.apache.kafka.common.utils.LoggingSignalHandler)
[2019-02-12 13:33:01,406] INFO Terminating process due to signal SIGINT (org.apache.kafka.common.utils.LoggingSignalHandler)
[2019-02-12 13:33:02,007] INFO Terminating process due to signal SIGINT (org.apache.kafka.common.utils.LoggingSignalHandler)
[2019-02-12 13:33:02,242] INFO Terminating process due to signal SIGINT (org.apache.kafka.common.utils.LoggingSignalHandler)
[2019-02-12 13:33:02,425] INFO Terminating process due to signal SIGINT (org.apache.kafka.common.utils.LoggingSignalHandler)
[2019-02-12 13:33:02,594] INFO Terminating process due to signal SIGINT (org.apache.kafka.common.utils.LoggingSignalHandler)
[2019-02-12 13:33:02,757] INFO Terminating process due to signal SIGINT (org.apache.kafka.common.utils.LoggingSignalHandler)
[2019-02-12 13:33:03,011] INFO Terminating process due to signal SIGINT (org.apache.kafka.common.utils.LoggingSignalHandler)
[2019-02-12 13:33:03,285] WARN Client session timed out, have not heard from server in 6001ms for sessionid 0x1000001b4720000 (org.apache.zookeeper.ClientCnxn)
[2019-02-12 13:33:03,290] INFO Client session timed out, have not heard from server in 6001ms for sessionid 0x1000001b4720000, closing socket connection and attempting reconnect (org.apache.zookeeper.ClientCnxn)
[2019-02-12 13:33:04,941] INFO Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
[2019-02-12 13:33:04,944] INFO Socket connection established to localhost/127.0.0.1:2181, initiating session (org.apache.zookeeper.ClientCnxn)
[2019-02-12 13:33:10,946] WARN Client session timed out, have not heard from server in 6001ms for sessionid 0x1000001b4720000 (org.apache.zookeeper.ClientCnxn)
[2019-02-12 13:33:10,950] INFO Client session timed out, have not heard from server in 6001ms for sessionid 0x1000001b4720000, closing socket connection and attempting reconnect (org.apache.zookeeper.ClientCnxn)
[2019-02-12 13:33:12,694] INFO Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
[2019-02-12 13:33:12,697] INFO Socket connection established to localhost/127.0.0.1:2181, initiating session (org.apache.zookeeper.ClientCnxn)
[2019-02-12 13:33:18,698] WARN Client session timed out, have not heard from server in 6001ms for sessionid 0x1000001b4720000 (org.apache.zookeeper.ClientCnxn)
[2019-02-12 13:33:18,703] INFO Client session timed out, have not heard from server in 6001ms for sessionid 0x1000001b4720000, closing socket connection and attempting reconnect (org.apache.zookeeper.ClientCnxn)
[2019-02-12 13:33:20,721] INFO Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
[2019-02-12 13:33:20,727] INFO Socket connection established to localhost/127.0.0.1:2181, initiating session (org.apache.zookeeper.ClientCnxn)
[2019-02-12 13:33:26,727] WARN Client session timed out, have not heard from server in 6001ms for sessionid 0x1000001b4720000 (org.apache.zookeeper.ClientCnxn)
[2019-02-12 13:33:26,732] INFO Client session timed out, have not heard from server in 6001ms for sessionid 0x1000001b4720000, closing socket connection and attempting reconnect (org.apache.zookeeper.ClientCnxn)
[2019-02-12 13:33:27,894] INFO Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
[2019-02-12 13:33:27,900] INFO Socket connection established to localhost/127.0.0.1:2181, initiating session (org.apache.zookeeper.ClientCnxn)
[2019-02-12 13:33:33,902] WARN Client session timed out, have not heard from server in 6001ms for sessionid 0x1000001b4720000 (org.apache.zookeeper.ClientCnxn)
[2019-02-12 13:33:33,908] INFO Client session timed out, have not heard from server in 6001ms for sessionid 0x1000001b4720000, closing socket connection and attempting reconnect (org.apache.zookeeper.ClientCnxn)
[2019-02-12 13:33:35,388] INFO Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
[2019-02-12 13:33:35,395] INFO Socket connection established to localhost/127.0.0.1:2181, initiating session (org.apache.zookeeper.ClientCnxn)
[2019-02-12 13:33:41,397] WARN Client session timed out, have not heard from server in 6001ms for sessionid 0x1000001b4720000 (org.apache.zookeeper.ClientCnxn)
[2019-02-12 13:33:41,404] INFO Client session timed out, have not heard from server in 6001ms for sessionid 0x1000001b4720000, closing socket connection and attempting reconnect (org.apache.zookeeper.ClientCnxn)
[2019-02-12 13:33:43,339] INFO Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
[2019-02-12 13:33:43,346] INFO Socket connection established to localhost/127.0.0.1:2181, initiating session (org.apache.zookeeper.ClientCnxn)
[2019-02-12 13:33:49,347] WARN Client session timed out, have not heard from server in 6001ms for sessionid 0x1000001b4720000 (org.apache.zookeeper.ClientCnxn)
[2019-02-12 13:33:49,349] INFO Client session timed out, have not heard from server in 6001ms for sessionid 0x1000001b4720000, closing socket connection and attempting reconnect (org.apache.zookeeper.ClientCnxn)
[2019-02-12 13:33:50,455] INFO Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
[2019-02-12 13:33:50,461] INFO Socket connection established to localhost/127.0.0.1:2181, initiating session (org.apache.zookeeper.ClientCnxn)
[2019-02-12 13:33:56,462] WARN Client session timed out, have not heard from server in 6001ms for sessionid 0x1000001b4720000 (org.apache.zookeeper.ClientCnxn)
[2019-02-12 13:33:56,469] INFO Client session timed out, have not heard from server in 6001ms for sessionid 0x1000001b4720000, closing socket connection and attempting reconnect (org.apache.zookeeper.ClientCnxn)
[2019-02-12 13:33:58,339] INFO Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
[2019-02-12 13:33:58,345] INFO Socket connection established to localhost/127.0.0.1:2181, initiating session (org.apache.zookeeper.ClientCnxn)
[2019-02-12 13:34:04,346] WARN Client session timed out, have not heard from server in 6001ms for sessionid 0x1000001b4720000 (org.apache.zookeeper.ClientCnxn)
[2019-02-12 13:34:04,352] INFO Client session timed out, have not heard from server in 6001ms for sessionid 0x1000001b4720000, closing socket connection and attempting reconnect (org.apache.zookeeper.ClientCnxn)
[2019-02-12 13:34:05,539] INFO Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
[2019-02-12 13:34:05,545] INFO Socket connection established to localhost/127.0.0.1:2181, initiating session (org.apache.zookeeper.ClientCnxn)
[2019-02-12 13:34:11,546] WARN Client session timed out, have not heard from server in 6001ms for sessionid 0x1000001b4720000 (org.apache.zookeeper.ClientCnxn)
[2019-02-12 13:34:11,549] INFO Client session timed out, have not heard from server in 6001ms for sessionid 0x1000001b4720000, closing socket connection and attempting reconnect (org.apache.zookeeper.ClientCnxn)
[2019-02-12 13:34:13,464] INFO Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
[2019-02-12 13:34:13,473] INFO Socket connection established to localhost/127.0.0.1:2181, initiating session (org.apache.zookeeper.ClientCnxn)
[2019-02-12 13:34:19,473] WARN Client session timed out, have not heard from server in 6001ms for sessionid 0x1000001b4720000 (org.apache.zookeeper.ClientCnxn)
[2019-02-12 13:34:19,476] INFO Client session timed out, have not heard from server in 6001ms for sessionid 0x1000001b4720000, closing socket connection and attempting reconnect (org.apache.zookeeper.ClientCnxn)
[2019-02-12 13:34:20,629] INFO Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
[2019-02-12 13:34:20,635] INFO Socket connection established to localhost/127.0.0.1:2181, initiating session (org.apache.zookeeper.ClientCnxn)
[2019-02-12 13:34:26,636] WARN Client session timed out, have not heard from server in 6000ms for sessionid 0x1000001b4720000 (org.apache.zookeeper.ClientCnxn)
[2019-02-12 13:34:26,641] INFO Client session timed out, have not heard from server in 6000ms for sessionid 0x1000001b4720000, closing socket connection and attempting reconnect (org.apache.zookeeper.ClientCnxn)
[2019-02-12 13:34:28,149] INFO Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
[2019-02-12 13:34:28,155] INFO Socket connection established to localhost/127.0.0.1:2181, initiating session (org.apache.zookeeper.ClientCnxn)
[2019-02-12 13:34:34,157] WARN Client session timed out, have not heard from server in 6001ms for sessionid 0x1000001b4720000 (org.apache.zookeeper.ClientCnxn)
[2019-02-12 13:34:34,161] INFO Client session timed out, have not heard from server in 6001ms for sessionid 0x1000001b4720000, closing socket connection and attempting reconnect (org.apache.zookeeper.ClientCnxn)
[2019-02-12 13:34:35,807] INFO Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
[2019-02-12 13:34:35,813] INFO Socket connection established to localhost/127.0.0.1:2181, initiating session (org.apache.zookeeper.ClientCnxn)
[2019-02-12 13:34:41,814] WARN Client session timed out, have not heard from server in 6001ms for sessionid 0x1000001b4720000 (org.apache.zookeeper.ClientCnxn)
[2019-02-12 13:34:41,818] INFO Client session timed out, have not heard from server in 6001ms for sessionid 0x1000001b4720000, closing socket connection and attempting reconnect (org.apache.zookeeper.ClientCnxn)
[2019-02-12 13:34:43,770] INFO Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
[2019-02-12 13:34:43,776] INFO Socket connection established to localhost/127.0.0.1:2181, initiating session (org.apache.zookeeper.ClientCnxn)
[2019-02-12 13:34:49,777] WARN Client session timed out, have not heard from server in 6002ms for sessionid 0x1000001b4720000 (org.apache.zookeeper.ClientCnxn)
[2019-02-12 13:34:49,782] INFO Client session timed out, have not heard from server in 6002ms for sessionid 0x1000001b4720000, closing socket connection and attempting reconnect (org.apache.zookeeper.ClientCnxn)
[2019-02-12 13:34:51,527] INFO Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
[2019-02-12 13:34:51,533] INFO Socket connection established to localhost/127.0.0.1:2181, initiating session (org.apache.zookeeper.ClientCnxn)
[2019-02-12 13:34:55,185] INFO Terminating process due to signal SIGINT (org.apache.kafka.common.utils.LoggingSignalHandler)
[2019-02-12 13:34:55,354] INFO Terminating process due to signal SIGINT (org.apache.kafka.common.utils.LoggingSignalHandler)
[2019-02-12 13:34:56,468] INFO Terminating process due to signal SIGINT (org.apache.kafka.common.utils.LoggingSignalHandler)
[2019-02-12 13:34:56,872] INFO Terminating process due to signal SIGINT (org.apache.kafka.common.utils.LoggingSignalHandler)
[2019-02-12 13:34:57,287] INFO Terminating process due to signal SIGINT (org.apache.kafka.common.utils.LoggingSignalHandler)
[2019-02-12 13:34:57,526] INFO Terminating process due to signal SIGINT (org.apache.kafka.common.utils.LoggingSignalHandler)
[2019-02-12 13:34:57,533] WARN Client session timed out, have not heard from server in 6001ms for sessionid 0x1000001b4720000 (org.apache.zookeeper.ClientCnxn)
[2019-02-12 13:34:57,536] INFO Client session timed out, have not heard from server in 6001ms for sessionid 0x1000001b4720000, closing socket connection and attempting reconnect (org.apache.zookeeper.ClientCnxn)
[2019-02-12 13:34:57,743] INFO Terminating process due to signal SIGINT (org.apache.kafka.common.utils.LoggingSignalHandler)
[2019-02-12 13:34:57,967] INFO Terminating process due to signal SIGINT (org.apache.kafka.common.utils.LoggingSignalHandler)
[2019-02-12 13:34:58,355] INFO Terminating process due to signal SIGINT (org.apache.kafka.common.utils.LoggingSignalHandler)
[2019-02-12 13:34:58,618] INFO Terminating process due to signal SIGINT (org.apache.kafka.common.utils.LoggingSignalHandler)
[2019-02-12 13:34:58,661] INFO Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
[2019-02-12 13:34:58,666] INFO Socket connection established to localhost/127.0.0.1:2181, initiating session (org.apache.zookeeper.ClientCnxn)
[2019-02-12 13:34:58,807] INFO Terminating process due to signal SIGINT (org.apache.kafka.common.utils.LoggingSignalHandler)
[2019-02-12 13:34:58,968] INFO Terminating process due to signal SIGINT (org.apache.kafka.common.utils.LoggingSignalHandler)
[2019-02-12 13:35:04,668] WARN Client session timed out, have not heard from server in 6001ms for sessionid 0x1000001b4720000 (org.apache.zookeeper.ClientCnxn)
[2019-02-12 13:35:04,672] INFO Client session timed out, have not heard from server in 6001ms for sessionid 0x1000001b4720000, closing socket connection and attempting reconnect (org.apache.zookeeper.ClientCnxn)
[2019-02-12 13:35:05,878] INFO Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
[2019-02-12 13:35:05,884] INFO Socket connection established to localhost/127.0.0.1:2181, initiating session (org.apache.zookeeper.ClientCnxn)
[2019-02-12 13:35:08,964] INFO Terminating process due to signal SIGHUP (org.apache.kafka.common.utils.LoggingSignalHandler)
[2019-02-12 13:35:11,885] WARN Client session timed out, have not heard from server in 6001ms for sessionid 0x1000001b4720000 (org.apache.zookeeper.ClientCnxn)
[2019-02-12 13:35:11,886] INFO Client session timed out, have not heard from server in 6001ms for sessionid 0x1000001b4720000, closing socket connection and attempting reconnect (org.apache.zookeeper.ClientCnxn)
[2019-02-12 13:35:13,719] INFO Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
[2019-02-12 13:35:13,720] INFO Socket connection established to localhost/127.0.0.1:2181, initiating session (org.apache.zookeeper.ClientCnxn)
[2019-02-12 13:35:19,721] WARN Client session timed out, have not heard from server in 6000ms for sessionid 0x1000001b4720000 (org.apache.zookeeper.ClientCnxn)
[2019-02-12 13:35:19,721] INFO Client session timed out, have not heard from server in 6000ms for sessionid 0x1000001b4720000, closing socket connection and attempting reconnect (org.apache.zookeeper.ClientCnxn)
[2019-02-12 13:35:21,434] INFO Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
[2019-02-12 13:35:21,435] INFO Socket connection established to localhost/127.0.0.1:2181, initiating session (org.apache.zookeeper.ClientCnxn)
[2019-02-12 13:35:27,436] WARN Client session timed out, have not heard from server in 6001ms for sessionid 0x1000001b4720000 (org.apache.zookeeper.ClientCnxn)
[2019-02-12 13:35:27,437] INFO Client session timed out, have not heard from server in 6001ms for sessionid 0x1000001b4720000, closing socket connection and attempting reconnect (org.apache.zookeeper.ClientCnxn)
[2019-02-12 13:35:29,250] INFO Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
[2019-02-12 13:35:29,253] INFO Socket connection established to localhost/127.0.0.1:2181, initiating session (org.apache.zookeeper.ClientCnxn)
[2019-02-12 13:35:35,254] WARN Client session timed out, have not heard from server in 6000ms for sessionid 0x1000001b4720000 (org.apache.zookeeper.ClientCnxn)
[2019-02-12 13:35:35,254] INFO Client session timed out, have not heard from server in 6000ms for sessionid 0x1000001b4720000, closing socket connection and attempting reconnect (org.apache.zookeeper.ClientCnxn)
[2019-02-12 13:35:37,105] INFO Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
[2019-02-12 13:35:37,106] INFO Socket connection established to localhost/127.0.0.1:2181, initiating session (org.apache.zookeeper.ClientCnxn)
[2019-02-12 13:35:43,107] WARN Client session timed out, have not heard from server in 6001ms for sessionid 0x1000001b4720000 (org.apache.zookeeper.ClientCnxn)
[2019-02-12 13:35:43,108] INFO Client session timed out, have not heard from server in 6001ms for sessionid 0x1000001b4720000, closing socket connection and attempting reconnect (org.apache.zookeeper.ClientCnxn)
[2019-02-12 13:35:44,301] INFO Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
[2019-02-12 13:35:44,304] INFO Socket connection established to localhost/127.0.0.1:2181, initiating session (org.apache.zookeeper.ClientCnxn)
[2019-02-12 13:35:50,304] WARN Client session timed out, have not heard from server in 6001ms for sessionid 0x1000001b4720000 (org.apache.zookeeper.ClientCnxn)
[2019-02-12 13:35:50,304] INFO Client session timed out, have not heard from server in 6001ms for sessionid 0x1000001b4720000, closing socket connection and attempting reconnect (org.apache.zookeeper.ClientCnxn)
[2019-02-12 13:35:52,321] INFO Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
[2019-02-12 13:35:52,323] INFO Socket connection established to localhost/127.0.0.1:2181, initiating session (org.apache.zookeeper.ClientCnxn)
[2019-02-12 13:35:55,792] WARN Unable to reconnect to ZooKeeper service, session 0x1000001b4720000 has expired (org.apache.zookeeper.ClientCnxn)
[2019-02-12 13:35:55,792] INFO Unable to reconnect to ZooKeeper service, session 0x1000001b4720000 has expired, closing socket connection (org.apache.zookeeper.ClientCnxn)
[2019-02-12 13:35:55,794] INFO EventThread shut down for session: 0x1000001b4720000 (org.apache.zookeeper.ClientCnxn)
[2019-02-12 13:35:55,796] INFO [ZooKeeperClient] Session expired. (kafka.zookeeper.ZooKeeperClient)
[2019-02-12 13:35:55,796] WARN Session expired either before or while waiting for connection (kafka.utils.CoreUtils$)
kafka.zookeeper.ZooKeeperClientExpiredException: Session expired either before or while waiting for connection
	at kafka.zookeeper.ZooKeeperClient.$anonfun$waitUntilConnected$3(ZooKeeperClient.scala:276)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.zookeeper.ZooKeeperClient.waitUntilConnected(ZooKeeperClient.scala:264)
	at kafka.zookeeper.ZooKeeperClient.$anonfun$waitUntilConnected$1(ZooKeeperClient.scala:258)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.zookeeper.ZooKeeperClient.waitUntilConnected(ZooKeeperClient.scala:258)
	at kafka.zk.KafkaZkClient.retryRequestsUntilConnected(KafkaZkClient.scala:1615)
	at kafka.zk.KafkaZkClient.kafka$zk$KafkaZkClient$$retryRequestUntilConnected(KafkaZkClient.scala:1589)
	at kafka.zk.KafkaZkClient.getControllerId(KafkaZkClient.scala:998)
	at kafka.server.KafkaServer.doControlledShutdown$1(KafkaServer.scala:469)
	at kafka.server.KafkaServer.controlledShutdown(KafkaServer.scala:545)
	at kafka.server.KafkaServer.$anonfun$shutdown$2(KafkaServer.scala:567)
	at kafka.utils.CoreUtils$.swallow(CoreUtils.scala:86)
	at kafka.server.KafkaServer.shutdown(KafkaServer.scala:567)
	at kafka.server.KafkaServerStartable.shutdown(KafkaServerStartable.scala:48)
	at kafka.Kafka$$anon$1.run(Kafka.scala:72)
[2019-02-12 13:35:55,799] INFO [ZooKeeperClient] Initializing a new session to localhost:2181. (kafka.zookeeper.ZooKeeperClient)
[2019-02-12 13:35:55,798] WARN Attempting to send response via channel for which there is no open connection, connection id 127.0.0.1:9092-127.0.0.1:60858-3 (kafka.network.Processor)
[2019-02-12 13:35:55,801] INFO Initiating client connection, connectString=localhost:2181 sessionTimeout=6000 watcher=kafka.zookeeper.ZooKeeperClient$ZooKeeperClientWatcher$@659a969b (org.apache.zookeeper.ZooKeeper)
[2019-02-12 13:35:55,803] INFO Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
[2019-02-12 13:35:55,803] INFO [/config/changes-event-process-thread]: Shutting down (kafka.common.ZkNodeChangeNotificationListener$ChangeEventProcessThread)
[2019-02-12 13:35:55,807] INFO [/config/changes-event-process-thread]: Stopped (kafka.common.ZkNodeChangeNotificationListener$ChangeEventProcessThread)
[2019-02-12 13:35:55,807] INFO Socket connection established to localhost/127.0.0.1:2181, initiating session (org.apache.zookeeper.ClientCnxn)
[2019-02-12 13:35:55,810] INFO [/config/changes-event-process-thread]: Shutdown completed (kafka.common.ZkNodeChangeNotificationListener$ChangeEventProcessThread)
[2019-02-12 13:35:55,812] INFO [SocketServer brokerId=0] Stopping socket server request processors (kafka.network.SocketServer)
[2019-02-12 13:35:55,815] INFO Creating /brokers/ids/0 (is it secure? false) (kafka.zk.KafkaZkClient)
[2019-02-12 13:35:55,822] INFO Session establishment complete on server localhost/127.0.0.1:2181, sessionid = 0x1000001b4720005, negotiated timeout = 6000 (org.apache.zookeeper.ClientCnxn)
[2019-02-12 13:35:55,827] INFO [SocketServer brokerId=0] Stopped socket server request processors (kafka.network.SocketServer)
[2019-02-12 13:35:55,828] INFO [Kafka Request Handler on Broker 0], shutting down (kafka.server.KafkaRequestHandlerPool)
[2019-02-12 13:35:55,830] INFO Result of znode creation at /brokers/ids/0 is: OK (kafka.zk.KafkaZkClient)
[2019-02-12 13:35:55,830] INFO Registered broker 0 at path /brokers/ids/0 with addresses: ArrayBuffer(EndPoint(DESKTOP-7K1FP8K.localdomain,9092,ListenerName(PLAINTEXT),PLAINTEXT)) (kafka.zk.KafkaZkClient)
[2019-02-12 13:35:55,834] INFO [Kafka Request Handler on Broker 0], shut down completely (kafka.server.KafkaRequestHandlerPool)
[2019-02-12 13:35:55,841] INFO [KafkaApi-0] Shutdown complete. (kafka.server.KafkaApis)
[2019-02-12 13:35:55,842] INFO [ExpirationReaper-0-topic]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2019-02-12 13:35:55,977] INFO [ExpirationReaper-0-topic]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2019-02-12 13:35:55,977] INFO [ExpirationReaper-0-topic]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2019-02-12 13:35:55,987] INFO [TransactionCoordinator id=0] Shutting down. (kafka.coordinator.transaction.TransactionCoordinator)
[2019-02-12 13:35:55,990] INFO [ProducerId Manager 0]: Shutdown complete: last producerId assigned 0 (kafka.coordinator.transaction.ProducerIdManager)
[2019-02-12 13:35:55,992] INFO [Transaction State Manager 0]: Shutdown complete (kafka.coordinator.transaction.TransactionStateManager)
[2019-02-12 13:35:55,992] INFO [Transaction Marker Channel Manager 0]: Shutting down (kafka.coordinator.transaction.TransactionMarkerChannelManager)
[2019-02-12 13:35:55,993] INFO [Transaction Marker Channel Manager 0]: Shutdown completed (kafka.coordinator.transaction.TransactionMarkerChannelManager)
[2019-02-12 13:35:55,993] INFO [Transaction Marker Channel Manager 0]: Stopped (kafka.coordinator.transaction.TransactionMarkerChannelManager)
[2019-02-12 13:35:55,996] INFO [TransactionCoordinator id=0] Shutdown complete. (kafka.coordinator.transaction.TransactionCoordinator)
[2019-02-12 13:35:55,998] INFO [GroupCoordinator 0]: Shutting down. (kafka.coordinator.group.GroupCoordinator)
[2019-02-12 13:35:55,999] INFO [ExpirationReaper-0-Heartbeat]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2019-02-12 13:35:56,001] INFO [ExpirationReaper-0-Heartbeat]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2019-02-12 13:35:56,002] INFO [ExpirationReaper-0-Rebalance]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2019-02-12 13:35:56,001] INFO [ExpirationReaper-0-Heartbeat]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2019-02-12 13:35:56,010] INFO [ExpirationReaper-0-Rebalance]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2019-02-12 13:35:56,010] INFO [ExpirationReaper-0-Rebalance]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2019-02-12 13:35:56,011] INFO [GroupCoordinator 0]: Shutdown complete. (kafka.coordinator.group.GroupCoordinator)
[2019-02-12 13:35:56,013] INFO [ReplicaManager broker=0] Shutting down (kafka.server.ReplicaManager)
[2019-02-12 13:35:56,014] INFO [LogDirFailureHandler]: Shutting down (kafka.server.ReplicaManager$LogDirFailureHandler)
[2019-02-12 13:35:56,014] INFO [LogDirFailureHandler]: Shutdown completed (kafka.server.ReplicaManager$LogDirFailureHandler)
[2019-02-12 13:35:56,014] INFO [LogDirFailureHandler]: Stopped (kafka.server.ReplicaManager$LogDirFailureHandler)
[2019-02-12 13:35:56,016] INFO [ReplicaFetcherManager on broker 0] shutting down (kafka.server.ReplicaFetcherManager)
[2019-02-12 13:35:56,019] INFO [ReplicaFetcherManager on broker 0] shutdown completed (kafka.server.ReplicaFetcherManager)
[2019-02-12 13:35:56,020] INFO [ReplicaAlterLogDirsManager on broker 0] shutting down (kafka.server.ReplicaAlterLogDirsManager)
[2019-02-12 13:35:56,021] INFO [ReplicaAlterLogDirsManager on broker 0] shutdown completed (kafka.server.ReplicaAlterLogDirsManager)
[2019-02-12 13:35:56,021] INFO [ExpirationReaper-0-Fetch]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2019-02-12 13:35:56,211] INFO [ExpirationReaper-0-Fetch]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2019-02-12 13:35:56,211] INFO [ExpirationReaper-0-Fetch]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2019-02-12 13:35:56,212] INFO [ExpirationReaper-0-Produce]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2019-02-12 13:35:56,292] INFO [ExpirationReaper-0-Produce]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2019-02-12 13:35:56,292] INFO [ExpirationReaper-0-Produce]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2019-02-12 13:35:56,294] INFO [ExpirationReaper-0-DeleteRecords]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2019-02-12 13:35:56,407] INFO [ExpirationReaper-0-DeleteRecords]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2019-02-12 13:35:56,407] INFO [ExpirationReaper-0-DeleteRecords]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2019-02-12 13:35:56,433] INFO [ReplicaManager broker=0] Shut down completely (kafka.server.ReplicaManager)
[2019-02-12 13:35:56,436] INFO Shutting down. (kafka.log.LogManager)
[2019-02-12 13:35:56,488] WARN Invalid argument (kafka.utils.CoreUtils$)
java.io.IOException: Invalid argument
	at java.io.RandomAccessFile.setLength(Native Method)
	at kafka.log.AbstractIndex.$anonfun$resize$1(AbstractIndex.scala:186)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:12)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.resize(AbstractIndex.scala:173)
	at kafka.log.AbstractIndex.$anonfun$trimToValidSize$1(AbstractIndex.scala:242)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:12)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.trimToValidSize(AbstractIndex.scala:242)
	at kafka.log.AbstractIndex.close(AbstractIndex.scala:253)
	at kafka.log.LogSegment.$anonfun$close$2(LogSegment.scala:565)
	at kafka.utils.CoreUtils$.swallow(CoreUtils.scala:86)
	at kafka.log.LogSegment.close(LogSegment.scala:565)
	at kafka.log.Log.$anonfun$close$4(Log.scala:714)
	at kafka.log.Log.$anonfun$close$4$adapted(Log.scala:714)
	at scala.collection.Iterator.foreach(Iterator.scala:937)
	at scala.collection.Iterator.foreach$(Iterator.scala:937)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1425)
	at scala.collection.IterableLike.foreach(IterableLike.scala:70)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:69)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at kafka.log.Log.$anonfun$close$3(Log.scala:714)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12)
	at kafka.log.Log.maybeHandleIOException(Log.scala:1927)
	at kafka.log.Log.close(Log.scala:709)
	at kafka.log.LogManager.$anonfun$shutdown$8(LogManager.scala:456)
	at kafka.utils.CoreUtils$$anon$1.run(CoreUtils.scala:63)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[2019-02-12 13:35:56,497] WARN Invalid argument (kafka.utils.CoreUtils$)
java.io.IOException: Invalid argument
	at java.io.RandomAccessFile.setLength(Native Method)
	at kafka.log.AbstractIndex.$anonfun$resize$1(AbstractIndex.scala:186)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:12)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.resize(AbstractIndex.scala:173)
	at kafka.log.TimeIndex.super$resize(TimeIndex.scala:189)
	at kafka.log.TimeIndex.$anonfun$resize$1(TimeIndex.scala:189)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:12)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.TimeIndex.resize(TimeIndex.scala:189)
	at kafka.log.AbstractIndex.$anonfun$trimToValidSize$1(AbstractIndex.scala:242)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:12)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.trimToValidSize(AbstractIndex.scala:242)
	at kafka.log.AbstractIndex.close(AbstractIndex.scala:253)
	at kafka.log.LogSegment.$anonfun$close$3(LogSegment.scala:566)
	at kafka.utils.CoreUtils$.swallow(CoreUtils.scala:86)
	at kafka.log.LogSegment.close(LogSegment.scala:566)
	at kafka.log.Log.$anonfun$close$4(Log.scala:714)
	at kafka.log.Log.$anonfun$close$4$adapted(Log.scala:714)
	at scala.collection.Iterator.foreach(Iterator.scala:937)
	at scala.collection.Iterator.foreach$(Iterator.scala:937)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1425)
	at scala.collection.IterableLike.foreach(IterableLike.scala:70)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:69)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at kafka.log.Log.$anonfun$close$3(Log.scala:714)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12)
	at kafka.log.Log.maybeHandleIOException(Log.scala:1927)
	at kafka.log.Log.close(Log.scala:709)
	at kafka.log.LogManager.$anonfun$shutdown$8(LogManager.scala:456)
	at kafka.utils.CoreUtils$$anon$1.run(CoreUtils.scala:63)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[2019-02-12 13:35:56,511] WARN Invalid argument (kafka.utils.CoreUtils$)
java.io.IOException: Invalid argument
	at java.io.RandomAccessFile.setLength(Native Method)
	at kafka.log.AbstractIndex.$anonfun$resize$1(AbstractIndex.scala:186)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:12)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.resize(AbstractIndex.scala:173)
	at kafka.log.AbstractIndex.$anonfun$trimToValidSize$1(AbstractIndex.scala:242)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:12)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.trimToValidSize(AbstractIndex.scala:242)
	at kafka.log.AbstractIndex.close(AbstractIndex.scala:253)
	at kafka.log.LogSegment.$anonfun$close$2(LogSegment.scala:565)
	at kafka.utils.CoreUtils$.swallow(CoreUtils.scala:86)
	at kafka.log.LogSegment.close(LogSegment.scala:565)
	at kafka.log.Log.$anonfun$close$4(Log.scala:714)
	at kafka.log.Log.$anonfun$close$4$adapted(Log.scala:714)
	at scala.collection.Iterator.foreach(Iterator.scala:937)
	at scala.collection.Iterator.foreach$(Iterator.scala:937)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1425)
	at scala.collection.IterableLike.foreach(IterableLike.scala:70)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:69)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at kafka.log.Log.$anonfun$close$3(Log.scala:714)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12)
	at kafka.log.Log.maybeHandleIOException(Log.scala:1927)
	at kafka.log.Log.close(Log.scala:709)
	at kafka.log.LogManager.$anonfun$shutdown$8(LogManager.scala:456)
	at kafka.utils.CoreUtils$$anon$1.run(CoreUtils.scala:63)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[2019-02-12 13:35:56,521] WARN Invalid argument (kafka.utils.CoreUtils$)
java.io.IOException: Invalid argument
	at java.io.RandomAccessFile.setLength(Native Method)
	at kafka.log.AbstractIndex.$anonfun$resize$1(AbstractIndex.scala:186)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:12)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.resize(AbstractIndex.scala:173)
	at kafka.log.TimeIndex.super$resize(TimeIndex.scala:189)
	at kafka.log.TimeIndex.$anonfun$resize$1(TimeIndex.scala:189)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:12)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.TimeIndex.resize(TimeIndex.scala:189)
	at kafka.log.AbstractIndex.$anonfun$trimToValidSize$1(AbstractIndex.scala:242)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:12)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.trimToValidSize(AbstractIndex.scala:242)
	at kafka.log.AbstractIndex.close(AbstractIndex.scala:253)
	at kafka.log.LogSegment.$anonfun$close$3(LogSegment.scala:566)
	at kafka.utils.CoreUtils$.swallow(CoreUtils.scala:86)
	at kafka.log.LogSegment.close(LogSegment.scala:566)
	at kafka.log.Log.$anonfun$close$4(Log.scala:714)
	at kafka.log.Log.$anonfun$close$4$adapted(Log.scala:714)
	at scala.collection.Iterator.foreach(Iterator.scala:937)
	at scala.collection.Iterator.foreach$(Iterator.scala:937)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1425)
	at scala.collection.IterableLike.foreach(IterableLike.scala:70)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:69)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at kafka.log.Log.$anonfun$close$3(Log.scala:714)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12)
	at kafka.log.Log.maybeHandleIOException(Log.scala:1927)
	at kafka.log.Log.close(Log.scala:709)
	at kafka.log.LogManager.$anonfun$shutdown$8(LogManager.scala:456)
	at kafka.utils.CoreUtils$$anon$1.run(CoreUtils.scala:63)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[2019-02-12 13:35:56,532] WARN Invalid argument (kafka.utils.CoreUtils$)
java.io.IOException: Invalid argument
	at java.io.RandomAccessFile.setLength(Native Method)
	at kafka.log.AbstractIndex.$anonfun$resize$1(AbstractIndex.scala:186)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:12)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.resize(AbstractIndex.scala:173)
	at kafka.log.AbstractIndex.$anonfun$trimToValidSize$1(AbstractIndex.scala:242)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:12)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.trimToValidSize(AbstractIndex.scala:242)
	at kafka.log.AbstractIndex.close(AbstractIndex.scala:253)
	at kafka.log.LogSegment.$anonfun$close$2(LogSegment.scala:565)
	at kafka.utils.CoreUtils$.swallow(CoreUtils.scala:86)
	at kafka.log.LogSegment.close(LogSegment.scala:565)
	at kafka.log.Log.$anonfun$close$4(Log.scala:714)
	at kafka.log.Log.$anonfun$close$4$adapted(Log.scala:714)
	at scala.collection.Iterator.foreach(Iterator.scala:937)
	at scala.collection.Iterator.foreach$(Iterator.scala:937)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1425)
	at scala.collection.IterableLike.foreach(IterableLike.scala:70)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:69)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at kafka.log.Log.$anonfun$close$3(Log.scala:714)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12)
	at kafka.log.Log.maybeHandleIOException(Log.scala:1927)
	at kafka.log.Log.close(Log.scala:709)
	at kafka.log.LogManager.$anonfun$shutdown$8(LogManager.scala:456)
	at kafka.utils.CoreUtils$$anon$1.run(CoreUtils.scala:63)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[2019-02-12 13:35:56,541] WARN Invalid argument (kafka.utils.CoreUtils$)
java.io.IOException: Invalid argument
	at java.io.RandomAccessFile.setLength(Native Method)
	at kafka.log.AbstractIndex.$anonfun$resize$1(AbstractIndex.scala:186)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:12)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.resize(AbstractIndex.scala:173)
	at kafka.log.TimeIndex.super$resize(TimeIndex.scala:189)
	at kafka.log.TimeIndex.$anonfun$resize$1(TimeIndex.scala:189)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:12)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.TimeIndex.resize(TimeIndex.scala:189)
	at kafka.log.AbstractIndex.$anonfun$trimToValidSize$1(AbstractIndex.scala:242)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:12)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.trimToValidSize(AbstractIndex.scala:242)
	at kafka.log.AbstractIndex.close(AbstractIndex.scala:253)
	at kafka.log.LogSegment.$anonfun$close$3(LogSegment.scala:566)
	at kafka.utils.CoreUtils$.swallow(CoreUtils.scala:86)
	at kafka.log.LogSegment.close(LogSegment.scala:566)
	at kafka.log.Log.$anonfun$close$4(Log.scala:714)
	at kafka.log.Log.$anonfun$close$4$adapted(Log.scala:714)
	at scala.collection.Iterator.foreach(Iterator.scala:937)
	at scala.collection.Iterator.foreach$(Iterator.scala:937)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1425)
	at scala.collection.IterableLike.foreach(IterableLike.scala:70)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:69)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at kafka.log.Log.$anonfun$close$3(Log.scala:714)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12)
	at kafka.log.Log.maybeHandleIOException(Log.scala:1927)
	at kafka.log.Log.close(Log.scala:709)
	at kafka.log.LogManager.$anonfun$shutdown$8(LogManager.scala:456)
	at kafka.utils.CoreUtils$$anon$1.run(CoreUtils.scala:63)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[2019-02-12 13:35:56,550] WARN Invalid argument (kafka.utils.CoreUtils$)
java.io.IOException: Invalid argument
	at java.io.RandomAccessFile.setLength(Native Method)
	at kafka.log.AbstractIndex.$anonfun$resize$1(AbstractIndex.scala:186)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:12)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.resize(AbstractIndex.scala:173)
	at kafka.log.AbstractIndex.$anonfun$trimToValidSize$1(AbstractIndex.scala:242)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:12)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.trimToValidSize(AbstractIndex.scala:242)
	at kafka.log.AbstractIndex.close(AbstractIndex.scala:253)
	at kafka.log.LogSegment.$anonfun$close$2(LogSegment.scala:565)
	at kafka.utils.CoreUtils$.swallow(CoreUtils.scala:86)
	at kafka.log.LogSegment.close(LogSegment.scala:565)
	at kafka.log.Log.$anonfun$close$4(Log.scala:714)
	at kafka.log.Log.$anonfun$close$4$adapted(Log.scala:714)
	at scala.collection.Iterator.foreach(Iterator.scala:937)
	at scala.collection.Iterator.foreach$(Iterator.scala:937)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1425)
	at scala.collection.IterableLike.foreach(IterableLike.scala:70)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:69)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at kafka.log.Log.$anonfun$close$3(Log.scala:714)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12)
	at kafka.log.Log.maybeHandleIOException(Log.scala:1927)
	at kafka.log.Log.close(Log.scala:709)
	at kafka.log.LogManager.$anonfun$shutdown$8(LogManager.scala:456)
	at kafka.utils.CoreUtils$$anon$1.run(CoreUtils.scala:63)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[2019-02-12 13:35:56,558] WARN Invalid argument (kafka.utils.CoreUtils$)
java.io.IOException: Invalid argument
	at java.io.RandomAccessFile.setLength(Native Method)
	at kafka.log.AbstractIndex.$anonfun$resize$1(AbstractIndex.scala:186)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:12)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.resize(AbstractIndex.scala:173)
	at kafka.log.TimeIndex.super$resize(TimeIndex.scala:189)
	at kafka.log.TimeIndex.$anonfun$resize$1(TimeIndex.scala:189)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:12)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.TimeIndex.resize(TimeIndex.scala:189)
	at kafka.log.AbstractIndex.$anonfun$trimToValidSize$1(AbstractIndex.scala:242)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:12)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.trimToValidSize(AbstractIndex.scala:242)
	at kafka.log.AbstractIndex.close(AbstractIndex.scala:253)
	at kafka.log.LogSegment.$anonfun$close$3(LogSegment.scala:566)
	at kafka.utils.CoreUtils$.swallow(CoreUtils.scala:86)
	at kafka.log.LogSegment.close(LogSegment.scala:566)
	at kafka.log.Log.$anonfun$close$4(Log.scala:714)
	at kafka.log.Log.$anonfun$close$4$adapted(Log.scala:714)
	at scala.collection.Iterator.foreach(Iterator.scala:937)
	at scala.collection.Iterator.foreach$(Iterator.scala:937)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1425)
	at scala.collection.IterableLike.foreach(IterableLike.scala:70)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:69)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at kafka.log.Log.$anonfun$close$3(Log.scala:714)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12)
	at kafka.log.Log.maybeHandleIOException(Log.scala:1927)
	at kafka.log.Log.close(Log.scala:709)
	at kafka.log.LogManager.$anonfun$shutdown$8(LogManager.scala:456)
	at kafka.utils.CoreUtils$$anon$1.run(CoreUtils.scala:63)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[2019-02-12 13:35:56,567] WARN Invalid argument (kafka.utils.CoreUtils$)
java.io.IOException: Invalid argument
	at java.io.RandomAccessFile.setLength(Native Method)
	at kafka.log.AbstractIndex.$anonfun$resize$1(AbstractIndex.scala:186)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:12)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.resize(AbstractIndex.scala:173)
	at kafka.log.AbstractIndex.$anonfun$trimToValidSize$1(AbstractIndex.scala:242)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:12)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.trimToValidSize(AbstractIndex.scala:242)
	at kafka.log.AbstractIndex.close(AbstractIndex.scala:253)
	at kafka.log.LogSegment.$anonfun$close$2(LogSegment.scala:565)
	at kafka.utils.CoreUtils$.swallow(CoreUtils.scala:86)
	at kafka.log.LogSegment.close(LogSegment.scala:565)
	at kafka.log.Log.$anonfun$close$4(Log.scala:714)
	at kafka.log.Log.$anonfun$close$4$adapted(Log.scala:714)
	at scala.collection.Iterator.foreach(Iterator.scala:937)
	at scala.collection.Iterator.foreach$(Iterator.scala:937)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1425)
	at scala.collection.IterableLike.foreach(IterableLike.scala:70)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:69)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at kafka.log.Log.$anonfun$close$3(Log.scala:714)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12)
	at kafka.log.Log.maybeHandleIOException(Log.scala:1927)
	at kafka.log.Log.close(Log.scala:709)
	at kafka.log.LogManager.$anonfun$shutdown$8(LogManager.scala:456)
	at kafka.utils.CoreUtils$$anon$1.run(CoreUtils.scala:63)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[2019-02-12 13:35:56,575] WARN Invalid argument (kafka.utils.CoreUtils$)
java.io.IOException: Invalid argument
	at java.io.RandomAccessFile.setLength(Native Method)
	at kafka.log.AbstractIndex.$anonfun$resize$1(AbstractIndex.scala:186)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:12)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.resize(AbstractIndex.scala:173)
	at kafka.log.TimeIndex.super$resize(TimeIndex.scala:189)
	at kafka.log.TimeIndex.$anonfun$resize$1(TimeIndex.scala:189)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:12)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.TimeIndex.resize(TimeIndex.scala:189)
	at kafka.log.AbstractIndex.$anonfun$trimToValidSize$1(AbstractIndex.scala:242)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:12)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.trimToValidSize(AbstractIndex.scala:242)
	at kafka.log.AbstractIndex.close(AbstractIndex.scala:253)
	at kafka.log.LogSegment.$anonfun$close$3(LogSegment.scala:566)
	at kafka.utils.CoreUtils$.swallow(CoreUtils.scala:86)
	at kafka.log.LogSegment.close(LogSegment.scala:566)
	at kafka.log.Log.$anonfun$close$4(Log.scala:714)
	at kafka.log.Log.$anonfun$close$4$adapted(Log.scala:714)
	at scala.collection.Iterator.foreach(Iterator.scala:937)
	at scala.collection.Iterator.foreach$(Iterator.scala:937)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1425)
	at scala.collection.IterableLike.foreach(IterableLike.scala:70)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:69)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at kafka.log.Log.$anonfun$close$3(Log.scala:714)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12)
	at kafka.log.Log.maybeHandleIOException(Log.scala:1927)
	at kafka.log.Log.close(Log.scala:709)
	at kafka.log.LogManager.$anonfun$shutdown$8(LogManager.scala:456)
	at kafka.utils.CoreUtils$$anon$1.run(CoreUtils.scala:63)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[2019-02-12 13:35:56,585] WARN Invalid argument (kafka.utils.CoreUtils$)
java.io.IOException: Invalid argument
	at java.io.RandomAccessFile.setLength(Native Method)
	at kafka.log.AbstractIndex.$anonfun$resize$1(AbstractIndex.scala:186)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:12)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.resize(AbstractIndex.scala:173)
	at kafka.log.AbstractIndex.$anonfun$trimToValidSize$1(AbstractIndex.scala:242)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:12)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.trimToValidSize(AbstractIndex.scala:242)
	at kafka.log.AbstractIndex.close(AbstractIndex.scala:253)
	at kafka.log.LogSegment.$anonfun$close$2(LogSegment.scala:565)
	at kafka.utils.CoreUtils$.swallow(CoreUtils.scala:86)
	at kafka.log.LogSegment.close(LogSegment.scala:565)
	at kafka.log.Log.$anonfun$close$4(Log.scala:714)
	at kafka.log.Log.$anonfun$close$4$adapted(Log.scala:714)
	at scala.collection.Iterator.foreach(Iterator.scala:937)
	at scala.collection.Iterator.foreach$(Iterator.scala:937)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1425)
	at scala.collection.IterableLike.foreach(IterableLike.scala:70)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:69)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at kafka.log.Log.$anonfun$close$3(Log.scala:714)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12)
	at kafka.log.Log.maybeHandleIOException(Log.scala:1927)
	at kafka.log.Log.close(Log.scala:709)
	at kafka.log.LogManager.$anonfun$shutdown$8(LogManager.scala:456)
	at kafka.utils.CoreUtils$$anon$1.run(CoreUtils.scala:63)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[2019-02-12 13:35:56,592] WARN Invalid argument (kafka.utils.CoreUtils$)
java.io.IOException: Invalid argument
	at java.io.RandomAccessFile.setLength(Native Method)
	at kafka.log.AbstractIndex.$anonfun$resize$1(AbstractIndex.scala:186)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:12)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.resize(AbstractIndex.scala:173)
	at kafka.log.TimeIndex.super$resize(TimeIndex.scala:189)
	at kafka.log.TimeIndex.$anonfun$resize$1(TimeIndex.scala:189)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:12)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.TimeIndex.resize(TimeIndex.scala:189)
	at kafka.log.AbstractIndex.$anonfun$trimToValidSize$1(AbstractIndex.scala:242)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:12)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.trimToValidSize(AbstractIndex.scala:242)
	at kafka.log.AbstractIndex.close(AbstractIndex.scala:253)
	at kafka.log.LogSegment.$anonfun$close$3(LogSegment.scala:566)
	at kafka.utils.CoreUtils$.swallow(CoreUtils.scala:86)
	at kafka.log.LogSegment.close(LogSegment.scala:566)
	at kafka.log.Log.$anonfun$close$4(Log.scala:714)
	at kafka.log.Log.$anonfun$close$4$adapted(Log.scala:714)
	at scala.collection.Iterator.foreach(Iterator.scala:937)
	at scala.collection.Iterator.foreach$(Iterator.scala:937)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1425)
	at scala.collection.IterableLike.foreach(IterableLike.scala:70)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:69)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at kafka.log.Log.$anonfun$close$3(Log.scala:714)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12)
	at kafka.log.Log.maybeHandleIOException(Log.scala:1927)
	at kafka.log.Log.close(Log.scala:709)
	at kafka.log.LogManager.$anonfun$shutdown$8(LogManager.scala:456)
	at kafka.utils.CoreUtils$$anon$1.run(CoreUtils.scala:63)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[2019-02-12 13:35:56,609] INFO [ProducerStateManager partition=__consumer_offsets-7] Writing producer snapshot at offset 2 (kafka.log.ProducerStateManager)
[2019-02-12 13:35:56,622] WARN Invalid argument (kafka.utils.CoreUtils$)
java.io.IOException: Invalid argument
	at java.io.RandomAccessFile.setLength(Native Method)
	at kafka.log.AbstractIndex.$anonfun$resize$1(AbstractIndex.scala:186)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:12)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.resize(AbstractIndex.scala:173)
	at kafka.log.AbstractIndex.$anonfun$trimToValidSize$1(AbstractIndex.scala:242)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:12)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.trimToValidSize(AbstractIndex.scala:242)
	at kafka.log.AbstractIndex.close(AbstractIndex.scala:253)
	at kafka.log.LogSegment.$anonfun$close$2(LogSegment.scala:565)
	at kafka.utils.CoreUtils$.swallow(CoreUtils.scala:86)
	at kafka.log.LogSegment.close(LogSegment.scala:565)
	at kafka.log.Log.$anonfun$close$4(Log.scala:714)
	at kafka.log.Log.$anonfun$close$4$adapted(Log.scala:714)
	at scala.collection.Iterator.foreach(Iterator.scala:937)
	at scala.collection.Iterator.foreach$(Iterator.scala:937)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1425)
	at scala.collection.IterableLike.foreach(IterableLike.scala:70)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:69)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at kafka.log.Log.$anonfun$close$3(Log.scala:714)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12)
	at kafka.log.Log.maybeHandleIOException(Log.scala:1927)
	at kafka.log.Log.close(Log.scala:709)
	at kafka.log.LogManager.$anonfun$shutdown$8(LogManager.scala:456)
	at kafka.utils.CoreUtils$$anon$1.run(CoreUtils.scala:63)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[2019-02-12 13:35:56,627] WARN Invalid argument (kafka.utils.CoreUtils$)
java.io.IOException: Invalid argument
	at java.io.RandomAccessFile.setLength(Native Method)
	at kafka.log.AbstractIndex.$anonfun$resize$1(AbstractIndex.scala:186)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:12)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.resize(AbstractIndex.scala:173)
	at kafka.log.TimeIndex.super$resize(TimeIndex.scala:189)
	at kafka.log.TimeIndex.$anonfun$resize$1(TimeIndex.scala:189)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:12)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.TimeIndex.resize(TimeIndex.scala:189)
	at kafka.log.AbstractIndex.$anonfun$trimToValidSize$1(AbstractIndex.scala:242)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:12)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.trimToValidSize(AbstractIndex.scala:242)
	at kafka.log.AbstractIndex.close(AbstractIndex.scala:253)
	at kafka.log.LogSegment.$anonfun$close$3(LogSegment.scala:566)
	at kafka.utils.CoreUtils$.swallow(CoreUtils.scala:86)
	at kafka.log.LogSegment.close(LogSegment.scala:566)
	at kafka.log.Log.$anonfun$close$4(Log.scala:714)
	at kafka.log.Log.$anonfun$close$4$adapted(Log.scala:714)
	at scala.collection.Iterator.foreach(Iterator.scala:937)
	at scala.collection.Iterator.foreach$(Iterator.scala:937)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1425)
	at scala.collection.IterableLike.foreach(IterableLike.scala:70)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:69)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at kafka.log.Log.$anonfun$close$3(Log.scala:714)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12)
	at kafka.log.Log.maybeHandleIOException(Log.scala:1927)
	at kafka.log.Log.close(Log.scala:709)
	at kafka.log.LogManager.$anonfun$shutdown$8(LogManager.scala:456)
	at kafka.utils.CoreUtils$$anon$1.run(CoreUtils.scala:63)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[2019-02-12 13:35:56,640] WARN Invalid argument (kafka.utils.CoreUtils$)
java.io.IOException: Invalid argument
	at java.io.RandomAccessFile.setLength(Native Method)
	at kafka.log.AbstractIndex.$anonfun$resize$1(AbstractIndex.scala:186)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:12)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.resize(AbstractIndex.scala:173)
	at kafka.log.AbstractIndex.$anonfun$trimToValidSize$1(AbstractIndex.scala:242)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:12)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.trimToValidSize(AbstractIndex.scala:242)
	at kafka.log.AbstractIndex.close(AbstractIndex.scala:253)
	at kafka.log.LogSegment.$anonfun$close$2(LogSegment.scala:565)
	at kafka.utils.CoreUtils$.swallow(CoreUtils.scala:86)
	at kafka.log.LogSegment.close(LogSegment.scala:565)
	at kafka.log.Log.$anonfun$close$4(Log.scala:714)
	at kafka.log.Log.$anonfun$close$4$adapted(Log.scala:714)
	at scala.collection.Iterator.foreach(Iterator.scala:937)
	at scala.collection.Iterator.foreach$(Iterator.scala:937)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1425)
	at scala.collection.IterableLike.foreach(IterableLike.scala:70)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:69)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at kafka.log.Log.$anonfun$close$3(Log.scala:714)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12)
	at kafka.log.Log.maybeHandleIOException(Log.scala:1927)
	at kafka.log.Log.close(Log.scala:709)
	at kafka.log.LogManager.$anonfun$shutdown$8(LogManager.scala:456)
	at kafka.utils.CoreUtils$$anon$1.run(CoreUtils.scala:63)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[2019-02-12 13:35:56,646] WARN Invalid argument (kafka.utils.CoreUtils$)
java.io.IOException: Invalid argument
	at java.io.RandomAccessFile.setLength(Native Method)
	at kafka.log.AbstractIndex.$anonfun$resize$1(AbstractIndex.scala:186)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:12)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.resize(AbstractIndex.scala:173)
	at kafka.log.TimeIndex.super$resize(TimeIndex.scala:189)
	at kafka.log.TimeIndex.$anonfun$resize$1(TimeIndex.scala:189)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:12)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.TimeIndex.resize(TimeIndex.scala:189)
	at kafka.log.AbstractIndex.$anonfun$trimToValidSize$1(AbstractIndex.scala:242)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:12)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.trimToValidSize(AbstractIndex.scala:242)
	at kafka.log.AbstractIndex.close(AbstractIndex.scala:253)
	at kafka.log.LogSegment.$anonfun$close$3(LogSegment.scala:566)
	at kafka.utils.CoreUtils$.swallow(CoreUtils.scala:86)
	at kafka.log.LogSegment.close(LogSegment.scala:566)
	at kafka.log.Log.$anonfun$close$4(Log.scala:714)
	at kafka.log.Log.$anonfun$close$4$adapted(Log.scala:714)
	at scala.collection.Iterator.foreach(Iterator.scala:937)
	at scala.collection.Iterator.foreach$(Iterator.scala:937)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1425)
	at scala.collection.IterableLike.foreach(IterableLike.scala:70)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:69)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at kafka.log.Log.$anonfun$close$3(Log.scala:714)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12)
	at kafka.log.Log.maybeHandleIOException(Log.scala:1927)
	at kafka.log.Log.close(Log.scala:709)
	at kafka.log.LogManager.$anonfun$shutdown$8(LogManager.scala:456)
	at kafka.utils.CoreUtils$$anon$1.run(CoreUtils.scala:63)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[2019-02-12 13:35:56,658] WARN Invalid argument (kafka.utils.CoreUtils$)
java.io.IOException: Invalid argument
	at java.io.RandomAccessFile.setLength(Native Method)
	at kafka.log.AbstractIndex.$anonfun$resize$1(AbstractIndex.scala:186)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:12)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.resize(AbstractIndex.scala:173)
	at kafka.log.AbstractIndex.$anonfun$trimToValidSize$1(AbstractIndex.scala:242)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:12)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.trimToValidSize(AbstractIndex.scala:242)
	at kafka.log.AbstractIndex.close(AbstractIndex.scala:253)
	at kafka.log.LogSegment.$anonfun$close$2(LogSegment.scala:565)
	at kafka.utils.CoreUtils$.swallow(CoreUtils.scala:86)
	at kafka.log.LogSegment.close(LogSegment.scala:565)
	at kafka.log.Log.$anonfun$close$4(Log.scala:714)
	at kafka.log.Log.$anonfun$close$4$adapted(Log.scala:714)
	at scala.collection.Iterator.foreach(Iterator.scala:937)
	at scala.collection.Iterator.foreach$(Iterator.scala:937)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1425)
	at scala.collection.IterableLike.foreach(IterableLike.scala:70)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:69)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at kafka.log.Log.$anonfun$close$3(Log.scala:714)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12)
	at kafka.log.Log.maybeHandleIOException(Log.scala:1927)
	at kafka.log.Log.close(Log.scala:709)
	at kafka.log.LogManager.$anonfun$shutdown$8(LogManager.scala:456)
	at kafka.utils.CoreUtils$$anon$1.run(CoreUtils.scala:63)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[2019-02-12 13:35:56,664] WARN Invalid argument (kafka.utils.CoreUtils$)
java.io.IOException: Invalid argument
	at java.io.RandomAccessFile.setLength(Native Method)
	at kafka.log.AbstractIndex.$anonfun$resize$1(AbstractIndex.scala:186)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:12)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.resize(AbstractIndex.scala:173)
	at kafka.log.TimeIndex.super$resize(TimeIndex.scala:189)
	at kafka.log.TimeIndex.$anonfun$resize$1(TimeIndex.scala:189)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:12)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.TimeIndex.resize(TimeIndex.scala:189)
	at kafka.log.AbstractIndex.$anonfun$trimToValidSize$1(AbstractIndex.scala:242)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:12)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.trimToValidSize(AbstractIndex.scala:242)
	at kafka.log.AbstractIndex.close(AbstractIndex.scala:253)
	at kafka.log.LogSegment.$anonfun$close$3(LogSegment.scala:566)
	at kafka.utils.CoreUtils$.swallow(CoreUtils.scala:86)
	at kafka.log.LogSegment.close(LogSegment.scala:566)
	at kafka.log.Log.$anonfun$close$4(Log.scala:714)
	at kafka.log.Log.$anonfun$close$4$adapted(Log.scala:714)
	at scala.collection.Iterator.foreach(Iterator.scala:937)
	at scala.collection.Iterator.foreach$(Iterator.scala:937)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1425)
	at scala.collection.IterableLike.foreach(IterableLike.scala:70)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:69)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at kafka.log.Log.$anonfun$close$3(Log.scala:714)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12)
	at kafka.log.Log.maybeHandleIOException(Log.scala:1927)
	at kafka.log.Log.close(Log.scala:709)
	at kafka.log.LogManager.$anonfun$shutdown$8(LogManager.scala:456)
	at kafka.utils.CoreUtils$$anon$1.run(CoreUtils.scala:63)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[2019-02-12 13:35:56,670] WARN Invalid argument (kafka.utils.CoreUtils$)
java.io.IOException: Invalid argument
	at java.io.RandomAccessFile.setLength(Native Method)
	at kafka.log.AbstractIndex.$anonfun$resize$1(AbstractIndex.scala:186)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:12)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.resize(AbstractIndex.scala:173)
	at kafka.log.AbstractIndex.$anonfun$trimToValidSize$1(AbstractIndex.scala:242)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:12)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.trimToValidSize(AbstractIndex.scala:242)
	at kafka.log.AbstractIndex.close(AbstractIndex.scala:253)
	at kafka.log.LogSegment.$anonfun$close$2(LogSegment.scala:565)
	at kafka.utils.CoreUtils$.swallow(CoreUtils.scala:86)
	at kafka.log.LogSegment.close(LogSegment.scala:565)
	at kafka.log.Log.$anonfun$close$4(Log.scala:714)
	at kafka.log.Log.$anonfun$close$4$adapted(Log.scala:714)
	at scala.collection.Iterator.foreach(Iterator.scala:937)
	at scala.collection.Iterator.foreach$(Iterator.scala:937)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1425)
	at scala.collection.IterableLike.foreach(IterableLike.scala:70)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:69)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at kafka.log.Log.$anonfun$close$3(Log.scala:714)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12)
	at kafka.log.Log.maybeHandleIOException(Log.scala:1927)
	at kafka.log.Log.close(Log.scala:709)
	at kafka.log.LogManager.$anonfun$shutdown$8(LogManager.scala:456)
	at kafka.utils.CoreUtils$$anon$1.run(CoreUtils.scala:63)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[2019-02-12 13:35:56,676] WARN Invalid argument (kafka.utils.CoreUtils$)
java.io.IOException: Invalid argument
	at java.io.RandomAccessFile.setLength(Native Method)
	at kafka.log.AbstractIndex.$anonfun$resize$1(AbstractIndex.scala:186)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:12)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.resize(AbstractIndex.scala:173)
	at kafka.log.TimeIndex.super$resize(TimeIndex.scala:189)
	at kafka.log.TimeIndex.$anonfun$resize$1(TimeIndex.scala:189)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:12)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.TimeIndex.resize(TimeIndex.scala:189)
	at kafka.log.AbstractIndex.$anonfun$trimToValidSize$1(AbstractIndex.scala:242)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:12)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.trimToValidSize(AbstractIndex.scala:242)
	at kafka.log.AbstractIndex.close(AbstractIndex.scala:253)
	at kafka.log.LogSegment.$anonfun$close$3(LogSegment.scala:566)
	at kafka.utils.CoreUtils$.swallow(CoreUtils.scala:86)
	at kafka.log.LogSegment.close(LogSegment.scala:566)
	at kafka.log.Log.$anonfun$close$4(Log.scala:714)
	at kafka.log.Log.$anonfun$close$4$adapted(Log.scala:714)
	at scala.collection.Iterator.foreach(Iterator.scala:937)
	at scala.collection.Iterator.foreach$(Iterator.scala:937)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1425)
	at scala.collection.IterableLike.foreach(IterableLike.scala:70)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:69)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at kafka.log.Log.$anonfun$close$3(Log.scala:714)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12)
	at kafka.log.Log.maybeHandleIOException(Log.scala:1927)
	at kafka.log.Log.close(Log.scala:709)
	at kafka.log.LogManager.$anonfun$shutdown$8(LogManager.scala:456)
	at kafka.utils.CoreUtils$$anon$1.run(CoreUtils.scala:63)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[2019-02-12 13:35:56,683] WARN Invalid argument (kafka.utils.CoreUtils$)
java.io.IOException: Invalid argument
	at java.io.RandomAccessFile.setLength(Native Method)
	at kafka.log.AbstractIndex.$anonfun$resize$1(AbstractIndex.scala:186)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:12)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.resize(AbstractIndex.scala:173)
	at kafka.log.AbstractIndex.$anonfun$trimToValidSize$1(AbstractIndex.scala:242)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:12)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.trimToValidSize(AbstractIndex.scala:242)
	at kafka.log.AbstractIndex.close(AbstractIndex.scala:253)
	at kafka.log.LogSegment.$anonfun$close$2(LogSegment.scala:565)
	at kafka.utils.CoreUtils$.swallow(CoreUtils.scala:86)
	at kafka.log.LogSegment.close(LogSegment.scala:565)
	at kafka.log.Log.$anonfun$close$4(Log.scala:714)
	at kafka.log.Log.$anonfun$close$4$adapted(Log.scala:714)
	at scala.collection.Iterator.foreach(Iterator.scala:937)
	at scala.collection.Iterator.foreach$(Iterator.scala:937)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1425)
	at scala.collection.IterableLike.foreach(IterableLike.scala:70)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:69)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at kafka.log.Log.$anonfun$close$3(Log.scala:714)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12)
	at kafka.log.Log.maybeHandleIOException(Log.scala:1927)
	at kafka.log.Log.close(Log.scala:709)
	at kafka.log.LogManager.$anonfun$shutdown$8(LogManager.scala:456)
	at kafka.utils.CoreUtils$$anon$1.run(CoreUtils.scala:63)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[2019-02-12 13:35:56,689] WARN Invalid argument (kafka.utils.CoreUtils$)
java.io.IOException: Invalid argument
	at java.io.RandomAccessFile.setLength(Native Method)
	at kafka.log.AbstractIndex.$anonfun$resize$1(AbstractIndex.scala:186)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:12)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.resize(AbstractIndex.scala:173)
	at kafka.log.TimeIndex.super$resize(TimeIndex.scala:189)
	at kafka.log.TimeIndex.$anonfun$resize$1(TimeIndex.scala:189)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:12)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.TimeIndex.resize(TimeIndex.scala:189)
	at kafka.log.AbstractIndex.$anonfun$trimToValidSize$1(AbstractIndex.scala:242)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:12)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.trimToValidSize(AbstractIndex.scala:242)
	at kafka.log.AbstractIndex.close(AbstractIndex.scala:253)
	at kafka.log.LogSegment.$anonfun$close$3(LogSegment.scala:566)
	at kafka.utils.CoreUtils$.swallow(CoreUtils.scala:86)
	at kafka.log.LogSegment.close(LogSegment.scala:566)
	at kafka.log.Log.$anonfun$close$4(Log.scala:714)
	at kafka.log.Log.$anonfun$close$4$adapted(Log.scala:714)
	at scala.collection.Iterator.foreach(Iterator.scala:937)
	at scala.collection.Iterator.foreach$(Iterator.scala:937)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1425)
	at scala.collection.IterableLike.foreach(IterableLike.scala:70)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:69)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at kafka.log.Log.$anonfun$close$3(Log.scala:714)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12)
	at kafka.log.Log.maybeHandleIOException(Log.scala:1927)
	at kafka.log.Log.close(Log.scala:709)
	at kafka.log.LogManager.$anonfun$shutdown$8(LogManager.scala:456)
	at kafka.utils.CoreUtils$$anon$1.run(CoreUtils.scala:63)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[2019-02-12 13:35:56,696] WARN Invalid argument (kafka.utils.CoreUtils$)
java.io.IOException: Invalid argument
	at java.io.RandomAccessFile.setLength(Native Method)
	at kafka.log.AbstractIndex.$anonfun$resize$1(AbstractIndex.scala:186)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:12)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.resize(AbstractIndex.scala:173)
	at kafka.log.AbstractIndex.$anonfun$trimToValidSize$1(AbstractIndex.scala:242)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:12)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.trimToValidSize(AbstractIndex.scala:242)
	at kafka.log.AbstractIndex.close(AbstractIndex.scala:253)
	at kafka.log.LogSegment.$anonfun$close$2(LogSegment.scala:565)
	at kafka.utils.CoreUtils$.swallow(CoreUtils.scala:86)
	at kafka.log.LogSegment.close(LogSegment.scala:565)
	at kafka.log.Log.$anonfun$close$4(Log.scala:714)
	at kafka.log.Log.$anonfun$close$4$adapted(Log.scala:714)
	at scala.collection.Iterator.foreach(Iterator.scala:937)
	at scala.collection.Iterator.foreach$(Iterator.scala:937)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1425)
	at scala.collection.IterableLike.foreach(IterableLike.scala:70)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:69)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at kafka.log.Log.$anonfun$close$3(Log.scala:714)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12)
	at kafka.log.Log.maybeHandleIOException(Log.scala:1927)
	at kafka.log.Log.close(Log.scala:709)
	at kafka.log.LogManager.$anonfun$shutdown$8(LogManager.scala:456)
	at kafka.utils.CoreUtils$$anon$1.run(CoreUtils.scala:63)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[2019-02-12 13:35:56,702] WARN Invalid argument (kafka.utils.CoreUtils$)
java.io.IOException: Invalid argument
	at java.io.RandomAccessFile.setLength(Native Method)
	at kafka.log.AbstractIndex.$anonfun$resize$1(AbstractIndex.scala:186)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:12)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.resize(AbstractIndex.scala:173)
	at kafka.log.TimeIndex.super$resize(TimeIndex.scala:189)
	at kafka.log.TimeIndex.$anonfun$resize$1(TimeIndex.scala:189)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:12)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.TimeIndex.resize(TimeIndex.scala:189)
	at kafka.log.AbstractIndex.$anonfun$trimToValidSize$1(AbstractIndex.scala:242)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:12)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.trimToValidSize(AbstractIndex.scala:242)
	at kafka.log.AbstractIndex.close(AbstractIndex.scala:253)
	at kafka.log.LogSegment.$anonfun$close$3(LogSegment.scala:566)
	at kafka.utils.CoreUtils$.swallow(CoreUtils.scala:86)
	at kafka.log.LogSegment.close(LogSegment.scala:566)
	at kafka.log.Log.$anonfun$close$4(Log.scala:714)
	at kafka.log.Log.$anonfun$close$4$adapted(Log.scala:714)
	at scala.collection.Iterator.foreach(Iterator.scala:937)
	at scala.collection.Iterator.foreach$(Iterator.scala:937)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1425)
	at scala.collection.IterableLike.foreach(IterableLike.scala:70)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:69)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at kafka.log.Log.$anonfun$close$3(Log.scala:714)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12)
	at kafka.log.Log.maybeHandleIOException(Log.scala:1927)
	at kafka.log.Log.close(Log.scala:709)
	at kafka.log.LogManager.$anonfun$shutdown$8(LogManager.scala:456)
	at kafka.utils.CoreUtils$$anon$1.run(CoreUtils.scala:63)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[2019-02-12 13:35:56,709] WARN Invalid argument (kafka.utils.CoreUtils$)
java.io.IOException: Invalid argument
	at java.io.RandomAccessFile.setLength(Native Method)
	at kafka.log.AbstractIndex.$anonfun$resize$1(AbstractIndex.scala:186)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:12)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.resize(AbstractIndex.scala:173)
	at kafka.log.AbstractIndex.$anonfun$trimToValidSize$1(AbstractIndex.scala:242)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:12)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.trimToValidSize(AbstractIndex.scala:242)
	at kafka.log.AbstractIndex.close(AbstractIndex.scala:253)
	at kafka.log.LogSegment.$anonfun$close$2(LogSegment.scala:565)
	at kafka.utils.CoreUtils$.swallow(CoreUtils.scala:86)
	at kafka.log.LogSegment.close(LogSegment.scala:565)
	at kafka.log.Log.$anonfun$close$4(Log.scala:714)
	at kafka.log.Log.$anonfun$close$4$adapted(Log.scala:714)
	at scala.collection.Iterator.foreach(Iterator.scala:937)
	at scala.collection.Iterator.foreach$(Iterator.scala:937)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1425)
	at scala.collection.IterableLike.foreach(IterableLike.scala:70)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:69)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at kafka.log.Log.$anonfun$close$3(Log.scala:714)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12)
	at kafka.log.Log.maybeHandleIOException(Log.scala:1927)
	at kafka.log.Log.close(Log.scala:709)
	at kafka.log.LogManager.$anonfun$shutdown$8(LogManager.scala:456)
	at kafka.utils.CoreUtils$$anon$1.run(CoreUtils.scala:63)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[2019-02-12 13:35:56,715] WARN Invalid argument (kafka.utils.CoreUtils$)
java.io.IOException: Invalid argument
	at java.io.RandomAccessFile.setLength(Native Method)
	at kafka.log.AbstractIndex.$anonfun$resize$1(AbstractIndex.scala:186)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:12)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.resize(AbstractIndex.scala:173)
	at kafka.log.TimeIndex.super$resize(TimeIndex.scala:189)
	at kafka.log.TimeIndex.$anonfun$resize$1(TimeIndex.scala:189)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:12)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.TimeIndex.resize(TimeIndex.scala:189)
	at kafka.log.AbstractIndex.$anonfun$trimToValidSize$1(AbstractIndex.scala:242)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:12)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.trimToValidSize(AbstractIndex.scala:242)
	at kafka.log.AbstractIndex.close(AbstractIndex.scala:253)
	at kafka.log.LogSegment.$anonfun$close$3(LogSegment.scala:566)
	at kafka.utils.CoreUtils$.swallow(CoreUtils.scala:86)
	at kafka.log.LogSegment.close(LogSegment.scala:566)
	at kafka.log.Log.$anonfun$close$4(Log.scala:714)
	at kafka.log.Log.$anonfun$close$4$adapted(Log.scala:714)
	at scala.collection.Iterator.foreach(Iterator.scala:937)
	at scala.collection.Iterator.foreach$(Iterator.scala:937)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1425)
	at scala.collection.IterableLike.foreach(IterableLike.scala:70)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:69)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at kafka.log.Log.$anonfun$close$3(Log.scala:714)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12)
	at kafka.log.Log.maybeHandleIOException(Log.scala:1927)
	at kafka.log.Log.close(Log.scala:709)
	at kafka.log.LogManager.$anonfun$shutdown$8(LogManager.scala:456)
	at kafka.utils.CoreUtils$$anon$1.run(CoreUtils.scala:63)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[2019-02-12 13:35:56,722] WARN Invalid argument (kafka.utils.CoreUtils$)
java.io.IOException: Invalid argument
	at java.io.RandomAccessFile.setLength(Native Method)
	at kafka.log.AbstractIndex.$anonfun$resize$1(AbstractIndex.scala:186)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:12)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.resize(AbstractIndex.scala:173)
	at kafka.log.AbstractIndex.$anonfun$trimToValidSize$1(AbstractIndex.scala:242)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:12)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.trimToValidSize(AbstractIndex.scala:242)
	at kafka.log.AbstractIndex.close(AbstractIndex.scala:253)
	at kafka.log.LogSegment.$anonfun$close$2(LogSegment.scala:565)
	at kafka.utils.CoreUtils$.swallow(CoreUtils.scala:86)
	at kafka.log.LogSegment.close(LogSegment.scala:565)
	at kafka.log.Log.$anonfun$close$4(Log.scala:714)
	at kafka.log.Log.$anonfun$close$4$adapted(Log.scala:714)
	at scala.collection.Iterator.foreach(Iterator.scala:937)
	at scala.collection.Iterator.foreach$(Iterator.scala:937)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1425)
	at scala.collection.IterableLike.foreach(IterableLike.scala:70)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:69)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at kafka.log.Log.$anonfun$close$3(Log.scala:714)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12)
	at kafka.log.Log.maybeHandleIOException(Log.scala:1927)
	at kafka.log.Log.close(Log.scala:709)
	at kafka.log.LogManager.$anonfun$shutdown$8(LogManager.scala:456)
	at kafka.utils.CoreUtils$$anon$1.run(CoreUtils.scala:63)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[2019-02-12 13:35:56,728] WARN Invalid argument (kafka.utils.CoreUtils$)
java.io.IOException: Invalid argument
	at java.io.RandomAccessFile.setLength(Native Method)
	at kafka.log.AbstractIndex.$anonfun$resize$1(AbstractIndex.scala:186)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:12)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.resize(AbstractIndex.scala:173)
	at kafka.log.TimeIndex.super$resize(TimeIndex.scala:189)
	at kafka.log.TimeIndex.$anonfun$resize$1(TimeIndex.scala:189)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:12)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.TimeIndex.resize(TimeIndex.scala:189)
	at kafka.log.AbstractIndex.$anonfun$trimToValidSize$1(AbstractIndex.scala:242)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:12)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.trimToValidSize(AbstractIndex.scala:242)
	at kafka.log.AbstractIndex.close(AbstractIndex.scala:253)
	at kafka.log.LogSegment.$anonfun$close$3(LogSegment.scala:566)
	at kafka.utils.CoreUtils$.swallow(CoreUtils.scala:86)
	at kafka.log.LogSegment.close(LogSegment.scala:566)
	at kafka.log.Log.$anonfun$close$4(Log.scala:714)
	at kafka.log.Log.$anonfun$close$4$adapted(Log.scala:714)
	at scala.collection.Iterator.foreach(Iterator.scala:937)
	at scala.collection.Iterator.foreach$(Iterator.scala:937)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1425)
	at scala.collection.IterableLike.foreach(IterableLike.scala:70)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:69)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at kafka.log.Log.$anonfun$close$3(Log.scala:714)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12)
	at kafka.log.Log.maybeHandleIOException(Log.scala:1927)
	at kafka.log.Log.close(Log.scala:709)
	at kafka.log.LogManager.$anonfun$shutdown$8(LogManager.scala:456)
	at kafka.utils.CoreUtils$$anon$1.run(CoreUtils.scala:63)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[2019-02-12 13:35:56,735] WARN Invalid argument (kafka.utils.CoreUtils$)
java.io.IOException: Invalid argument
	at java.io.RandomAccessFile.setLength(Native Method)
	at kafka.log.AbstractIndex.$anonfun$resize$1(AbstractIndex.scala:186)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:12)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.resize(AbstractIndex.scala:173)
	at kafka.log.AbstractIndex.$anonfun$trimToValidSize$1(AbstractIndex.scala:242)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:12)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.trimToValidSize(AbstractIndex.scala:242)
	at kafka.log.AbstractIndex.close(AbstractIndex.scala:253)
	at kafka.log.LogSegment.$anonfun$close$2(LogSegment.scala:565)
	at kafka.utils.CoreUtils$.swallow(CoreUtils.scala:86)
	at kafka.log.LogSegment.close(LogSegment.scala:565)
	at kafka.log.Log.$anonfun$close$4(Log.scala:714)
	at kafka.log.Log.$anonfun$close$4$adapted(Log.scala:714)
	at scala.collection.Iterator.foreach(Iterator.scala:937)
	at scala.collection.Iterator.foreach$(Iterator.scala:937)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1425)
	at scala.collection.IterableLike.foreach(IterableLike.scala:70)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:69)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at kafka.log.Log.$anonfun$close$3(Log.scala:714)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12)
	at kafka.log.Log.maybeHandleIOException(Log.scala:1927)
	at kafka.log.Log.close(Log.scala:709)
	at kafka.log.LogManager.$anonfun$shutdown$8(LogManager.scala:456)
	at kafka.utils.CoreUtils$$anon$1.run(CoreUtils.scala:63)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[2019-02-12 13:35:56,740] WARN Invalid argument (kafka.utils.CoreUtils$)
java.io.IOException: Invalid argument
	at java.io.RandomAccessFile.setLength(Native Method)
	at kafka.log.AbstractIndex.$anonfun$resize$1(AbstractIndex.scala:186)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:12)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.resize(AbstractIndex.scala:173)
	at kafka.log.TimeIndex.super$resize(TimeIndex.scala:189)
	at kafka.log.TimeIndex.$anonfun$resize$1(TimeIndex.scala:189)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:12)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.TimeIndex.resize(TimeIndex.scala:189)
	at kafka.log.AbstractIndex.$anonfun$trimToValidSize$1(AbstractIndex.scala:242)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:12)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.trimToValidSize(AbstractIndex.scala:242)
	at kafka.log.AbstractIndex.close(AbstractIndex.scala:253)
	at kafka.log.LogSegment.$anonfun$close$3(LogSegment.scala:566)
	at kafka.utils.CoreUtils$.swallow(CoreUtils.scala:86)
	at kafka.log.LogSegment.close(LogSegment.scala:566)
	at kafka.log.Log.$anonfun$close$4(Log.scala:714)
	at kafka.log.Log.$anonfun$close$4$adapted(Log.scala:714)
	at scala.collection.Iterator.foreach(Iterator.scala:937)
	at scala.collection.Iterator.foreach$(Iterator.scala:937)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1425)
	at scala.collection.IterableLike.foreach(IterableLike.scala:70)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:69)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at kafka.log.Log.$anonfun$close$3(Log.scala:714)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12)
	at kafka.log.Log.maybeHandleIOException(Log.scala:1927)
	at kafka.log.Log.close(Log.scala:709)
	at kafka.log.LogManager.$anonfun$shutdown$8(LogManager.scala:456)
	at kafka.utils.CoreUtils$$anon$1.run(CoreUtils.scala:63)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[2019-02-12 13:35:56,747] WARN Invalid argument (kafka.utils.CoreUtils$)
java.io.IOException: Invalid argument
	at java.io.RandomAccessFile.setLength(Native Method)
	at kafka.log.AbstractIndex.$anonfun$resize$1(AbstractIndex.scala:186)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:12)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.resize(AbstractIndex.scala:173)
	at kafka.log.AbstractIndex.$anonfun$trimToValidSize$1(AbstractIndex.scala:242)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:12)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.trimToValidSize(AbstractIndex.scala:242)
	at kafka.log.AbstractIndex.close(AbstractIndex.scala:253)
	at kafka.log.LogSegment.$anonfun$close$2(LogSegment.scala:565)
	at kafka.utils.CoreUtils$.swallow(CoreUtils.scala:86)
	at kafka.log.LogSegment.close(LogSegment.scala:565)
	at kafka.log.Log.$anonfun$close$4(Log.scala:714)
	at kafka.log.Log.$anonfun$close$4$adapted(Log.scala:714)
	at scala.collection.Iterator.foreach(Iterator.scala:937)
	at scala.collection.Iterator.foreach$(Iterator.scala:937)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1425)
	at scala.collection.IterableLike.foreach(IterableLike.scala:70)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:69)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at kafka.log.Log.$anonfun$close$3(Log.scala:714)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12)
	at kafka.log.Log.maybeHandleIOException(Log.scala:1927)
	at kafka.log.Log.close(Log.scala:709)
	at kafka.log.LogManager.$anonfun$shutdown$8(LogManager.scala:456)
	at kafka.utils.CoreUtils$$anon$1.run(CoreUtils.scala:63)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[2019-02-12 13:35:56,753] WARN Invalid argument (kafka.utils.CoreUtils$)
java.io.IOException: Invalid argument
	at java.io.RandomAccessFile.setLength(Native Method)
	at kafka.log.AbstractIndex.$anonfun$resize$1(AbstractIndex.scala:186)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:12)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.resize(AbstractIndex.scala:173)
	at kafka.log.TimeIndex.super$resize(TimeIndex.scala:189)
	at kafka.log.TimeIndex.$anonfun$resize$1(TimeIndex.scala:189)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:12)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.TimeIndex.resize(TimeIndex.scala:189)
	at kafka.log.AbstractIndex.$anonfun$trimToValidSize$1(AbstractIndex.scala:242)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:12)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.trimToValidSize(AbstractIndex.scala:242)
	at kafka.log.AbstractIndex.close(AbstractIndex.scala:253)
	at kafka.log.LogSegment.$anonfun$close$3(LogSegment.scala:566)
	at kafka.utils.CoreUtils$.swallow(CoreUtils.scala:86)
	at kafka.log.LogSegment.close(LogSegment.scala:566)
	at kafka.log.Log.$anonfun$close$4(Log.scala:714)
	at kafka.log.Log.$anonfun$close$4$adapted(Log.scala:714)
	at scala.collection.Iterator.foreach(Iterator.scala:937)
	at scala.collection.Iterator.foreach$(Iterator.scala:937)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1425)
	at scala.collection.IterableLike.foreach(IterableLike.scala:70)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:69)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at kafka.log.Log.$anonfun$close$3(Log.scala:714)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12)
	at kafka.log.Log.maybeHandleIOException(Log.scala:1927)
	at kafka.log.Log.close(Log.scala:709)
	at kafka.log.LogManager.$anonfun$shutdown$8(LogManager.scala:456)
	at kafka.utils.CoreUtils$$anon$1.run(CoreUtils.scala:63)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[2019-02-12 13:35:56,760] WARN Invalid argument (kafka.utils.CoreUtils$)
java.io.IOException: Invalid argument
	at java.io.RandomAccessFile.setLength(Native Method)
	at kafka.log.AbstractIndex.$anonfun$resize$1(AbstractIndex.scala:186)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:12)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.resize(AbstractIndex.scala:173)
	at kafka.log.AbstractIndex.$anonfun$trimToValidSize$1(AbstractIndex.scala:242)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:12)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.trimToValidSize(AbstractIndex.scala:242)
	at kafka.log.AbstractIndex.close(AbstractIndex.scala:253)
	at kafka.log.LogSegment.$anonfun$close$2(LogSegment.scala:565)
	at kafka.utils.CoreUtils$.swallow(CoreUtils.scala:86)
	at kafka.log.LogSegment.close(LogSegment.scala:565)
	at kafka.log.Log.$anonfun$close$4(Log.scala:714)
	at kafka.log.Log.$anonfun$close$4$adapted(Log.scala:714)
	at scala.collection.Iterator.foreach(Iterator.scala:937)
	at scala.collection.Iterator.foreach$(Iterator.scala:937)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1425)
	at scala.collection.IterableLike.foreach(IterableLike.scala:70)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:69)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at kafka.log.Log.$anonfun$close$3(Log.scala:714)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12)
	at kafka.log.Log.maybeHandleIOException(Log.scala:1927)
	at kafka.log.Log.close(Log.scala:709)
	at kafka.log.LogManager.$anonfun$shutdown$8(LogManager.scala:456)
	at kafka.utils.CoreUtils$$anon$1.run(CoreUtils.scala:63)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[2019-02-12 13:35:56,766] WARN Invalid argument (kafka.utils.CoreUtils$)
java.io.IOException: Invalid argument
	at java.io.RandomAccessFile.setLength(Native Method)
	at kafka.log.AbstractIndex.$anonfun$resize$1(AbstractIndex.scala:186)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:12)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.resize(AbstractIndex.scala:173)
	at kafka.log.TimeIndex.super$resize(TimeIndex.scala:189)
	at kafka.log.TimeIndex.$anonfun$resize$1(TimeIndex.scala:189)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:12)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.TimeIndex.resize(TimeIndex.scala:189)
	at kafka.log.AbstractIndex.$anonfun$trimToValidSize$1(AbstractIndex.scala:242)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:12)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.trimToValidSize(AbstractIndex.scala:242)
	at kafka.log.AbstractIndex.close(AbstractIndex.scala:253)
	at kafka.log.LogSegment.$anonfun$close$3(LogSegment.scala:566)
	at kafka.utils.CoreUtils$.swallow(CoreUtils.scala:86)
	at kafka.log.LogSegment.close(LogSegment.scala:566)
	at kafka.log.Log.$anonfun$close$4(Log.scala:714)
	at kafka.log.Log.$anonfun$close$4$adapted(Log.scala:714)
	at scala.collection.Iterator.foreach(Iterator.scala:937)
	at scala.collection.Iterator.foreach$(Iterator.scala:937)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1425)
	at scala.collection.IterableLike.foreach(IterableLike.scala:70)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:69)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at kafka.log.Log.$anonfun$close$3(Log.scala:714)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12)
	at kafka.log.Log.maybeHandleIOException(Log.scala:1927)
	at kafka.log.Log.close(Log.scala:709)
	at kafka.log.LogManager.$anonfun$shutdown$8(LogManager.scala:456)
	at kafka.utils.CoreUtils$$anon$1.run(CoreUtils.scala:63)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[2019-02-12 13:35:56,781] WARN Invalid argument (kafka.utils.CoreUtils$)
java.io.IOException: Invalid argument
	at java.io.RandomAccessFile.setLength(Native Method)
	at kafka.log.AbstractIndex.$anonfun$resize$1(AbstractIndex.scala:186)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:12)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.resize(AbstractIndex.scala:173)
	at kafka.log.AbstractIndex.$anonfun$trimToValidSize$1(AbstractIndex.scala:242)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:12)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.trimToValidSize(AbstractIndex.scala:242)
	at kafka.log.AbstractIndex.close(AbstractIndex.scala:253)
	at kafka.log.LogSegment.$anonfun$close$2(LogSegment.scala:565)
	at kafka.utils.CoreUtils$.swallow(CoreUtils.scala:86)
	at kafka.log.LogSegment.close(LogSegment.scala:565)
	at kafka.log.Log.$anonfun$close$4(Log.scala:714)
	at kafka.log.Log.$anonfun$close$4$adapted(Log.scala:714)
	at scala.collection.Iterator.foreach(Iterator.scala:937)
	at scala.collection.Iterator.foreach$(Iterator.scala:937)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1425)
	at scala.collection.IterableLike.foreach(IterableLike.scala:70)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:69)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at kafka.log.Log.$anonfun$close$3(Log.scala:714)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12)
	at kafka.log.Log.maybeHandleIOException(Log.scala:1927)
	at kafka.log.Log.close(Log.scala:709)
	at kafka.log.LogManager.$anonfun$shutdown$8(LogManager.scala:456)
	at kafka.utils.CoreUtils$$anon$1.run(CoreUtils.scala:63)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[2019-02-12 13:35:56,787] WARN Invalid argument (kafka.utils.CoreUtils$)
java.io.IOException: Invalid argument
	at java.io.RandomAccessFile.setLength(Native Method)
	at kafka.log.AbstractIndex.$anonfun$resize$1(AbstractIndex.scala:186)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:12)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.resize(AbstractIndex.scala:173)
	at kafka.log.TimeIndex.super$resize(TimeIndex.scala:189)
	at kafka.log.TimeIndex.$anonfun$resize$1(TimeIndex.scala:189)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:12)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.TimeIndex.resize(TimeIndex.scala:189)
	at kafka.log.AbstractIndex.$anonfun$trimToValidSize$1(AbstractIndex.scala:242)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:12)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.trimToValidSize(AbstractIndex.scala:242)
	at kafka.log.AbstractIndex.close(AbstractIndex.scala:253)
	at kafka.log.LogSegment.$anonfun$close$3(LogSegment.scala:566)
	at kafka.utils.CoreUtils$.swallow(CoreUtils.scala:86)
	at kafka.log.LogSegment.close(LogSegment.scala:566)
	at kafka.log.Log.$anonfun$close$4(Log.scala:714)
	at kafka.log.Log.$anonfun$close$4$adapted(Log.scala:714)
	at scala.collection.Iterator.foreach(Iterator.scala:937)
	at scala.collection.Iterator.foreach$(Iterator.scala:937)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1425)
	at scala.collection.IterableLike.foreach(IterableLike.scala:70)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:69)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at kafka.log.Log.$anonfun$close$3(Log.scala:714)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12)
	at kafka.log.Log.maybeHandleIOException(Log.scala:1927)
	at kafka.log.Log.close(Log.scala:709)
	at kafka.log.LogManager.$anonfun$shutdown$8(LogManager.scala:456)
	at kafka.utils.CoreUtils$$anon$1.run(CoreUtils.scala:63)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[2019-02-12 13:35:56,801] WARN Invalid argument (kafka.utils.CoreUtils$)
java.io.IOException: Invalid argument
	at java.io.RandomAccessFile.setLength(Native Method)
	at kafka.log.AbstractIndex.$anonfun$resize$1(AbstractIndex.scala:186)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:12)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.resize(AbstractIndex.scala:173)
	at kafka.log.AbstractIndex.$anonfun$trimToValidSize$1(AbstractIndex.scala:242)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:12)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.trimToValidSize(AbstractIndex.scala:242)
	at kafka.log.AbstractIndex.close(AbstractIndex.scala:253)
	at kafka.log.LogSegment.$anonfun$close$2(LogSegment.scala:565)
	at kafka.utils.CoreUtils$.swallow(CoreUtils.scala:86)
	at kafka.log.LogSegment.close(LogSegment.scala:565)
	at kafka.log.Log.$anonfun$close$4(Log.scala:714)
	at kafka.log.Log.$anonfun$close$4$adapted(Log.scala:714)
	at scala.collection.Iterator.foreach(Iterator.scala:937)
	at scala.collection.Iterator.foreach$(Iterator.scala:937)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1425)
	at scala.collection.IterableLike.foreach(IterableLike.scala:70)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:69)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at kafka.log.Log.$anonfun$close$3(Log.scala:714)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12)
	at kafka.log.Log.maybeHandleIOException(Log.scala:1927)
	at kafka.log.Log.close(Log.scala:709)
	at kafka.log.LogManager.$anonfun$shutdown$8(LogManager.scala:456)
	at kafka.utils.CoreUtils$$anon$1.run(CoreUtils.scala:63)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[2019-02-12 13:35:56,808] WARN Invalid argument (kafka.utils.CoreUtils$)
java.io.IOException: Invalid argument
	at java.io.RandomAccessFile.setLength(Native Method)
	at kafka.log.AbstractIndex.$anonfun$resize$1(AbstractIndex.scala:186)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:12)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.resize(AbstractIndex.scala:173)
	at kafka.log.TimeIndex.super$resize(TimeIndex.scala:189)
	at kafka.log.TimeIndex.$anonfun$resize$1(TimeIndex.scala:189)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:12)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.TimeIndex.resize(TimeIndex.scala:189)
	at kafka.log.AbstractIndex.$anonfun$trimToValidSize$1(AbstractIndex.scala:242)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:12)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.trimToValidSize(AbstractIndex.scala:242)
	at kafka.log.AbstractIndex.close(AbstractIndex.scala:253)
	at kafka.log.LogSegment.$anonfun$close$3(LogSegment.scala:566)
	at kafka.utils.CoreUtils$.swallow(CoreUtils.scala:86)
	at kafka.log.LogSegment.close(LogSegment.scala:566)
	at kafka.log.Log.$anonfun$close$4(Log.scala:714)
	at kafka.log.Log.$anonfun$close$4$adapted(Log.scala:714)
	at scala.collection.Iterator.foreach(Iterator.scala:937)
	at scala.collection.Iterator.foreach$(Iterator.scala:937)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1425)
	at scala.collection.IterableLike.foreach(IterableLike.scala:70)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:69)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at kafka.log.Log.$anonfun$close$3(Log.scala:714)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12)
	at kafka.log.Log.maybeHandleIOException(Log.scala:1927)
	at kafka.log.Log.close(Log.scala:709)
	at kafka.log.LogManager.$anonfun$shutdown$8(LogManager.scala:456)
	at kafka.utils.CoreUtils$$anon$1.run(CoreUtils.scala:63)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[2019-02-12 13:35:56,821] WARN Invalid argument (kafka.utils.CoreUtils$)
java.io.IOException: Invalid argument
	at java.io.RandomAccessFile.setLength(Native Method)
	at kafka.log.AbstractIndex.$anonfun$resize$1(AbstractIndex.scala:186)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:12)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.resize(AbstractIndex.scala:173)
	at kafka.log.AbstractIndex.$anonfun$trimToValidSize$1(AbstractIndex.scala:242)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:12)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.trimToValidSize(AbstractIndex.scala:242)
	at kafka.log.AbstractIndex.close(AbstractIndex.scala:253)
	at kafka.log.LogSegment.$anonfun$close$2(LogSegment.scala:565)
	at kafka.utils.CoreUtils$.swallow(CoreUtils.scala:86)
	at kafka.log.LogSegment.close(LogSegment.scala:565)
	at kafka.log.Log.$anonfun$close$4(Log.scala:714)
	at kafka.log.Log.$anonfun$close$4$adapted(Log.scala:714)
	at scala.collection.Iterator.foreach(Iterator.scala:937)
	at scala.collection.Iterator.foreach$(Iterator.scala:937)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1425)
	at scala.collection.IterableLike.foreach(IterableLike.scala:70)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:69)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at kafka.log.Log.$anonfun$close$3(Log.scala:714)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12)
	at kafka.log.Log.maybeHandleIOException(Log.scala:1927)
	at kafka.log.Log.close(Log.scala:709)
	at kafka.log.LogManager.$anonfun$shutdown$8(LogManager.scala:456)
	at kafka.utils.CoreUtils$$anon$1.run(CoreUtils.scala:63)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[2019-02-12 13:35:56,826] WARN Invalid argument (kafka.utils.CoreUtils$)
java.io.IOException: Invalid argument
	at java.io.RandomAccessFile.setLength(Native Method)
	at kafka.log.AbstractIndex.$anonfun$resize$1(AbstractIndex.scala:186)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:12)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.resize(AbstractIndex.scala:173)
	at kafka.log.TimeIndex.super$resize(TimeIndex.scala:189)
	at kafka.log.TimeIndex.$anonfun$resize$1(TimeIndex.scala:189)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:12)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.TimeIndex.resize(TimeIndex.scala:189)
	at kafka.log.AbstractIndex.$anonfun$trimToValidSize$1(AbstractIndex.scala:242)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:12)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.trimToValidSize(AbstractIndex.scala:242)
	at kafka.log.AbstractIndex.close(AbstractIndex.scala:253)
	at kafka.log.LogSegment.$anonfun$close$3(LogSegment.scala:566)
	at kafka.utils.CoreUtils$.swallow(CoreUtils.scala:86)
	at kafka.log.LogSegment.close(LogSegment.scala:566)
	at kafka.log.Log.$anonfun$close$4(Log.scala:714)
	at kafka.log.Log.$anonfun$close$4$adapted(Log.scala:714)
	at scala.collection.Iterator.foreach(Iterator.scala:937)
	at scala.collection.Iterator.foreach$(Iterator.scala:937)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1425)
	at scala.collection.IterableLike.foreach(IterableLike.scala:70)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:69)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at kafka.log.Log.$anonfun$close$3(Log.scala:714)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12)
	at kafka.log.Log.maybeHandleIOException(Log.scala:1927)
	at kafka.log.Log.close(Log.scala:709)
	at kafka.log.LogManager.$anonfun$shutdown$8(LogManager.scala:456)
	at kafka.utils.CoreUtils$$anon$1.run(CoreUtils.scala:63)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[2019-02-12 13:35:56,840] WARN Invalid argument (kafka.utils.CoreUtils$)
java.io.IOException: Invalid argument
	at java.io.RandomAccessFile.setLength(Native Method)
	at kafka.log.AbstractIndex.$anonfun$resize$1(AbstractIndex.scala:186)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:12)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.resize(AbstractIndex.scala:173)
	at kafka.log.AbstractIndex.$anonfun$trimToValidSize$1(AbstractIndex.scala:242)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:12)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.trimToValidSize(AbstractIndex.scala:242)
	at kafka.log.AbstractIndex.close(AbstractIndex.scala:253)
	at kafka.log.LogSegment.$anonfun$close$2(LogSegment.scala:565)
	at kafka.utils.CoreUtils$.swallow(CoreUtils.scala:86)
	at kafka.log.LogSegment.close(LogSegment.scala:565)
	at kafka.log.Log.$anonfun$close$4(Log.scala:714)
	at kafka.log.Log.$anonfun$close$4$adapted(Log.scala:714)
	at scala.collection.Iterator.foreach(Iterator.scala:937)
	at scala.collection.Iterator.foreach$(Iterator.scala:937)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1425)
	at scala.collection.IterableLike.foreach(IterableLike.scala:70)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:69)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at kafka.log.Log.$anonfun$close$3(Log.scala:714)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12)
	at kafka.log.Log.maybeHandleIOException(Log.scala:1927)
	at kafka.log.Log.close(Log.scala:709)
	at kafka.log.LogManager.$anonfun$shutdown$8(LogManager.scala:456)
	at kafka.utils.CoreUtils$$anon$1.run(CoreUtils.scala:63)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[2019-02-12 13:35:56,847] WARN Invalid argument (kafka.utils.CoreUtils$)
java.io.IOException: Invalid argument
	at java.io.RandomAccessFile.setLength(Native Method)
	at kafka.log.AbstractIndex.$anonfun$resize$1(AbstractIndex.scala:186)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:12)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.resize(AbstractIndex.scala:173)
	at kafka.log.TimeIndex.super$resize(TimeIndex.scala:189)
	at kafka.log.TimeIndex.$anonfun$resize$1(TimeIndex.scala:189)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:12)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.TimeIndex.resize(TimeIndex.scala:189)
	at kafka.log.AbstractIndex.$anonfun$trimToValidSize$1(AbstractIndex.scala:242)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:12)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.trimToValidSize(AbstractIndex.scala:242)
	at kafka.log.AbstractIndex.close(AbstractIndex.scala:253)
	at kafka.log.LogSegment.$anonfun$close$3(LogSegment.scala:566)
	at kafka.utils.CoreUtils$.swallow(CoreUtils.scala:86)
	at kafka.log.LogSegment.close(LogSegment.scala:566)
	at kafka.log.Log.$anonfun$close$4(Log.scala:714)
	at kafka.log.Log.$anonfun$close$4$adapted(Log.scala:714)
	at scala.collection.Iterator.foreach(Iterator.scala:937)
	at scala.collection.Iterator.foreach$(Iterator.scala:937)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1425)
	at scala.collection.IterableLike.foreach(IterableLike.scala:70)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:69)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at kafka.log.Log.$anonfun$close$3(Log.scala:714)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12)
	at kafka.log.Log.maybeHandleIOException(Log.scala:1927)
	at kafka.log.Log.close(Log.scala:709)
	at kafka.log.LogManager.$anonfun$shutdown$8(LogManager.scala:456)
	at kafka.utils.CoreUtils$$anon$1.run(CoreUtils.scala:63)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[2019-02-12 13:35:56,854] WARN Invalid argument (kafka.utils.CoreUtils$)
java.io.IOException: Invalid argument
	at java.io.RandomAccessFile.setLength(Native Method)
	at kafka.log.AbstractIndex.$anonfun$resize$1(AbstractIndex.scala:186)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:12)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.resize(AbstractIndex.scala:173)
	at kafka.log.AbstractIndex.$anonfun$trimToValidSize$1(AbstractIndex.scala:242)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:12)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.trimToValidSize(AbstractIndex.scala:242)
	at kafka.log.AbstractIndex.close(AbstractIndex.scala:253)
	at kafka.log.LogSegment.$anonfun$close$2(LogSegment.scala:565)
	at kafka.utils.CoreUtils$.swallow(CoreUtils.scala:86)
	at kafka.log.LogSegment.close(LogSegment.scala:565)
	at kafka.log.Log.$anonfun$close$4(Log.scala:714)
	at kafka.log.Log.$anonfun$close$4$adapted(Log.scala:714)
	at scala.collection.Iterator.foreach(Iterator.scala:937)
	at scala.collection.Iterator.foreach$(Iterator.scala:937)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1425)
	at scala.collection.IterableLike.foreach(IterableLike.scala:70)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:69)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at kafka.log.Log.$anonfun$close$3(Log.scala:714)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12)
	at kafka.log.Log.maybeHandleIOException(Log.scala:1927)
	at kafka.log.Log.close(Log.scala:709)
	at kafka.log.LogManager.$anonfun$shutdown$8(LogManager.scala:456)
	at kafka.utils.CoreUtils$$anon$1.run(CoreUtils.scala:63)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[2019-02-12 13:35:56,861] WARN Invalid argument (kafka.utils.CoreUtils$)
java.io.IOException: Invalid argument
	at java.io.RandomAccessFile.setLength(Native Method)
	at kafka.log.AbstractIndex.$anonfun$resize$1(AbstractIndex.scala:186)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:12)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.resize(AbstractIndex.scala:173)
	at kafka.log.TimeIndex.super$resize(TimeIndex.scala:189)
	at kafka.log.TimeIndex.$anonfun$resize$1(TimeIndex.scala:189)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:12)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.TimeIndex.resize(TimeIndex.scala:189)
	at kafka.log.AbstractIndex.$anonfun$trimToValidSize$1(AbstractIndex.scala:242)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:12)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.trimToValidSize(AbstractIndex.scala:242)
	at kafka.log.AbstractIndex.close(AbstractIndex.scala:253)
	at kafka.log.LogSegment.$anonfun$close$3(LogSegment.scala:566)
	at kafka.utils.CoreUtils$.swallow(CoreUtils.scala:86)
	at kafka.log.LogSegment.close(LogSegment.scala:566)
	at kafka.log.Log.$anonfun$close$4(Log.scala:714)
	at kafka.log.Log.$anonfun$close$4$adapted(Log.scala:714)
	at scala.collection.Iterator.foreach(Iterator.scala:937)
	at scala.collection.Iterator.foreach$(Iterator.scala:937)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1425)
	at scala.collection.IterableLike.foreach(IterableLike.scala:70)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:69)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at kafka.log.Log.$anonfun$close$3(Log.scala:714)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12)
	at kafka.log.Log.maybeHandleIOException(Log.scala:1927)
	at kafka.log.Log.close(Log.scala:709)
	at kafka.log.LogManager.$anonfun$shutdown$8(LogManager.scala:456)
	at kafka.utils.CoreUtils$$anon$1.run(CoreUtils.scala:63)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[2019-02-12 13:35:56,867] WARN Invalid argument (kafka.utils.CoreUtils$)
java.io.IOException: Invalid argument
	at java.io.RandomAccessFile.setLength(Native Method)
	at kafka.log.AbstractIndex.$anonfun$resize$1(AbstractIndex.scala:186)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:12)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.resize(AbstractIndex.scala:173)
	at kafka.log.AbstractIndex.$anonfun$trimToValidSize$1(AbstractIndex.scala:242)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:12)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.trimToValidSize(AbstractIndex.scala:242)
	at kafka.log.AbstractIndex.close(AbstractIndex.scala:253)
	at kafka.log.LogSegment.$anonfun$close$2(LogSegment.scala:565)
	at kafka.utils.CoreUtils$.swallow(CoreUtils.scala:86)
	at kafka.log.LogSegment.close(LogSegment.scala:565)
	at kafka.log.Log.$anonfun$close$4(Log.scala:714)
	at kafka.log.Log.$anonfun$close$4$adapted(Log.scala:714)
	at scala.collection.Iterator.foreach(Iterator.scala:937)
	at scala.collection.Iterator.foreach$(Iterator.scala:937)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1425)
	at scala.collection.IterableLike.foreach(IterableLike.scala:70)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:69)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at kafka.log.Log.$anonfun$close$3(Log.scala:714)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12)
	at kafka.log.Log.maybeHandleIOException(Log.scala:1927)
	at kafka.log.Log.close(Log.scala:709)
	at kafka.log.LogManager.$anonfun$shutdown$8(LogManager.scala:456)
	at kafka.utils.CoreUtils$$anon$1.run(CoreUtils.scala:63)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[2019-02-12 13:35:56,873] WARN Invalid argument (kafka.utils.CoreUtils$)
java.io.IOException: Invalid argument
	at java.io.RandomAccessFile.setLength(Native Method)
	at kafka.log.AbstractIndex.$anonfun$resize$1(AbstractIndex.scala:186)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:12)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.resize(AbstractIndex.scala:173)
	at kafka.log.TimeIndex.super$resize(TimeIndex.scala:189)
	at kafka.log.TimeIndex.$anonfun$resize$1(TimeIndex.scala:189)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:12)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.TimeIndex.resize(TimeIndex.scala:189)
	at kafka.log.AbstractIndex.$anonfun$trimToValidSize$1(AbstractIndex.scala:242)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:12)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.trimToValidSize(AbstractIndex.scala:242)
	at kafka.log.AbstractIndex.close(AbstractIndex.scala:253)
	at kafka.log.LogSegment.$anonfun$close$3(LogSegment.scala:566)
	at kafka.utils.CoreUtils$.swallow(CoreUtils.scala:86)
	at kafka.log.LogSegment.close(LogSegment.scala:566)
	at kafka.log.Log.$anonfun$close$4(Log.scala:714)
	at kafka.log.Log.$anonfun$close$4$adapted(Log.scala:714)
	at scala.collection.Iterator.foreach(Iterator.scala:937)
	at scala.collection.Iterator.foreach$(Iterator.scala:937)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1425)
	at scala.collection.IterableLike.foreach(IterableLike.scala:70)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:69)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at kafka.log.Log.$anonfun$close$3(Log.scala:714)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12)
	at kafka.log.Log.maybeHandleIOException(Log.scala:1927)
	at kafka.log.Log.close(Log.scala:709)
	at kafka.log.LogManager.$anonfun$shutdown$8(LogManager.scala:456)
	at kafka.utils.CoreUtils$$anon$1.run(CoreUtils.scala:63)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[2019-02-12 13:35:56,880] WARN Invalid argument (kafka.utils.CoreUtils$)
java.io.IOException: Invalid argument
	at java.io.RandomAccessFile.setLength(Native Method)
	at kafka.log.AbstractIndex.$anonfun$resize$1(AbstractIndex.scala:186)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:12)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.resize(AbstractIndex.scala:173)
	at kafka.log.AbstractIndex.$anonfun$trimToValidSize$1(AbstractIndex.scala:242)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:12)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.trimToValidSize(AbstractIndex.scala:242)
	at kafka.log.AbstractIndex.close(AbstractIndex.scala:253)
	at kafka.log.LogSegment.$anonfun$close$2(LogSegment.scala:565)
	at kafka.utils.CoreUtils$.swallow(CoreUtils.scala:86)
	at kafka.log.LogSegment.close(LogSegment.scala:565)
	at kafka.log.Log.$anonfun$close$4(Log.scala:714)
	at kafka.log.Log.$anonfun$close$4$adapted(Log.scala:714)
	at scala.collection.Iterator.foreach(Iterator.scala:937)
	at scala.collection.Iterator.foreach$(Iterator.scala:937)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1425)
	at scala.collection.IterableLike.foreach(IterableLike.scala:70)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:69)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at kafka.log.Log.$anonfun$close$3(Log.scala:714)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12)
	at kafka.log.Log.maybeHandleIOException(Log.scala:1927)
	at kafka.log.Log.close(Log.scala:709)
	at kafka.log.LogManager.$anonfun$shutdown$8(LogManager.scala:456)
	at kafka.utils.CoreUtils$$anon$1.run(CoreUtils.scala:63)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[2019-02-12 13:35:56,881] WARN [Controller id=0, targetBrokerId=0] Connection to node 0 (DESKTOP-7K1FP8K.localdomain/127.0.1.1:9092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient)
[2019-02-12 13:35:56,886] WARN Invalid argument (kafka.utils.CoreUtils$)
java.io.IOException: Invalid argument
	at java.io.RandomAccessFile.setLength(Native Method)
	at kafka.log.AbstractIndex.$anonfun$resize$1(AbstractIndex.scala:186)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:12)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.resize(AbstractIndex.scala:173)
	at kafka.log.TimeIndex.super$resize(TimeIndex.scala:189)
	at kafka.log.TimeIndex.$anonfun$resize$1(TimeIndex.scala:189)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:12)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.TimeIndex.resize(TimeIndex.scala:189)
	at kafka.log.AbstractIndex.$anonfun$trimToValidSize$1(AbstractIndex.scala:242)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:12)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.trimToValidSize(AbstractIndex.scala:242)
	at kafka.log.AbstractIndex.close(AbstractIndex.scala:253)
	at kafka.log.LogSegment.$anonfun$close$3(LogSegment.scala:566)
	at kafka.utils.CoreUtils$.swallow(CoreUtils.scala:86)
	at kafka.log.LogSegment.close(LogSegment.scala:566)
	at kafka.log.Log.$anonfun$close$4(Log.scala:714)
	at kafka.log.Log.$anonfun$close$4$adapted(Log.scala:714)
	at scala.collection.Iterator.foreach(Iterator.scala:937)
	at scala.collection.Iterator.foreach$(Iterator.scala:937)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1425)
	at scala.collection.IterableLike.foreach(IterableLike.scala:70)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:69)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at kafka.log.Log.$anonfun$close$3(Log.scala:714)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12)
	at kafka.log.Log.maybeHandleIOException(Log.scala:1927)
	at kafka.log.Log.close(Log.scala:709)
	at kafka.log.LogManager.$anonfun$shutdown$8(LogManager.scala:456)
	at kafka.utils.CoreUtils$$anon$1.run(CoreUtils.scala:63)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[2019-02-12 13:35:56,893] WARN Invalid argument (kafka.utils.CoreUtils$)
java.io.IOException: Invalid argument
	at java.io.RandomAccessFile.setLength(Native Method)
	at kafka.log.AbstractIndex.$anonfun$resize$1(AbstractIndex.scala:186)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:12)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.resize(AbstractIndex.scala:173)
	at kafka.log.AbstractIndex.$anonfun$trimToValidSize$1(AbstractIndex.scala:242)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:12)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.trimToValidSize(AbstractIndex.scala:242)
	at kafka.log.AbstractIndex.close(AbstractIndex.scala:253)
	at kafka.log.LogSegment.$anonfun$close$2(LogSegment.scala:565)
	at kafka.utils.CoreUtils$.swallow(CoreUtils.scala:86)
	at kafka.log.LogSegment.close(LogSegment.scala:565)
	at kafka.log.Log.$anonfun$close$4(Log.scala:714)
	at kafka.log.Log.$anonfun$close$4$adapted(Log.scala:714)
	at scala.collection.Iterator.foreach(Iterator.scala:937)
	at scala.collection.Iterator.foreach$(Iterator.scala:937)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1425)
	at scala.collection.IterableLike.foreach(IterableLike.scala:70)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:69)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at kafka.log.Log.$anonfun$close$3(Log.scala:714)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12)
	at kafka.log.Log.maybeHandleIOException(Log.scala:1927)
	at kafka.log.Log.close(Log.scala:709)
	at kafka.log.LogManager.$anonfun$shutdown$8(LogManager.scala:456)
	at kafka.utils.CoreUtils$$anon$1.run(CoreUtils.scala:63)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[2019-02-12 13:35:56,899] WARN Invalid argument (kafka.utils.CoreUtils$)
java.io.IOException: Invalid argument
	at java.io.RandomAccessFile.setLength(Native Method)
	at kafka.log.AbstractIndex.$anonfun$resize$1(AbstractIndex.scala:186)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:12)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.resize(AbstractIndex.scala:173)
	at kafka.log.TimeIndex.super$resize(TimeIndex.scala:189)
	at kafka.log.TimeIndex.$anonfun$resize$1(TimeIndex.scala:189)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:12)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.TimeIndex.resize(TimeIndex.scala:189)
	at kafka.log.AbstractIndex.$anonfun$trimToValidSize$1(AbstractIndex.scala:242)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:12)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.trimToValidSize(AbstractIndex.scala:242)
	at kafka.log.AbstractIndex.close(AbstractIndex.scala:253)
	at kafka.log.LogSegment.$anonfun$close$3(LogSegment.scala:566)
	at kafka.utils.CoreUtils$.swallow(CoreUtils.scala:86)
	at kafka.log.LogSegment.close(LogSegment.scala:566)
	at kafka.log.Log.$anonfun$close$4(Log.scala:714)
	at kafka.log.Log.$anonfun$close$4$adapted(Log.scala:714)
	at scala.collection.Iterator.foreach(Iterator.scala:937)
	at scala.collection.Iterator.foreach$(Iterator.scala:937)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1425)
	at scala.collection.IterableLike.foreach(IterableLike.scala:70)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:69)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at kafka.log.Log.$anonfun$close$3(Log.scala:714)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12)
	at kafka.log.Log.maybeHandleIOException(Log.scala:1927)
	at kafka.log.Log.close(Log.scala:709)
	at kafka.log.LogManager.$anonfun$shutdown$8(LogManager.scala:456)
	at kafka.utils.CoreUtils$$anon$1.run(CoreUtils.scala:63)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[2019-02-12 13:35:56,906] WARN Invalid argument (kafka.utils.CoreUtils$)
java.io.IOException: Invalid argument
	at java.io.RandomAccessFile.setLength(Native Method)
	at kafka.log.AbstractIndex.$anonfun$resize$1(AbstractIndex.scala:186)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:12)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.resize(AbstractIndex.scala:173)
	at kafka.log.AbstractIndex.$anonfun$trimToValidSize$1(AbstractIndex.scala:242)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:12)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.trimToValidSize(AbstractIndex.scala:242)
	at kafka.log.AbstractIndex.close(AbstractIndex.scala:253)
	at kafka.log.LogSegment.$anonfun$close$2(LogSegment.scala:565)
	at kafka.utils.CoreUtils$.swallow(CoreUtils.scala:86)
	at kafka.log.LogSegment.close(LogSegment.scala:565)
	at kafka.log.Log.$anonfun$close$4(Log.scala:714)
	at kafka.log.Log.$anonfun$close$4$adapted(Log.scala:714)
	at scala.collection.Iterator.foreach(Iterator.scala:937)
	at scala.collection.Iterator.foreach$(Iterator.scala:937)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1425)
	at scala.collection.IterableLike.foreach(IterableLike.scala:70)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:69)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at kafka.log.Log.$anonfun$close$3(Log.scala:714)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12)
	at kafka.log.Log.maybeHandleIOException(Log.scala:1927)
	at kafka.log.Log.close(Log.scala:709)
	at kafka.log.LogManager.$anonfun$shutdown$8(LogManager.scala:456)
	at kafka.utils.CoreUtils$$anon$1.run(CoreUtils.scala:63)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[2019-02-12 13:35:56,912] WARN Invalid argument (kafka.utils.CoreUtils$)
java.io.IOException: Invalid argument
	at java.io.RandomAccessFile.setLength(Native Method)
	at kafka.log.AbstractIndex.$anonfun$resize$1(AbstractIndex.scala:186)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:12)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.resize(AbstractIndex.scala:173)
	at kafka.log.TimeIndex.super$resize(TimeIndex.scala:189)
	at kafka.log.TimeIndex.$anonfun$resize$1(TimeIndex.scala:189)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:12)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.TimeIndex.resize(TimeIndex.scala:189)
	at kafka.log.AbstractIndex.$anonfun$trimToValidSize$1(AbstractIndex.scala:242)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:12)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.trimToValidSize(AbstractIndex.scala:242)
	at kafka.log.AbstractIndex.close(AbstractIndex.scala:253)
	at kafka.log.LogSegment.$anonfun$close$3(LogSegment.scala:566)
	at kafka.utils.CoreUtils$.swallow(CoreUtils.scala:86)
	at kafka.log.LogSegment.close(LogSegment.scala:566)
	at kafka.log.Log.$anonfun$close$4(Log.scala:714)
	at kafka.log.Log.$anonfun$close$4$adapted(Log.scala:714)
	at scala.collection.Iterator.foreach(Iterator.scala:937)
	at scala.collection.Iterator.foreach$(Iterator.scala:937)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1425)
	at scala.collection.IterableLike.foreach(IterableLike.scala:70)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:69)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at kafka.log.Log.$anonfun$close$3(Log.scala:714)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12)
	at kafka.log.Log.maybeHandleIOException(Log.scala:1927)
	at kafka.log.Log.close(Log.scala:709)
	at kafka.log.LogManager.$anonfun$shutdown$8(LogManager.scala:456)
	at kafka.utils.CoreUtils$$anon$1.run(CoreUtils.scala:63)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[2019-02-12 13:35:56,919] WARN Invalid argument (kafka.utils.CoreUtils$)
java.io.IOException: Invalid argument
	at java.io.RandomAccessFile.setLength(Native Method)
	at kafka.log.AbstractIndex.$anonfun$resize$1(AbstractIndex.scala:186)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:12)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.resize(AbstractIndex.scala:173)
	at kafka.log.AbstractIndex.$anonfun$trimToValidSize$1(AbstractIndex.scala:242)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:12)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.trimToValidSize(AbstractIndex.scala:242)
	at kafka.log.AbstractIndex.close(AbstractIndex.scala:253)
	at kafka.log.LogSegment.$anonfun$close$2(LogSegment.scala:565)
	at kafka.utils.CoreUtils$.swallow(CoreUtils.scala:86)
	at kafka.log.LogSegment.close(LogSegment.scala:565)
	at kafka.log.Log.$anonfun$close$4(Log.scala:714)
	at kafka.log.Log.$anonfun$close$4$adapted(Log.scala:714)
	at scala.collection.Iterator.foreach(Iterator.scala:937)
	at scala.collection.Iterator.foreach$(Iterator.scala:937)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1425)
	at scala.collection.IterableLike.foreach(IterableLike.scala:70)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:69)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at kafka.log.Log.$anonfun$close$3(Log.scala:714)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12)
	at kafka.log.Log.maybeHandleIOException(Log.scala:1927)
	at kafka.log.Log.close(Log.scala:709)
	at kafka.log.LogManager.$anonfun$shutdown$8(LogManager.scala:456)
	at kafka.utils.CoreUtils$$anon$1.run(CoreUtils.scala:63)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[2019-02-12 13:35:56,924] WARN Invalid argument (kafka.utils.CoreUtils$)
java.io.IOException: Invalid argument
	at java.io.RandomAccessFile.setLength(Native Method)
	at kafka.log.AbstractIndex.$anonfun$resize$1(AbstractIndex.scala:186)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:12)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.resize(AbstractIndex.scala:173)
	at kafka.log.TimeIndex.super$resize(TimeIndex.scala:189)
	at kafka.log.TimeIndex.$anonfun$resize$1(TimeIndex.scala:189)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:12)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.TimeIndex.resize(TimeIndex.scala:189)
	at kafka.log.AbstractIndex.$anonfun$trimToValidSize$1(AbstractIndex.scala:242)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:12)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.trimToValidSize(AbstractIndex.scala:242)
	at kafka.log.AbstractIndex.close(AbstractIndex.scala:253)
	at kafka.log.LogSegment.$anonfun$close$3(LogSegment.scala:566)
	at kafka.utils.CoreUtils$.swallow(CoreUtils.scala:86)
	at kafka.log.LogSegment.close(LogSegment.scala:566)
	at kafka.log.Log.$anonfun$close$4(Log.scala:714)
	at kafka.log.Log.$anonfun$close$4$adapted(Log.scala:714)
	at scala.collection.Iterator.foreach(Iterator.scala:937)
	at scala.collection.Iterator.foreach$(Iterator.scala:937)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1425)
	at scala.collection.IterableLike.foreach(IterableLike.scala:70)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:69)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at kafka.log.Log.$anonfun$close$3(Log.scala:714)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12)
	at kafka.log.Log.maybeHandleIOException(Log.scala:1927)
	at kafka.log.Log.close(Log.scala:709)
	at kafka.log.LogManager.$anonfun$shutdown$8(LogManager.scala:456)
	at kafka.utils.CoreUtils$$anon$1.run(CoreUtils.scala:63)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[2019-02-12 13:35:56,931] WARN Invalid argument (kafka.utils.CoreUtils$)
java.io.IOException: Invalid argument
	at java.io.RandomAccessFile.setLength(Native Method)
	at kafka.log.AbstractIndex.$anonfun$resize$1(AbstractIndex.scala:186)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:12)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.resize(AbstractIndex.scala:173)
	at kafka.log.AbstractIndex.$anonfun$trimToValidSize$1(AbstractIndex.scala:242)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:12)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.trimToValidSize(AbstractIndex.scala:242)
	at kafka.log.AbstractIndex.close(AbstractIndex.scala:253)
	at kafka.log.LogSegment.$anonfun$close$2(LogSegment.scala:565)
	at kafka.utils.CoreUtils$.swallow(CoreUtils.scala:86)
	at kafka.log.LogSegment.close(LogSegment.scala:565)
	at kafka.log.Log.$anonfun$close$4(Log.scala:714)
	at kafka.log.Log.$anonfun$close$4$adapted(Log.scala:714)
	at scala.collection.Iterator.foreach(Iterator.scala:937)
	at scala.collection.Iterator.foreach$(Iterator.scala:937)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1425)
	at scala.collection.IterableLike.foreach(IterableLike.scala:70)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:69)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at kafka.log.Log.$anonfun$close$3(Log.scala:714)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12)
	at kafka.log.Log.maybeHandleIOException(Log.scala:1927)
	at kafka.log.Log.close(Log.scala:709)
	at kafka.log.LogManager.$anonfun$shutdown$8(LogManager.scala:456)
	at kafka.utils.CoreUtils$$anon$1.run(CoreUtils.scala:63)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[2019-02-12 13:35:56,937] WARN Invalid argument (kafka.utils.CoreUtils$)
java.io.IOException: Invalid argument
	at java.io.RandomAccessFile.setLength(Native Method)
	at kafka.log.AbstractIndex.$anonfun$resize$1(AbstractIndex.scala:186)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:12)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.resize(AbstractIndex.scala:173)
	at kafka.log.TimeIndex.super$resize(TimeIndex.scala:189)
	at kafka.log.TimeIndex.$anonfun$resize$1(TimeIndex.scala:189)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:12)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.TimeIndex.resize(TimeIndex.scala:189)
	at kafka.log.AbstractIndex.$anonfun$trimToValidSize$1(AbstractIndex.scala:242)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:12)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.trimToValidSize(AbstractIndex.scala:242)
	at kafka.log.AbstractIndex.close(AbstractIndex.scala:253)
	at kafka.log.LogSegment.$anonfun$close$3(LogSegment.scala:566)
	at kafka.utils.CoreUtils$.swallow(CoreUtils.scala:86)
	at kafka.log.LogSegment.close(LogSegment.scala:566)
	at kafka.log.Log.$anonfun$close$4(Log.scala:714)
	at kafka.log.Log.$anonfun$close$4$adapted(Log.scala:714)
	at scala.collection.Iterator.foreach(Iterator.scala:937)
	at scala.collection.Iterator.foreach$(Iterator.scala:937)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1425)
	at scala.collection.IterableLike.foreach(IterableLike.scala:70)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:69)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at kafka.log.Log.$anonfun$close$3(Log.scala:714)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12)
	at kafka.log.Log.maybeHandleIOException(Log.scala:1927)
	at kafka.log.Log.close(Log.scala:709)
	at kafka.log.LogManager.$anonfun$shutdown$8(LogManager.scala:456)
	at kafka.utils.CoreUtils$$anon$1.run(CoreUtils.scala:63)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[2019-02-12 13:35:56,944] WARN Invalid argument (kafka.utils.CoreUtils$)
java.io.IOException: Invalid argument
	at java.io.RandomAccessFile.setLength(Native Method)
	at kafka.log.AbstractIndex.$anonfun$resize$1(AbstractIndex.scala:186)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:12)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.resize(AbstractIndex.scala:173)
	at kafka.log.AbstractIndex.$anonfun$trimToValidSize$1(AbstractIndex.scala:242)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:12)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.trimToValidSize(AbstractIndex.scala:242)
	at kafka.log.AbstractIndex.close(AbstractIndex.scala:253)
	at kafka.log.LogSegment.$anonfun$close$2(LogSegment.scala:565)
	at kafka.utils.CoreUtils$.swallow(CoreUtils.scala:86)
	at kafka.log.LogSegment.close(LogSegment.scala:565)
	at kafka.log.Log.$anonfun$close$4(Log.scala:714)
	at kafka.log.Log.$anonfun$close$4$adapted(Log.scala:714)
	at scala.collection.Iterator.foreach(Iterator.scala:937)
	at scala.collection.Iterator.foreach$(Iterator.scala:937)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1425)
	at scala.collection.IterableLike.foreach(IterableLike.scala:70)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:69)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at kafka.log.Log.$anonfun$close$3(Log.scala:714)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12)
	at kafka.log.Log.maybeHandleIOException(Log.scala:1927)
	at kafka.log.Log.close(Log.scala:709)
	at kafka.log.LogManager.$anonfun$shutdown$8(LogManager.scala:456)
	at kafka.utils.CoreUtils$$anon$1.run(CoreUtils.scala:63)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[2019-02-12 13:35:56,949] WARN Invalid argument (kafka.utils.CoreUtils$)
java.io.IOException: Invalid argument
	at java.io.RandomAccessFile.setLength(Native Method)
	at kafka.log.AbstractIndex.$anonfun$resize$1(AbstractIndex.scala:186)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:12)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.resize(AbstractIndex.scala:173)
	at kafka.log.TimeIndex.super$resize(TimeIndex.scala:189)
	at kafka.log.TimeIndex.$anonfun$resize$1(TimeIndex.scala:189)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:12)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.TimeIndex.resize(TimeIndex.scala:189)
	at kafka.log.AbstractIndex.$anonfun$trimToValidSize$1(AbstractIndex.scala:242)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:12)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.trimToValidSize(AbstractIndex.scala:242)
	at kafka.log.AbstractIndex.close(AbstractIndex.scala:253)
	at kafka.log.LogSegment.$anonfun$close$3(LogSegment.scala:566)
	at kafka.utils.CoreUtils$.swallow(CoreUtils.scala:86)
	at kafka.log.LogSegment.close(LogSegment.scala:566)
	at kafka.log.Log.$anonfun$close$4(Log.scala:714)
	at kafka.log.Log.$anonfun$close$4$adapted(Log.scala:714)
	at scala.collection.Iterator.foreach(Iterator.scala:937)
	at scala.collection.Iterator.foreach$(Iterator.scala:937)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1425)
	at scala.collection.IterableLike.foreach(IterableLike.scala:70)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:69)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at kafka.log.Log.$anonfun$close$3(Log.scala:714)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12)
	at kafka.log.Log.maybeHandleIOException(Log.scala:1927)
	at kafka.log.Log.close(Log.scala:709)
	at kafka.log.LogManager.$anonfun$shutdown$8(LogManager.scala:456)
	at kafka.utils.CoreUtils$$anon$1.run(CoreUtils.scala:63)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[2019-02-12 13:35:56,962] WARN Invalid argument (kafka.utils.CoreUtils$)
java.io.IOException: Invalid argument
	at java.io.RandomAccessFile.setLength(Native Method)
	at kafka.log.AbstractIndex.$anonfun$resize$1(AbstractIndex.scala:186)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:12)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.resize(AbstractIndex.scala:173)
	at kafka.log.AbstractIndex.$anonfun$trimToValidSize$1(AbstractIndex.scala:242)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:12)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.trimToValidSize(AbstractIndex.scala:242)
	at kafka.log.AbstractIndex.close(AbstractIndex.scala:253)
	at kafka.log.LogSegment.$anonfun$close$2(LogSegment.scala:565)
	at kafka.utils.CoreUtils$.swallow(CoreUtils.scala:86)
	at kafka.log.LogSegment.close(LogSegment.scala:565)
	at kafka.log.Log.$anonfun$close$4(Log.scala:714)
	at kafka.log.Log.$anonfun$close$4$adapted(Log.scala:714)
	at scala.collection.Iterator.foreach(Iterator.scala:937)
	at scala.collection.Iterator.foreach$(Iterator.scala:937)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1425)
	at scala.collection.IterableLike.foreach(IterableLike.scala:70)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:69)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at kafka.log.Log.$anonfun$close$3(Log.scala:714)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12)
	at kafka.log.Log.maybeHandleIOException(Log.scala:1927)
	at kafka.log.Log.close(Log.scala:709)
	at kafka.log.LogManager.$anonfun$shutdown$8(LogManager.scala:456)
	at kafka.utils.CoreUtils$$anon$1.run(CoreUtils.scala:63)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[2019-02-12 13:35:56,968] WARN Invalid argument (kafka.utils.CoreUtils$)
java.io.IOException: Invalid argument
	at java.io.RandomAccessFile.setLength(Native Method)
	at kafka.log.AbstractIndex.$anonfun$resize$1(AbstractIndex.scala:186)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:12)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.resize(AbstractIndex.scala:173)
	at kafka.log.TimeIndex.super$resize(TimeIndex.scala:189)
	at kafka.log.TimeIndex.$anonfun$resize$1(TimeIndex.scala:189)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:12)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.TimeIndex.resize(TimeIndex.scala:189)
	at kafka.log.AbstractIndex.$anonfun$trimToValidSize$1(AbstractIndex.scala:242)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:12)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.trimToValidSize(AbstractIndex.scala:242)
	at kafka.log.AbstractIndex.close(AbstractIndex.scala:253)
	at kafka.log.LogSegment.$anonfun$close$3(LogSegment.scala:566)
	at kafka.utils.CoreUtils$.swallow(CoreUtils.scala:86)
	at kafka.log.LogSegment.close(LogSegment.scala:566)
	at kafka.log.Log.$anonfun$close$4(Log.scala:714)
	at kafka.log.Log.$anonfun$close$4$adapted(Log.scala:714)
	at scala.collection.Iterator.foreach(Iterator.scala:937)
	at scala.collection.Iterator.foreach$(Iterator.scala:937)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1425)
	at scala.collection.IterableLike.foreach(IterableLike.scala:70)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:69)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at kafka.log.Log.$anonfun$close$3(Log.scala:714)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12)
	at kafka.log.Log.maybeHandleIOException(Log.scala:1927)
	at kafka.log.Log.close(Log.scala:709)
	at kafka.log.LogManager.$anonfun$shutdown$8(LogManager.scala:456)
	at kafka.utils.CoreUtils$$anon$1.run(CoreUtils.scala:63)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[2019-02-12 13:35:56,981] WARN Invalid argument (kafka.utils.CoreUtils$)
java.io.IOException: Invalid argument
	at java.io.RandomAccessFile.setLength(Native Method)
	at kafka.log.AbstractIndex.$anonfun$resize$1(AbstractIndex.scala:186)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:12)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.resize(AbstractIndex.scala:173)
	at kafka.log.AbstractIndex.$anonfun$trimToValidSize$1(AbstractIndex.scala:242)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:12)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.trimToValidSize(AbstractIndex.scala:242)
	at kafka.log.AbstractIndex.close(AbstractIndex.scala:253)
	at kafka.log.LogSegment.$anonfun$close$2(LogSegment.scala:565)
	at kafka.utils.CoreUtils$.swallow(CoreUtils.scala:86)
	at kafka.log.LogSegment.close(LogSegment.scala:565)
	at kafka.log.Log.$anonfun$close$4(Log.scala:714)
	at kafka.log.Log.$anonfun$close$4$adapted(Log.scala:714)
	at scala.collection.Iterator.foreach(Iterator.scala:937)
	at scala.collection.Iterator.foreach$(Iterator.scala:937)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1425)
	at scala.collection.IterableLike.foreach(IterableLike.scala:70)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:69)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at kafka.log.Log.$anonfun$close$3(Log.scala:714)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12)
	at kafka.log.Log.maybeHandleIOException(Log.scala:1927)
	at kafka.log.Log.close(Log.scala:709)
	at kafka.log.LogManager.$anonfun$shutdown$8(LogManager.scala:456)
	at kafka.utils.CoreUtils$$anon$1.run(CoreUtils.scala:63)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[2019-02-12 13:35:56,987] WARN Invalid argument (kafka.utils.CoreUtils$)
java.io.IOException: Invalid argument
	at java.io.RandomAccessFile.setLength(Native Method)
	at kafka.log.AbstractIndex.$anonfun$resize$1(AbstractIndex.scala:186)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:12)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.resize(AbstractIndex.scala:173)
	at kafka.log.TimeIndex.super$resize(TimeIndex.scala:189)
	at kafka.log.TimeIndex.$anonfun$resize$1(TimeIndex.scala:189)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:12)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.TimeIndex.resize(TimeIndex.scala:189)
	at kafka.log.AbstractIndex.$anonfun$trimToValidSize$1(AbstractIndex.scala:242)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:12)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.trimToValidSize(AbstractIndex.scala:242)
	at kafka.log.AbstractIndex.close(AbstractIndex.scala:253)
	at kafka.log.LogSegment.$anonfun$close$3(LogSegment.scala:566)
	at kafka.utils.CoreUtils$.swallow(CoreUtils.scala:86)
	at kafka.log.LogSegment.close(LogSegment.scala:566)
	at kafka.log.Log.$anonfun$close$4(Log.scala:714)
	at kafka.log.Log.$anonfun$close$4$adapted(Log.scala:714)
	at scala.collection.Iterator.foreach(Iterator.scala:937)
	at scala.collection.Iterator.foreach$(Iterator.scala:937)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1425)
	at scala.collection.IterableLike.foreach(IterableLike.scala:70)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:69)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at kafka.log.Log.$anonfun$close$3(Log.scala:714)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12)
	at kafka.log.Log.maybeHandleIOException(Log.scala:1927)
	at kafka.log.Log.close(Log.scala:709)
	at kafka.log.LogManager.$anonfun$shutdown$8(LogManager.scala:456)
	at kafka.utils.CoreUtils$$anon$1.run(CoreUtils.scala:63)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[2019-02-12 13:35:57,000] WARN Invalid argument (kafka.utils.CoreUtils$)
java.io.IOException: Invalid argument
	at java.io.RandomAccessFile.setLength(Native Method)
	at kafka.log.AbstractIndex.$anonfun$resize$1(AbstractIndex.scala:186)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:12)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.resize(AbstractIndex.scala:173)
	at kafka.log.AbstractIndex.$anonfun$trimToValidSize$1(AbstractIndex.scala:242)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:12)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.trimToValidSize(AbstractIndex.scala:242)
	at kafka.log.AbstractIndex.close(AbstractIndex.scala:253)
	at kafka.log.LogSegment.$anonfun$close$2(LogSegment.scala:565)
	at kafka.utils.CoreUtils$.swallow(CoreUtils.scala:86)
	at kafka.log.LogSegment.close(LogSegment.scala:565)
	at kafka.log.Log.$anonfun$close$4(Log.scala:714)
	at kafka.log.Log.$anonfun$close$4$adapted(Log.scala:714)
	at scala.collection.Iterator.foreach(Iterator.scala:937)
	at scala.collection.Iterator.foreach$(Iterator.scala:937)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1425)
	at scala.collection.IterableLike.foreach(IterableLike.scala:70)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:69)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at kafka.log.Log.$anonfun$close$3(Log.scala:714)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12)
	at kafka.log.Log.maybeHandleIOException(Log.scala:1927)
	at kafka.log.Log.close(Log.scala:709)
	at kafka.log.LogManager.$anonfun$shutdown$8(LogManager.scala:456)
	at kafka.utils.CoreUtils$$anon$1.run(CoreUtils.scala:63)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[2019-02-12 13:35:57,006] WARN Invalid argument (kafka.utils.CoreUtils$)
java.io.IOException: Invalid argument
	at java.io.RandomAccessFile.setLength(Native Method)
	at kafka.log.AbstractIndex.$anonfun$resize$1(AbstractIndex.scala:186)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:12)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.resize(AbstractIndex.scala:173)
	at kafka.log.TimeIndex.super$resize(TimeIndex.scala:189)
	at kafka.log.TimeIndex.$anonfun$resize$1(TimeIndex.scala:189)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:12)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.TimeIndex.resize(TimeIndex.scala:189)
	at kafka.log.AbstractIndex.$anonfun$trimToValidSize$1(AbstractIndex.scala:242)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:12)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.trimToValidSize(AbstractIndex.scala:242)
	at kafka.log.AbstractIndex.close(AbstractIndex.scala:253)
	at kafka.log.LogSegment.$anonfun$close$3(LogSegment.scala:566)
	at kafka.utils.CoreUtils$.swallow(CoreUtils.scala:86)
	at kafka.log.LogSegment.close(LogSegment.scala:566)
	at kafka.log.Log.$anonfun$close$4(Log.scala:714)
	at kafka.log.Log.$anonfun$close$4$adapted(Log.scala:714)
	at scala.collection.Iterator.foreach(Iterator.scala:937)
	at scala.collection.Iterator.foreach$(Iterator.scala:937)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1425)
	at scala.collection.IterableLike.foreach(IterableLike.scala:70)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:69)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at kafka.log.Log.$anonfun$close$3(Log.scala:714)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12)
	at kafka.log.Log.maybeHandleIOException(Log.scala:1927)
	at kafka.log.Log.close(Log.scala:709)
	at kafka.log.LogManager.$anonfun$shutdown$8(LogManager.scala:456)
	at kafka.utils.CoreUtils$$anon$1.run(CoreUtils.scala:63)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[2019-02-12 13:35:57,019] WARN Invalid argument (kafka.utils.CoreUtils$)
java.io.IOException: Invalid argument
	at java.io.RandomAccessFile.setLength(Native Method)
	at kafka.log.AbstractIndex.$anonfun$resize$1(AbstractIndex.scala:186)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:12)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.resize(AbstractIndex.scala:173)
	at kafka.log.AbstractIndex.$anonfun$trimToValidSize$1(AbstractIndex.scala:242)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:12)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.trimToValidSize(AbstractIndex.scala:242)
	at kafka.log.AbstractIndex.close(AbstractIndex.scala:253)
	at kafka.log.LogSegment.$anonfun$close$2(LogSegment.scala:565)
	at kafka.utils.CoreUtils$.swallow(CoreUtils.scala:86)
	at kafka.log.LogSegment.close(LogSegment.scala:565)
	at kafka.log.Log.$anonfun$close$4(Log.scala:714)
	at kafka.log.Log.$anonfun$close$4$adapted(Log.scala:714)
	at scala.collection.Iterator.foreach(Iterator.scala:937)
	at scala.collection.Iterator.foreach$(Iterator.scala:937)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1425)
	at scala.collection.IterableLike.foreach(IterableLike.scala:70)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:69)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at kafka.log.Log.$anonfun$close$3(Log.scala:714)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12)
	at kafka.log.Log.maybeHandleIOException(Log.scala:1927)
	at kafka.log.Log.close(Log.scala:709)
	at kafka.log.LogManager.$anonfun$shutdown$8(LogManager.scala:456)
	at kafka.utils.CoreUtils$$anon$1.run(CoreUtils.scala:63)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[2019-02-12 13:35:57,025] WARN Invalid argument (kafka.utils.CoreUtils$)
java.io.IOException: Invalid argument
	at java.io.RandomAccessFile.setLength(Native Method)
	at kafka.log.AbstractIndex.$anonfun$resize$1(AbstractIndex.scala:186)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:12)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.resize(AbstractIndex.scala:173)
	at kafka.log.TimeIndex.super$resize(TimeIndex.scala:189)
	at kafka.log.TimeIndex.$anonfun$resize$1(TimeIndex.scala:189)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:12)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.TimeIndex.resize(TimeIndex.scala:189)
	at kafka.log.AbstractIndex.$anonfun$trimToValidSize$1(AbstractIndex.scala:242)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:12)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.trimToValidSize(AbstractIndex.scala:242)
	at kafka.log.AbstractIndex.close(AbstractIndex.scala:253)
	at kafka.log.LogSegment.$anonfun$close$3(LogSegment.scala:566)
	at kafka.utils.CoreUtils$.swallow(CoreUtils.scala:86)
	at kafka.log.LogSegment.close(LogSegment.scala:566)
	at kafka.log.Log.$anonfun$close$4(Log.scala:714)
	at kafka.log.Log.$anonfun$close$4$adapted(Log.scala:714)
	at scala.collection.Iterator.foreach(Iterator.scala:937)
	at scala.collection.Iterator.foreach$(Iterator.scala:937)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1425)
	at scala.collection.IterableLike.foreach(IterableLike.scala:70)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:69)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at kafka.log.Log.$anonfun$close$3(Log.scala:714)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12)
	at kafka.log.Log.maybeHandleIOException(Log.scala:1927)
	at kafka.log.Log.close(Log.scala:709)
	at kafka.log.LogManager.$anonfun$shutdown$8(LogManager.scala:456)
	at kafka.utils.CoreUtils$$anon$1.run(CoreUtils.scala:63)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[2019-02-12 13:35:57,032] WARN Invalid argument (kafka.utils.CoreUtils$)
java.io.IOException: Invalid argument
	at java.io.RandomAccessFile.setLength(Native Method)
	at kafka.log.AbstractIndex.$anonfun$resize$1(AbstractIndex.scala:186)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:12)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.resize(AbstractIndex.scala:173)
	at kafka.log.AbstractIndex.$anonfun$trimToValidSize$1(AbstractIndex.scala:242)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:12)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.trimToValidSize(AbstractIndex.scala:242)
	at kafka.log.AbstractIndex.close(AbstractIndex.scala:253)
	at kafka.log.LogSegment.$anonfun$close$2(LogSegment.scala:565)
	at kafka.utils.CoreUtils$.swallow(CoreUtils.scala:86)
	at kafka.log.LogSegment.close(LogSegment.scala:565)
	at kafka.log.Log.$anonfun$close$4(Log.scala:714)
	at kafka.log.Log.$anonfun$close$4$adapted(Log.scala:714)
	at scala.collection.Iterator.foreach(Iterator.scala:937)
	at scala.collection.Iterator.foreach$(Iterator.scala:937)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1425)
	at scala.collection.IterableLike.foreach(IterableLike.scala:70)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:69)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at kafka.log.Log.$anonfun$close$3(Log.scala:714)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12)
	at kafka.log.Log.maybeHandleIOException(Log.scala:1927)
	at kafka.log.Log.close(Log.scala:709)
	at kafka.log.LogManager.$anonfun$shutdown$8(LogManager.scala:456)
	at kafka.utils.CoreUtils$$anon$1.run(CoreUtils.scala:63)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[2019-02-12 13:35:57,038] WARN Invalid argument (kafka.utils.CoreUtils$)
java.io.IOException: Invalid argument
	at java.io.RandomAccessFile.setLength(Native Method)
	at kafka.log.AbstractIndex.$anonfun$resize$1(AbstractIndex.scala:186)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:12)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.resize(AbstractIndex.scala:173)
	at kafka.log.TimeIndex.super$resize(TimeIndex.scala:189)
	at kafka.log.TimeIndex.$anonfun$resize$1(TimeIndex.scala:189)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:12)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.TimeIndex.resize(TimeIndex.scala:189)
	at kafka.log.AbstractIndex.$anonfun$trimToValidSize$1(AbstractIndex.scala:242)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:12)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.trimToValidSize(AbstractIndex.scala:242)
	at kafka.log.AbstractIndex.close(AbstractIndex.scala:253)
	at kafka.log.LogSegment.$anonfun$close$3(LogSegment.scala:566)
	at kafka.utils.CoreUtils$.swallow(CoreUtils.scala:86)
	at kafka.log.LogSegment.close(LogSegment.scala:566)
	at kafka.log.Log.$anonfun$close$4(Log.scala:714)
	at kafka.log.Log.$anonfun$close$4$adapted(Log.scala:714)
	at scala.collection.Iterator.foreach(Iterator.scala:937)
	at scala.collection.Iterator.foreach$(Iterator.scala:937)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1425)
	at scala.collection.IterableLike.foreach(IterableLike.scala:70)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:69)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at kafka.log.Log.$anonfun$close$3(Log.scala:714)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12)
	at kafka.log.Log.maybeHandleIOException(Log.scala:1927)
	at kafka.log.Log.close(Log.scala:709)
	at kafka.log.LogManager.$anonfun$shutdown$8(LogManager.scala:456)
	at kafka.utils.CoreUtils$$anon$1.run(CoreUtils.scala:63)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[2019-02-12 13:35:57,044] WARN Invalid argument (kafka.utils.CoreUtils$)
java.io.IOException: Invalid argument
	at java.io.RandomAccessFile.setLength(Native Method)
	at kafka.log.AbstractIndex.$anonfun$resize$1(AbstractIndex.scala:186)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:12)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.resize(AbstractIndex.scala:173)
	at kafka.log.AbstractIndex.$anonfun$trimToValidSize$1(AbstractIndex.scala:242)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:12)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.trimToValidSize(AbstractIndex.scala:242)
	at kafka.log.AbstractIndex.close(AbstractIndex.scala:253)
	at kafka.log.LogSegment.$anonfun$close$2(LogSegment.scala:565)
	at kafka.utils.CoreUtils$.swallow(CoreUtils.scala:86)
	at kafka.log.LogSegment.close(LogSegment.scala:565)
	at kafka.log.Log.$anonfun$close$4(Log.scala:714)
	at kafka.log.Log.$anonfun$close$4$adapted(Log.scala:714)
	at scala.collection.Iterator.foreach(Iterator.scala:937)
	at scala.collection.Iterator.foreach$(Iterator.scala:937)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1425)
	at scala.collection.IterableLike.foreach(IterableLike.scala:70)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:69)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at kafka.log.Log.$anonfun$close$3(Log.scala:714)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12)
	at kafka.log.Log.maybeHandleIOException(Log.scala:1927)
	at kafka.log.Log.close(Log.scala:709)
	at kafka.log.LogManager.$anonfun$shutdown$8(LogManager.scala:456)
	at kafka.utils.CoreUtils$$anon$1.run(CoreUtils.scala:63)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[2019-02-12 13:35:57,050] WARN Invalid argument (kafka.utils.CoreUtils$)
java.io.IOException: Invalid argument
	at java.io.RandomAccessFile.setLength(Native Method)
	at kafka.log.AbstractIndex.$anonfun$resize$1(AbstractIndex.scala:186)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:12)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.resize(AbstractIndex.scala:173)
	at kafka.log.TimeIndex.super$resize(TimeIndex.scala:189)
	at kafka.log.TimeIndex.$anonfun$resize$1(TimeIndex.scala:189)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:12)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.TimeIndex.resize(TimeIndex.scala:189)
	at kafka.log.AbstractIndex.$anonfun$trimToValidSize$1(AbstractIndex.scala:242)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:12)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.trimToValidSize(AbstractIndex.scala:242)
	at kafka.log.AbstractIndex.close(AbstractIndex.scala:253)
	at kafka.log.LogSegment.$anonfun$close$3(LogSegment.scala:566)
	at kafka.utils.CoreUtils$.swallow(CoreUtils.scala:86)
	at kafka.log.LogSegment.close(LogSegment.scala:566)
	at kafka.log.Log.$anonfun$close$4(Log.scala:714)
	at kafka.log.Log.$anonfun$close$4$adapted(Log.scala:714)
	at scala.collection.Iterator.foreach(Iterator.scala:937)
	at scala.collection.Iterator.foreach$(Iterator.scala:937)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1425)
	at scala.collection.IterableLike.foreach(IterableLike.scala:70)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:69)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at kafka.log.Log.$anonfun$close$3(Log.scala:714)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12)
	at kafka.log.Log.maybeHandleIOException(Log.scala:1927)
	at kafka.log.Log.close(Log.scala:709)
	at kafka.log.LogManager.$anonfun$shutdown$8(LogManager.scala:456)
	at kafka.utils.CoreUtils$$anon$1.run(CoreUtils.scala:63)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[2019-02-12 13:35:57,057] WARN Invalid argument (kafka.utils.CoreUtils$)
java.io.IOException: Invalid argument
	at java.io.RandomAccessFile.setLength(Native Method)
	at kafka.log.AbstractIndex.$anonfun$resize$1(AbstractIndex.scala:186)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:12)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.resize(AbstractIndex.scala:173)
	at kafka.log.AbstractIndex.$anonfun$trimToValidSize$1(AbstractIndex.scala:242)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:12)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.trimToValidSize(AbstractIndex.scala:242)
	at kafka.log.AbstractIndex.close(AbstractIndex.scala:253)
	at kafka.log.LogSegment.$anonfun$close$2(LogSegment.scala:565)
	at kafka.utils.CoreUtils$.swallow(CoreUtils.scala:86)
	at kafka.log.LogSegment.close(LogSegment.scala:565)
	at kafka.log.Log.$anonfun$close$4(Log.scala:714)
	at kafka.log.Log.$anonfun$close$4$adapted(Log.scala:714)
	at scala.collection.Iterator.foreach(Iterator.scala:937)
	at scala.collection.Iterator.foreach$(Iterator.scala:937)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1425)
	at scala.collection.IterableLike.foreach(IterableLike.scala:70)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:69)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at kafka.log.Log.$anonfun$close$3(Log.scala:714)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12)
	at kafka.log.Log.maybeHandleIOException(Log.scala:1927)
	at kafka.log.Log.close(Log.scala:709)
	at kafka.log.LogManager.$anonfun$shutdown$8(LogManager.scala:456)
	at kafka.utils.CoreUtils$$anon$1.run(CoreUtils.scala:63)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[2019-02-12 13:35:57,063] WARN Invalid argument (kafka.utils.CoreUtils$)
java.io.IOException: Invalid argument
	at java.io.RandomAccessFile.setLength(Native Method)
	at kafka.log.AbstractIndex.$anonfun$resize$1(AbstractIndex.scala:186)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:12)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.resize(AbstractIndex.scala:173)
	at kafka.log.TimeIndex.super$resize(TimeIndex.scala:189)
	at kafka.log.TimeIndex.$anonfun$resize$1(TimeIndex.scala:189)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:12)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.TimeIndex.resize(TimeIndex.scala:189)
	at kafka.log.AbstractIndex.$anonfun$trimToValidSize$1(AbstractIndex.scala:242)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:12)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.trimToValidSize(AbstractIndex.scala:242)
	at kafka.log.AbstractIndex.close(AbstractIndex.scala:253)
	at kafka.log.LogSegment.$anonfun$close$3(LogSegment.scala:566)
	at kafka.utils.CoreUtils$.swallow(CoreUtils.scala:86)
	at kafka.log.LogSegment.close(LogSegment.scala:566)
	at kafka.log.Log.$anonfun$close$4(Log.scala:714)
	at kafka.log.Log.$anonfun$close$4$adapted(Log.scala:714)
	at scala.collection.Iterator.foreach(Iterator.scala:937)
	at scala.collection.Iterator.foreach$(Iterator.scala:937)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1425)
	at scala.collection.IterableLike.foreach(IterableLike.scala:70)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:69)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at kafka.log.Log.$anonfun$close$3(Log.scala:714)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12)
	at kafka.log.Log.maybeHandleIOException(Log.scala:1927)
	at kafka.log.Log.close(Log.scala:709)
	at kafka.log.LogManager.$anonfun$shutdown$8(LogManager.scala:456)
	at kafka.utils.CoreUtils$$anon$1.run(CoreUtils.scala:63)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[2019-02-12 13:35:57,069] WARN Invalid argument (kafka.utils.CoreUtils$)
java.io.IOException: Invalid argument
	at java.io.RandomAccessFile.setLength(Native Method)
	at kafka.log.AbstractIndex.$anonfun$resize$1(AbstractIndex.scala:186)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:12)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.resize(AbstractIndex.scala:173)
	at kafka.log.AbstractIndex.$anonfun$trimToValidSize$1(AbstractIndex.scala:242)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:12)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.trimToValidSize(AbstractIndex.scala:242)
	at kafka.log.AbstractIndex.close(AbstractIndex.scala:253)
	at kafka.log.LogSegment.$anonfun$close$2(LogSegment.scala:565)
	at kafka.utils.CoreUtils$.swallow(CoreUtils.scala:86)
	at kafka.log.LogSegment.close(LogSegment.scala:565)
	at kafka.log.Log.$anonfun$close$4(Log.scala:714)
	at kafka.log.Log.$anonfun$close$4$adapted(Log.scala:714)
	at scala.collection.Iterator.foreach(Iterator.scala:937)
	at scala.collection.Iterator.foreach$(Iterator.scala:937)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1425)
	at scala.collection.IterableLike.foreach(IterableLike.scala:70)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:69)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at kafka.log.Log.$anonfun$close$3(Log.scala:714)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12)
	at kafka.log.Log.maybeHandleIOException(Log.scala:1927)
	at kafka.log.Log.close(Log.scala:709)
	at kafka.log.LogManager.$anonfun$shutdown$8(LogManager.scala:456)
	at kafka.utils.CoreUtils$$anon$1.run(CoreUtils.scala:63)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[2019-02-12 13:35:57,075] WARN Invalid argument (kafka.utils.CoreUtils$)
java.io.IOException: Invalid argument
	at java.io.RandomAccessFile.setLength(Native Method)
	at kafka.log.AbstractIndex.$anonfun$resize$1(AbstractIndex.scala:186)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:12)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.resize(AbstractIndex.scala:173)
	at kafka.log.TimeIndex.super$resize(TimeIndex.scala:189)
	at kafka.log.TimeIndex.$anonfun$resize$1(TimeIndex.scala:189)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:12)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.TimeIndex.resize(TimeIndex.scala:189)
	at kafka.log.AbstractIndex.$anonfun$trimToValidSize$1(AbstractIndex.scala:242)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:12)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.trimToValidSize(AbstractIndex.scala:242)
	at kafka.log.AbstractIndex.close(AbstractIndex.scala:253)
	at kafka.log.LogSegment.$anonfun$close$3(LogSegment.scala:566)
	at kafka.utils.CoreUtils$.swallow(CoreUtils.scala:86)
	at kafka.log.LogSegment.close(LogSegment.scala:566)
	at kafka.log.Log.$anonfun$close$4(Log.scala:714)
	at kafka.log.Log.$anonfun$close$4$adapted(Log.scala:714)
	at scala.collection.Iterator.foreach(Iterator.scala:937)
	at scala.collection.Iterator.foreach$(Iterator.scala:937)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1425)
	at scala.collection.IterableLike.foreach(IterableLike.scala:70)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:69)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at kafka.log.Log.$anonfun$close$3(Log.scala:714)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12)
	at kafka.log.Log.maybeHandleIOException(Log.scala:1927)
	at kafka.log.Log.close(Log.scala:709)
	at kafka.log.LogManager.$anonfun$shutdown$8(LogManager.scala:456)
	at kafka.utils.CoreUtils$$anon$1.run(CoreUtils.scala:63)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[2019-02-12 13:35:57,082] WARN Invalid argument (kafka.utils.CoreUtils$)
java.io.IOException: Invalid argument
	at java.io.RandomAccessFile.setLength(Native Method)
	at kafka.log.AbstractIndex.$anonfun$resize$1(AbstractIndex.scala:186)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:12)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.resize(AbstractIndex.scala:173)
	at kafka.log.AbstractIndex.$anonfun$trimToValidSize$1(AbstractIndex.scala:242)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:12)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.trimToValidSize(AbstractIndex.scala:242)
	at kafka.log.AbstractIndex.close(AbstractIndex.scala:253)
	at kafka.log.LogSegment.$anonfun$close$2(LogSegment.scala:565)
	at kafka.utils.CoreUtils$.swallow(CoreUtils.scala:86)
	at kafka.log.LogSegment.close(LogSegment.scala:565)
	at kafka.log.Log.$anonfun$close$4(Log.scala:714)
	at kafka.log.Log.$anonfun$close$4$adapted(Log.scala:714)
	at scala.collection.Iterator.foreach(Iterator.scala:937)
	at scala.collection.Iterator.foreach$(Iterator.scala:937)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1425)
	at scala.collection.IterableLike.foreach(IterableLike.scala:70)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:69)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at kafka.log.Log.$anonfun$close$3(Log.scala:714)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12)
	at kafka.log.Log.maybeHandleIOException(Log.scala:1927)
	at kafka.log.Log.close(Log.scala:709)
	at kafka.log.LogManager.$anonfun$shutdown$8(LogManager.scala:456)
	at kafka.utils.CoreUtils$$anon$1.run(CoreUtils.scala:63)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[2019-02-12 13:35:57,088] WARN Invalid argument (kafka.utils.CoreUtils$)
java.io.IOException: Invalid argument
	at java.io.RandomAccessFile.setLength(Native Method)
	at kafka.log.AbstractIndex.$anonfun$resize$1(AbstractIndex.scala:186)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:12)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.resize(AbstractIndex.scala:173)
	at kafka.log.TimeIndex.super$resize(TimeIndex.scala:189)
	at kafka.log.TimeIndex.$anonfun$resize$1(TimeIndex.scala:189)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:12)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.TimeIndex.resize(TimeIndex.scala:189)
	at kafka.log.AbstractIndex.$anonfun$trimToValidSize$1(AbstractIndex.scala:242)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:12)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.trimToValidSize(AbstractIndex.scala:242)
	at kafka.log.AbstractIndex.close(AbstractIndex.scala:253)
	at kafka.log.LogSegment.$anonfun$close$3(LogSegment.scala:566)
	at kafka.utils.CoreUtils$.swallow(CoreUtils.scala:86)
	at kafka.log.LogSegment.close(LogSegment.scala:566)
	at kafka.log.Log.$anonfun$close$4(Log.scala:714)
	at kafka.log.Log.$anonfun$close$4$adapted(Log.scala:714)
	at scala.collection.Iterator.foreach(Iterator.scala:937)
	at scala.collection.Iterator.foreach$(Iterator.scala:937)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1425)
	at scala.collection.IterableLike.foreach(IterableLike.scala:70)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:69)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at kafka.log.Log.$anonfun$close$3(Log.scala:714)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12)
	at kafka.log.Log.maybeHandleIOException(Log.scala:1927)
	at kafka.log.Log.close(Log.scala:709)
	at kafka.log.LogManager.$anonfun$shutdown$8(LogManager.scala:456)
	at kafka.utils.CoreUtils$$anon$1.run(CoreUtils.scala:63)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[2019-02-12 13:35:57,095] WARN Invalid argument (kafka.utils.CoreUtils$)
java.io.IOException: Invalid argument
	at java.io.RandomAccessFile.setLength(Native Method)
	at kafka.log.AbstractIndex.$anonfun$resize$1(AbstractIndex.scala:186)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:12)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.resize(AbstractIndex.scala:173)
	at kafka.log.AbstractIndex.$anonfun$trimToValidSize$1(AbstractIndex.scala:242)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:12)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.trimToValidSize(AbstractIndex.scala:242)
	at kafka.log.AbstractIndex.close(AbstractIndex.scala:253)
	at kafka.log.LogSegment.$anonfun$close$2(LogSegment.scala:565)
	at kafka.utils.CoreUtils$.swallow(CoreUtils.scala:86)
	at kafka.log.LogSegment.close(LogSegment.scala:565)
	at kafka.log.Log.$anonfun$close$4(Log.scala:714)
	at kafka.log.Log.$anonfun$close$4$adapted(Log.scala:714)
	at scala.collection.Iterator.foreach(Iterator.scala:937)
	at scala.collection.Iterator.foreach$(Iterator.scala:937)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1425)
	at scala.collection.IterableLike.foreach(IterableLike.scala:70)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:69)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at kafka.log.Log.$anonfun$close$3(Log.scala:714)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12)
	at kafka.log.Log.maybeHandleIOException(Log.scala:1927)
	at kafka.log.Log.close(Log.scala:709)
	at kafka.log.LogManager.$anonfun$shutdown$8(LogManager.scala:456)
	at kafka.utils.CoreUtils$$anon$1.run(CoreUtils.scala:63)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[2019-02-12 13:35:57,101] WARN Invalid argument (kafka.utils.CoreUtils$)
java.io.IOException: Invalid argument
	at java.io.RandomAccessFile.setLength(Native Method)
	at kafka.log.AbstractIndex.$anonfun$resize$1(AbstractIndex.scala:186)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:12)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.resize(AbstractIndex.scala:173)
	at kafka.log.TimeIndex.super$resize(TimeIndex.scala:189)
	at kafka.log.TimeIndex.$anonfun$resize$1(TimeIndex.scala:189)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:12)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.TimeIndex.resize(TimeIndex.scala:189)
	at kafka.log.AbstractIndex.$anonfun$trimToValidSize$1(AbstractIndex.scala:242)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:12)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.trimToValidSize(AbstractIndex.scala:242)
	at kafka.log.AbstractIndex.close(AbstractIndex.scala:253)
	at kafka.log.LogSegment.$anonfun$close$3(LogSegment.scala:566)
	at kafka.utils.CoreUtils$.swallow(CoreUtils.scala:86)
	at kafka.log.LogSegment.close(LogSegment.scala:566)
	at kafka.log.Log.$anonfun$close$4(Log.scala:714)
	at kafka.log.Log.$anonfun$close$4$adapted(Log.scala:714)
	at scala.collection.Iterator.foreach(Iterator.scala:937)
	at scala.collection.Iterator.foreach$(Iterator.scala:937)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1425)
	at scala.collection.IterableLike.foreach(IterableLike.scala:70)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:69)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at kafka.log.Log.$anonfun$close$3(Log.scala:714)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12)
	at kafka.log.Log.maybeHandleIOException(Log.scala:1927)
	at kafka.log.Log.close(Log.scala:709)
	at kafka.log.LogManager.$anonfun$shutdown$8(LogManager.scala:456)
	at kafka.utils.CoreUtils$$anon$1.run(CoreUtils.scala:63)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[2019-02-12 13:35:57,109] WARN Invalid argument (kafka.utils.CoreUtils$)
java.io.IOException: Invalid argument
	at java.io.RandomAccessFile.setLength(Native Method)
	at kafka.log.AbstractIndex.$anonfun$resize$1(AbstractIndex.scala:186)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:12)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.resize(AbstractIndex.scala:173)
	at kafka.log.AbstractIndex.$anonfun$trimToValidSize$1(AbstractIndex.scala:242)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:12)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.trimToValidSize(AbstractIndex.scala:242)
	at kafka.log.AbstractIndex.close(AbstractIndex.scala:253)
	at kafka.log.LogSegment.$anonfun$close$2(LogSegment.scala:565)
	at kafka.utils.CoreUtils$.swallow(CoreUtils.scala:86)
	at kafka.log.LogSegment.close(LogSegment.scala:565)
	at kafka.log.Log.$anonfun$close$4(Log.scala:714)
	at kafka.log.Log.$anonfun$close$4$adapted(Log.scala:714)
	at scala.collection.Iterator.foreach(Iterator.scala:937)
	at scala.collection.Iterator.foreach$(Iterator.scala:937)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1425)
	at scala.collection.IterableLike.foreach(IterableLike.scala:70)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:69)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at kafka.log.Log.$anonfun$close$3(Log.scala:714)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12)
	at kafka.log.Log.maybeHandleIOException(Log.scala:1927)
	at kafka.log.Log.close(Log.scala:709)
	at kafka.log.LogManager.$anonfun$shutdown$8(LogManager.scala:456)
	at kafka.utils.CoreUtils$$anon$1.run(CoreUtils.scala:63)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[2019-02-12 13:35:57,115] WARN Invalid argument (kafka.utils.CoreUtils$)
java.io.IOException: Invalid argument
	at java.io.RandomAccessFile.setLength(Native Method)
	at kafka.log.AbstractIndex.$anonfun$resize$1(AbstractIndex.scala:186)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:12)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.resize(AbstractIndex.scala:173)
	at kafka.log.TimeIndex.super$resize(TimeIndex.scala:189)
	at kafka.log.TimeIndex.$anonfun$resize$1(TimeIndex.scala:189)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:12)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.TimeIndex.resize(TimeIndex.scala:189)
	at kafka.log.AbstractIndex.$anonfun$trimToValidSize$1(AbstractIndex.scala:242)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:12)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.trimToValidSize(AbstractIndex.scala:242)
	at kafka.log.AbstractIndex.close(AbstractIndex.scala:253)
	at kafka.log.LogSegment.$anonfun$close$3(LogSegment.scala:566)
	at kafka.utils.CoreUtils$.swallow(CoreUtils.scala:86)
	at kafka.log.LogSegment.close(LogSegment.scala:566)
	at kafka.log.Log.$anonfun$close$4(Log.scala:714)
	at kafka.log.Log.$anonfun$close$4$adapted(Log.scala:714)
	at scala.collection.Iterator.foreach(Iterator.scala:937)
	at scala.collection.Iterator.foreach$(Iterator.scala:937)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1425)
	at scala.collection.IterableLike.foreach(IterableLike.scala:70)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:69)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at kafka.log.Log.$anonfun$close$3(Log.scala:714)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12)
	at kafka.log.Log.maybeHandleIOException(Log.scala:1927)
	at kafka.log.Log.close(Log.scala:709)
	at kafka.log.LogManager.$anonfun$shutdown$8(LogManager.scala:456)
	at kafka.utils.CoreUtils$$anon$1.run(CoreUtils.scala:63)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[2019-02-12 13:35:57,128] WARN Invalid argument (kafka.utils.CoreUtils$)
java.io.IOException: Invalid argument
	at java.io.RandomAccessFile.setLength(Native Method)
	at kafka.log.AbstractIndex.$anonfun$resize$1(AbstractIndex.scala:186)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:12)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.resize(AbstractIndex.scala:173)
	at kafka.log.AbstractIndex.$anonfun$trimToValidSize$1(AbstractIndex.scala:242)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:12)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.trimToValidSize(AbstractIndex.scala:242)
	at kafka.log.AbstractIndex.close(AbstractIndex.scala:253)
	at kafka.log.LogSegment.$anonfun$close$2(LogSegment.scala:565)
	at kafka.utils.CoreUtils$.swallow(CoreUtils.scala:86)
	at kafka.log.LogSegment.close(LogSegment.scala:565)
	at kafka.log.Log.$anonfun$close$4(Log.scala:714)
	at kafka.log.Log.$anonfun$close$4$adapted(Log.scala:714)
	at scala.collection.Iterator.foreach(Iterator.scala:937)
	at scala.collection.Iterator.foreach$(Iterator.scala:937)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1425)
	at scala.collection.IterableLike.foreach(IterableLike.scala:70)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:69)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at kafka.log.Log.$anonfun$close$3(Log.scala:714)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12)
	at kafka.log.Log.maybeHandleIOException(Log.scala:1927)
	at kafka.log.Log.close(Log.scala:709)
	at kafka.log.LogManager.$anonfun$shutdown$8(LogManager.scala:456)
	at kafka.utils.CoreUtils$$anon$1.run(CoreUtils.scala:63)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[2019-02-12 13:35:57,133] WARN Invalid argument (kafka.utils.CoreUtils$)
java.io.IOException: Invalid argument
	at java.io.RandomAccessFile.setLength(Native Method)
	at kafka.log.AbstractIndex.$anonfun$resize$1(AbstractIndex.scala:186)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:12)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.resize(AbstractIndex.scala:173)
	at kafka.log.TimeIndex.super$resize(TimeIndex.scala:189)
	at kafka.log.TimeIndex.$anonfun$resize$1(TimeIndex.scala:189)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:12)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.TimeIndex.resize(TimeIndex.scala:189)
	at kafka.log.AbstractIndex.$anonfun$trimToValidSize$1(AbstractIndex.scala:242)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:12)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.trimToValidSize(AbstractIndex.scala:242)
	at kafka.log.AbstractIndex.close(AbstractIndex.scala:253)
	at kafka.log.LogSegment.$anonfun$close$3(LogSegment.scala:566)
	at kafka.utils.CoreUtils$.swallow(CoreUtils.scala:86)
	at kafka.log.LogSegment.close(LogSegment.scala:566)
	at kafka.log.Log.$anonfun$close$4(Log.scala:714)
	at kafka.log.Log.$anonfun$close$4$adapted(Log.scala:714)
	at scala.collection.Iterator.foreach(Iterator.scala:937)
	at scala.collection.Iterator.foreach$(Iterator.scala:937)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1425)
	at scala.collection.IterableLike.foreach(IterableLike.scala:70)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:69)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at kafka.log.Log.$anonfun$close$3(Log.scala:714)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12)
	at kafka.log.Log.maybeHandleIOException(Log.scala:1927)
	at kafka.log.Log.close(Log.scala:709)
	at kafka.log.LogManager.$anonfun$shutdown$8(LogManager.scala:456)
	at kafka.utils.CoreUtils$$anon$1.run(CoreUtils.scala:63)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[2019-02-12 13:35:57,146] WARN Invalid argument (kafka.utils.CoreUtils$)
java.io.IOException: Invalid argument
	at java.io.RandomAccessFile.setLength(Native Method)
	at kafka.log.AbstractIndex.$anonfun$resize$1(AbstractIndex.scala:186)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:12)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.resize(AbstractIndex.scala:173)
	at kafka.log.AbstractIndex.$anonfun$trimToValidSize$1(AbstractIndex.scala:242)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:12)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.trimToValidSize(AbstractIndex.scala:242)
	at kafka.log.AbstractIndex.close(AbstractIndex.scala:253)
	at kafka.log.LogSegment.$anonfun$close$2(LogSegment.scala:565)
	at kafka.utils.CoreUtils$.swallow(CoreUtils.scala:86)
	at kafka.log.LogSegment.close(LogSegment.scala:565)
	at kafka.log.Log.$anonfun$close$4(Log.scala:714)
	at kafka.log.Log.$anonfun$close$4$adapted(Log.scala:714)
	at scala.collection.Iterator.foreach(Iterator.scala:937)
	at scala.collection.Iterator.foreach$(Iterator.scala:937)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1425)
	at scala.collection.IterableLike.foreach(IterableLike.scala:70)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:69)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at kafka.log.Log.$anonfun$close$3(Log.scala:714)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12)
	at kafka.log.Log.maybeHandleIOException(Log.scala:1927)
	at kafka.log.Log.close(Log.scala:709)
	at kafka.log.LogManager.$anonfun$shutdown$8(LogManager.scala:456)
	at kafka.utils.CoreUtils$$anon$1.run(CoreUtils.scala:63)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[2019-02-12 13:35:57,152] WARN Invalid argument (kafka.utils.CoreUtils$)
java.io.IOException: Invalid argument
	at java.io.RandomAccessFile.setLength(Native Method)
	at kafka.log.AbstractIndex.$anonfun$resize$1(AbstractIndex.scala:186)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:12)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.resize(AbstractIndex.scala:173)
	at kafka.log.TimeIndex.super$resize(TimeIndex.scala:189)
	at kafka.log.TimeIndex.$anonfun$resize$1(TimeIndex.scala:189)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:12)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.TimeIndex.resize(TimeIndex.scala:189)
	at kafka.log.AbstractIndex.$anonfun$trimToValidSize$1(AbstractIndex.scala:242)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:12)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.trimToValidSize(AbstractIndex.scala:242)
	at kafka.log.AbstractIndex.close(AbstractIndex.scala:253)
	at kafka.log.LogSegment.$anonfun$close$3(LogSegment.scala:566)
	at kafka.utils.CoreUtils$.swallow(CoreUtils.scala:86)
	at kafka.log.LogSegment.close(LogSegment.scala:566)
	at kafka.log.Log.$anonfun$close$4(Log.scala:714)
	at kafka.log.Log.$anonfun$close$4$adapted(Log.scala:714)
	at scala.collection.Iterator.foreach(Iterator.scala:937)
	at scala.collection.Iterator.foreach$(Iterator.scala:937)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1425)
	at scala.collection.IterableLike.foreach(IterableLike.scala:70)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:69)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at kafka.log.Log.$anonfun$close$3(Log.scala:714)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12)
	at kafka.log.Log.maybeHandleIOException(Log.scala:1927)
	at kafka.log.Log.close(Log.scala:709)
	at kafka.log.LogManager.$anonfun$shutdown$8(LogManager.scala:456)
	at kafka.utils.CoreUtils$$anon$1.run(CoreUtils.scala:63)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[2019-02-12 13:35:57,164] WARN Invalid argument (kafka.utils.CoreUtils$)
java.io.IOException: Invalid argument
	at java.io.RandomAccessFile.setLength(Native Method)
	at kafka.log.AbstractIndex.$anonfun$resize$1(AbstractIndex.scala:186)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:12)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.resize(AbstractIndex.scala:173)
	at kafka.log.AbstractIndex.$anonfun$trimToValidSize$1(AbstractIndex.scala:242)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:12)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.trimToValidSize(AbstractIndex.scala:242)
	at kafka.log.AbstractIndex.close(AbstractIndex.scala:253)
	at kafka.log.LogSegment.$anonfun$close$2(LogSegment.scala:565)
	at kafka.utils.CoreUtils$.swallow(CoreUtils.scala:86)
	at kafka.log.LogSegment.close(LogSegment.scala:565)
	at kafka.log.Log.$anonfun$close$4(Log.scala:714)
	at kafka.log.Log.$anonfun$close$4$adapted(Log.scala:714)
	at scala.collection.Iterator.foreach(Iterator.scala:937)
	at scala.collection.Iterator.foreach$(Iterator.scala:937)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1425)
	at scala.collection.IterableLike.foreach(IterableLike.scala:70)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:69)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at kafka.log.Log.$anonfun$close$3(Log.scala:714)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12)
	at kafka.log.Log.maybeHandleIOException(Log.scala:1927)
	at kafka.log.Log.close(Log.scala:709)
	at kafka.log.LogManager.$anonfun$shutdown$8(LogManager.scala:456)
	at kafka.utils.CoreUtils$$anon$1.run(CoreUtils.scala:63)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[2019-02-12 13:35:57,171] WARN Invalid argument (kafka.utils.CoreUtils$)
java.io.IOException: Invalid argument
	at java.io.RandomAccessFile.setLength(Native Method)
	at kafka.log.AbstractIndex.$anonfun$resize$1(AbstractIndex.scala:186)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:12)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.resize(AbstractIndex.scala:173)
	at kafka.log.TimeIndex.super$resize(TimeIndex.scala:189)
	at kafka.log.TimeIndex.$anonfun$resize$1(TimeIndex.scala:189)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:12)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.TimeIndex.resize(TimeIndex.scala:189)
	at kafka.log.AbstractIndex.$anonfun$trimToValidSize$1(AbstractIndex.scala:242)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:12)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.trimToValidSize(AbstractIndex.scala:242)
	at kafka.log.AbstractIndex.close(AbstractIndex.scala:253)
	at kafka.log.LogSegment.$anonfun$close$3(LogSegment.scala:566)
	at kafka.utils.CoreUtils$.swallow(CoreUtils.scala:86)
	at kafka.log.LogSegment.close(LogSegment.scala:566)
	at kafka.log.Log.$anonfun$close$4(Log.scala:714)
	at kafka.log.Log.$anonfun$close$4$adapted(Log.scala:714)
	at scala.collection.Iterator.foreach(Iterator.scala:937)
	at scala.collection.Iterator.foreach$(Iterator.scala:937)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1425)
	at scala.collection.IterableLike.foreach(IterableLike.scala:70)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:69)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at kafka.log.Log.$anonfun$close$3(Log.scala:714)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12)
	at kafka.log.Log.maybeHandleIOException(Log.scala:1927)
	at kafka.log.Log.close(Log.scala:709)
	at kafka.log.LogManager.$anonfun$shutdown$8(LogManager.scala:456)
	at kafka.utils.CoreUtils$$anon$1.run(CoreUtils.scala:63)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[2019-02-12 13:35:57,185] WARN Invalid argument (kafka.utils.CoreUtils$)
java.io.IOException: Invalid argument
	at java.io.RandomAccessFile.setLength(Native Method)
	at kafka.log.AbstractIndex.$anonfun$resize$1(AbstractIndex.scala:186)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:12)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.resize(AbstractIndex.scala:173)
	at kafka.log.AbstractIndex.$anonfun$trimToValidSize$1(AbstractIndex.scala:242)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:12)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.trimToValidSize(AbstractIndex.scala:242)
	at kafka.log.AbstractIndex.close(AbstractIndex.scala:253)
	at kafka.log.LogSegment.$anonfun$close$2(LogSegment.scala:565)
	at kafka.utils.CoreUtils$.swallow(CoreUtils.scala:86)
	at kafka.log.LogSegment.close(LogSegment.scala:565)
	at kafka.log.Log.$anonfun$close$4(Log.scala:714)
	at kafka.log.Log.$anonfun$close$4$adapted(Log.scala:714)
	at scala.collection.Iterator.foreach(Iterator.scala:937)
	at scala.collection.Iterator.foreach$(Iterator.scala:937)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1425)
	at scala.collection.IterableLike.foreach(IterableLike.scala:70)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:69)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at kafka.log.Log.$anonfun$close$3(Log.scala:714)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12)
	at kafka.log.Log.maybeHandleIOException(Log.scala:1927)
	at kafka.log.Log.close(Log.scala:709)
	at kafka.log.LogManager.$anonfun$shutdown$8(LogManager.scala:456)
	at kafka.utils.CoreUtils$$anon$1.run(CoreUtils.scala:63)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[2019-02-12 13:35:57,191] WARN Invalid argument (kafka.utils.CoreUtils$)
java.io.IOException: Invalid argument
	at java.io.RandomAccessFile.setLength(Native Method)
	at kafka.log.AbstractIndex.$anonfun$resize$1(AbstractIndex.scala:186)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:12)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.resize(AbstractIndex.scala:173)
	at kafka.log.TimeIndex.super$resize(TimeIndex.scala:189)
	at kafka.log.TimeIndex.$anonfun$resize$1(TimeIndex.scala:189)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:12)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.TimeIndex.resize(TimeIndex.scala:189)
	at kafka.log.AbstractIndex.$anonfun$trimToValidSize$1(AbstractIndex.scala:242)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:12)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.trimToValidSize(AbstractIndex.scala:242)
	at kafka.log.AbstractIndex.close(AbstractIndex.scala:253)
	at kafka.log.LogSegment.$anonfun$close$3(LogSegment.scala:566)
	at kafka.utils.CoreUtils$.swallow(CoreUtils.scala:86)
	at kafka.log.LogSegment.close(LogSegment.scala:566)
	at kafka.log.Log.$anonfun$close$4(Log.scala:714)
	at kafka.log.Log.$anonfun$close$4$adapted(Log.scala:714)
	at scala.collection.Iterator.foreach(Iterator.scala:937)
	at scala.collection.Iterator.foreach$(Iterator.scala:937)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1425)
	at scala.collection.IterableLike.foreach(IterableLike.scala:70)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:69)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at kafka.log.Log.$anonfun$close$3(Log.scala:714)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12)
	at kafka.log.Log.maybeHandleIOException(Log.scala:1927)
	at kafka.log.Log.close(Log.scala:709)
	at kafka.log.LogManager.$anonfun$shutdown$8(LogManager.scala:456)
	at kafka.utils.CoreUtils$$anon$1.run(CoreUtils.scala:63)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[2019-02-12 13:35:57,198] WARN Invalid argument (kafka.utils.CoreUtils$)
java.io.IOException: Invalid argument
	at java.io.RandomAccessFile.setLength(Native Method)
	at kafka.log.AbstractIndex.$anonfun$resize$1(AbstractIndex.scala:186)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:12)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.resize(AbstractIndex.scala:173)
	at kafka.log.AbstractIndex.$anonfun$trimToValidSize$1(AbstractIndex.scala:242)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:12)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.trimToValidSize(AbstractIndex.scala:242)
	at kafka.log.AbstractIndex.close(AbstractIndex.scala:253)
	at kafka.log.LogSegment.$anonfun$close$2(LogSegment.scala:565)
	at kafka.utils.CoreUtils$.swallow(CoreUtils.scala:86)
	at kafka.log.LogSegment.close(LogSegment.scala:565)
	at kafka.log.Log.$anonfun$close$4(Log.scala:714)
	at kafka.log.Log.$anonfun$close$4$adapted(Log.scala:714)
	at scala.collection.Iterator.foreach(Iterator.scala:937)
	at scala.collection.Iterator.foreach$(Iterator.scala:937)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1425)
	at scala.collection.IterableLike.foreach(IterableLike.scala:70)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:69)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at kafka.log.Log.$anonfun$close$3(Log.scala:714)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12)
	at kafka.log.Log.maybeHandleIOException(Log.scala:1927)
	at kafka.log.Log.close(Log.scala:709)
	at kafka.log.LogManager.$anonfun$shutdown$8(LogManager.scala:456)
	at kafka.utils.CoreUtils$$anon$1.run(CoreUtils.scala:63)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[2019-02-12 13:35:57,205] WARN Invalid argument (kafka.utils.CoreUtils$)
java.io.IOException: Invalid argument
	at java.io.RandomAccessFile.setLength(Native Method)
	at kafka.log.AbstractIndex.$anonfun$resize$1(AbstractIndex.scala:186)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:12)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.resize(AbstractIndex.scala:173)
	at kafka.log.TimeIndex.super$resize(TimeIndex.scala:189)
	at kafka.log.TimeIndex.$anonfun$resize$1(TimeIndex.scala:189)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:12)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.TimeIndex.resize(TimeIndex.scala:189)
	at kafka.log.AbstractIndex.$anonfun$trimToValidSize$1(AbstractIndex.scala:242)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:12)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.trimToValidSize(AbstractIndex.scala:242)
	at kafka.log.AbstractIndex.close(AbstractIndex.scala:253)
	at kafka.log.LogSegment.$anonfun$close$3(LogSegment.scala:566)
	at kafka.utils.CoreUtils$.swallow(CoreUtils.scala:86)
	at kafka.log.LogSegment.close(LogSegment.scala:566)
	at kafka.log.Log.$anonfun$close$4(Log.scala:714)
	at kafka.log.Log.$anonfun$close$4$adapted(Log.scala:714)
	at scala.collection.Iterator.foreach(Iterator.scala:937)
	at scala.collection.Iterator.foreach$(Iterator.scala:937)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1425)
	at scala.collection.IterableLike.foreach(IterableLike.scala:70)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:69)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at kafka.log.Log.$anonfun$close$3(Log.scala:714)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12)
	at kafka.log.Log.maybeHandleIOException(Log.scala:1927)
	at kafka.log.Log.close(Log.scala:709)
	at kafka.log.LogManager.$anonfun$shutdown$8(LogManager.scala:456)
	at kafka.utils.CoreUtils$$anon$1.run(CoreUtils.scala:63)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[2019-02-12 13:35:57,213] WARN Invalid argument (kafka.utils.CoreUtils$)
java.io.IOException: Invalid argument
	at java.io.RandomAccessFile.setLength(Native Method)
	at kafka.log.AbstractIndex.$anonfun$resize$1(AbstractIndex.scala:186)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:12)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.resize(AbstractIndex.scala:173)
	at kafka.log.AbstractIndex.$anonfun$trimToValidSize$1(AbstractIndex.scala:242)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:12)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.trimToValidSize(AbstractIndex.scala:242)
	at kafka.log.AbstractIndex.close(AbstractIndex.scala:253)
	at kafka.log.LogSegment.$anonfun$close$2(LogSegment.scala:565)
	at kafka.utils.CoreUtils$.swallow(CoreUtils.scala:86)
	at kafka.log.LogSegment.close(LogSegment.scala:565)
	at kafka.log.Log.$anonfun$close$4(Log.scala:714)
	at kafka.log.Log.$anonfun$close$4$adapted(Log.scala:714)
	at scala.collection.Iterator.foreach(Iterator.scala:937)
	at scala.collection.Iterator.foreach$(Iterator.scala:937)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1425)
	at scala.collection.IterableLike.foreach(IterableLike.scala:70)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:69)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at kafka.log.Log.$anonfun$close$3(Log.scala:714)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12)
	at kafka.log.Log.maybeHandleIOException(Log.scala:1927)
	at kafka.log.Log.close(Log.scala:709)
	at kafka.log.LogManager.$anonfun$shutdown$8(LogManager.scala:456)
	at kafka.utils.CoreUtils$$anon$1.run(CoreUtils.scala:63)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[2019-02-12 13:35:57,220] WARN Invalid argument (kafka.utils.CoreUtils$)
java.io.IOException: Invalid argument
	at java.io.RandomAccessFile.setLength(Native Method)
	at kafka.log.AbstractIndex.$anonfun$resize$1(AbstractIndex.scala:186)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:12)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.resize(AbstractIndex.scala:173)
	at kafka.log.TimeIndex.super$resize(TimeIndex.scala:189)
	at kafka.log.TimeIndex.$anonfun$resize$1(TimeIndex.scala:189)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:12)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.TimeIndex.resize(TimeIndex.scala:189)
	at kafka.log.AbstractIndex.$anonfun$trimToValidSize$1(AbstractIndex.scala:242)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:12)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.trimToValidSize(AbstractIndex.scala:242)
	at kafka.log.AbstractIndex.close(AbstractIndex.scala:253)
	at kafka.log.LogSegment.$anonfun$close$3(LogSegment.scala:566)
	at kafka.utils.CoreUtils$.swallow(CoreUtils.scala:86)
	at kafka.log.LogSegment.close(LogSegment.scala:566)
	at kafka.log.Log.$anonfun$close$4(Log.scala:714)
	at kafka.log.Log.$anonfun$close$4$adapted(Log.scala:714)
	at scala.collection.Iterator.foreach(Iterator.scala:937)
	at scala.collection.Iterator.foreach$(Iterator.scala:937)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1425)
	at scala.collection.IterableLike.foreach(IterableLike.scala:70)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:69)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at kafka.log.Log.$anonfun$close$3(Log.scala:714)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12)
	at kafka.log.Log.maybeHandleIOException(Log.scala:1927)
	at kafka.log.Log.close(Log.scala:709)
	at kafka.log.LogManager.$anonfun$shutdown$8(LogManager.scala:456)
	at kafka.utils.CoreUtils$$anon$1.run(CoreUtils.scala:63)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[2019-02-12 13:35:57,227] WARN Invalid argument (kafka.utils.CoreUtils$)
java.io.IOException: Invalid argument
	at java.io.RandomAccessFile.setLength(Native Method)
	at kafka.log.AbstractIndex.$anonfun$resize$1(AbstractIndex.scala:186)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:12)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.resize(AbstractIndex.scala:173)
	at kafka.log.AbstractIndex.$anonfun$trimToValidSize$1(AbstractIndex.scala:242)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:12)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.trimToValidSize(AbstractIndex.scala:242)
	at kafka.log.AbstractIndex.close(AbstractIndex.scala:253)
	at kafka.log.LogSegment.$anonfun$close$2(LogSegment.scala:565)
	at kafka.utils.CoreUtils$.swallow(CoreUtils.scala:86)
	at kafka.log.LogSegment.close(LogSegment.scala:565)
	at kafka.log.Log.$anonfun$close$4(Log.scala:714)
	at kafka.log.Log.$anonfun$close$4$adapted(Log.scala:714)
	at scala.collection.Iterator.foreach(Iterator.scala:937)
	at scala.collection.Iterator.foreach$(Iterator.scala:937)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1425)
	at scala.collection.IterableLike.foreach(IterableLike.scala:70)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:69)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at kafka.log.Log.$anonfun$close$3(Log.scala:714)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12)
	at kafka.log.Log.maybeHandleIOException(Log.scala:1927)
	at kafka.log.Log.close(Log.scala:709)
	at kafka.log.LogManager.$anonfun$shutdown$8(LogManager.scala:456)
	at kafka.utils.CoreUtils$$anon$1.run(CoreUtils.scala:63)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[2019-02-12 13:35:57,233] WARN Invalid argument (kafka.utils.CoreUtils$)
java.io.IOException: Invalid argument
	at java.io.RandomAccessFile.setLength(Native Method)
	at kafka.log.AbstractIndex.$anonfun$resize$1(AbstractIndex.scala:186)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:12)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.resize(AbstractIndex.scala:173)
	at kafka.log.TimeIndex.super$resize(TimeIndex.scala:189)
	at kafka.log.TimeIndex.$anonfun$resize$1(TimeIndex.scala:189)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:12)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.TimeIndex.resize(TimeIndex.scala:189)
	at kafka.log.AbstractIndex.$anonfun$trimToValidSize$1(AbstractIndex.scala:242)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:12)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.trimToValidSize(AbstractIndex.scala:242)
	at kafka.log.AbstractIndex.close(AbstractIndex.scala:253)
	at kafka.log.LogSegment.$anonfun$close$3(LogSegment.scala:566)
	at kafka.utils.CoreUtils$.swallow(CoreUtils.scala:86)
	at kafka.log.LogSegment.close(LogSegment.scala:566)
	at kafka.log.Log.$anonfun$close$4(Log.scala:714)
	at kafka.log.Log.$anonfun$close$4$adapted(Log.scala:714)
	at scala.collection.Iterator.foreach(Iterator.scala:937)
	at scala.collection.Iterator.foreach$(Iterator.scala:937)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1425)
	at scala.collection.IterableLike.foreach(IterableLike.scala:70)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:69)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at kafka.log.Log.$anonfun$close$3(Log.scala:714)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12)
	at kafka.log.Log.maybeHandleIOException(Log.scala:1927)
	at kafka.log.Log.close(Log.scala:709)
	at kafka.log.LogManager.$anonfun$shutdown$8(LogManager.scala:456)
	at kafka.utils.CoreUtils$$anon$1.run(CoreUtils.scala:63)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[2019-02-12 13:35:57,240] WARN Invalid argument (kafka.utils.CoreUtils$)
java.io.IOException: Invalid argument
	at java.io.RandomAccessFile.setLength(Native Method)
	at kafka.log.AbstractIndex.$anonfun$resize$1(AbstractIndex.scala:186)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:12)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.resize(AbstractIndex.scala:173)
	at kafka.log.AbstractIndex.$anonfun$trimToValidSize$1(AbstractIndex.scala:242)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:12)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.trimToValidSize(AbstractIndex.scala:242)
	at kafka.log.AbstractIndex.close(AbstractIndex.scala:253)
	at kafka.log.LogSegment.$anonfun$close$2(LogSegment.scala:565)
	at kafka.utils.CoreUtils$.swallow(CoreUtils.scala:86)
	at kafka.log.LogSegment.close(LogSegment.scala:565)
	at kafka.log.Log.$anonfun$close$4(Log.scala:714)
	at kafka.log.Log.$anonfun$close$4$adapted(Log.scala:714)
	at scala.collection.Iterator.foreach(Iterator.scala:937)
	at scala.collection.Iterator.foreach$(Iterator.scala:937)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1425)
	at scala.collection.IterableLike.foreach(IterableLike.scala:70)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:69)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at kafka.log.Log.$anonfun$close$3(Log.scala:714)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12)
	at kafka.log.Log.maybeHandleIOException(Log.scala:1927)
	at kafka.log.Log.close(Log.scala:709)
	at kafka.log.LogManager.$anonfun$shutdown$8(LogManager.scala:456)
	at kafka.utils.CoreUtils$$anon$1.run(CoreUtils.scala:63)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[2019-02-12 13:35:57,246] WARN Invalid argument (kafka.utils.CoreUtils$)
java.io.IOException: Invalid argument
	at java.io.RandomAccessFile.setLength(Native Method)
	at kafka.log.AbstractIndex.$anonfun$resize$1(AbstractIndex.scala:186)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:12)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.resize(AbstractIndex.scala:173)
	at kafka.log.TimeIndex.super$resize(TimeIndex.scala:189)
	at kafka.log.TimeIndex.$anonfun$resize$1(TimeIndex.scala:189)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:12)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.TimeIndex.resize(TimeIndex.scala:189)
	at kafka.log.AbstractIndex.$anonfun$trimToValidSize$1(AbstractIndex.scala:242)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:12)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.trimToValidSize(AbstractIndex.scala:242)
	at kafka.log.AbstractIndex.close(AbstractIndex.scala:253)
	at kafka.log.LogSegment.$anonfun$close$3(LogSegment.scala:566)
	at kafka.utils.CoreUtils$.swallow(CoreUtils.scala:86)
	at kafka.log.LogSegment.close(LogSegment.scala:566)
	at kafka.log.Log.$anonfun$close$4(Log.scala:714)
	at kafka.log.Log.$anonfun$close$4$adapted(Log.scala:714)
	at scala.collection.Iterator.foreach(Iterator.scala:937)
	at scala.collection.Iterator.foreach$(Iterator.scala:937)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1425)
	at scala.collection.IterableLike.foreach(IterableLike.scala:70)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:69)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at kafka.log.Log.$anonfun$close$3(Log.scala:714)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12)
	at kafka.log.Log.maybeHandleIOException(Log.scala:1927)
	at kafka.log.Log.close(Log.scala:709)
	at kafka.log.LogManager.$anonfun$shutdown$8(LogManager.scala:456)
	at kafka.utils.CoreUtils$$anon$1.run(CoreUtils.scala:63)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[2019-02-12 13:35:57,253] WARN Invalid argument (kafka.utils.CoreUtils$)
java.io.IOException: Invalid argument
	at java.io.RandomAccessFile.setLength(Native Method)
	at kafka.log.AbstractIndex.$anonfun$resize$1(AbstractIndex.scala:186)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:12)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.resize(AbstractIndex.scala:173)
	at kafka.log.AbstractIndex.$anonfun$trimToValidSize$1(AbstractIndex.scala:242)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:12)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.trimToValidSize(AbstractIndex.scala:242)
	at kafka.log.AbstractIndex.close(AbstractIndex.scala:253)
	at kafka.log.LogSegment.$anonfun$close$2(LogSegment.scala:565)
	at kafka.utils.CoreUtils$.swallow(CoreUtils.scala:86)
	at kafka.log.LogSegment.close(LogSegment.scala:565)
	at kafka.log.Log.$anonfun$close$4(Log.scala:714)
	at kafka.log.Log.$anonfun$close$4$adapted(Log.scala:714)
	at scala.collection.Iterator.foreach(Iterator.scala:937)
	at scala.collection.Iterator.foreach$(Iterator.scala:937)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1425)
	at scala.collection.IterableLike.foreach(IterableLike.scala:70)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:69)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at kafka.log.Log.$anonfun$close$3(Log.scala:714)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12)
	at kafka.log.Log.maybeHandleIOException(Log.scala:1927)
	at kafka.log.Log.close(Log.scala:709)
	at kafka.log.LogManager.$anonfun$shutdown$8(LogManager.scala:456)
	at kafka.utils.CoreUtils$$anon$1.run(CoreUtils.scala:63)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[2019-02-12 13:35:57,260] WARN Invalid argument (kafka.utils.CoreUtils$)
java.io.IOException: Invalid argument
	at java.io.RandomAccessFile.setLength(Native Method)
	at kafka.log.AbstractIndex.$anonfun$resize$1(AbstractIndex.scala:186)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:12)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.resize(AbstractIndex.scala:173)
	at kafka.log.TimeIndex.super$resize(TimeIndex.scala:189)
	at kafka.log.TimeIndex.$anonfun$resize$1(TimeIndex.scala:189)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:12)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.TimeIndex.resize(TimeIndex.scala:189)
	at kafka.log.AbstractIndex.$anonfun$trimToValidSize$1(AbstractIndex.scala:242)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:12)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.trimToValidSize(AbstractIndex.scala:242)
	at kafka.log.AbstractIndex.close(AbstractIndex.scala:253)
	at kafka.log.LogSegment.$anonfun$close$3(LogSegment.scala:566)
	at kafka.utils.CoreUtils$.swallow(CoreUtils.scala:86)
	at kafka.log.LogSegment.close(LogSegment.scala:566)
	at kafka.log.Log.$anonfun$close$4(Log.scala:714)
	at kafka.log.Log.$anonfun$close$4$adapted(Log.scala:714)
	at scala.collection.Iterator.foreach(Iterator.scala:937)
	at scala.collection.Iterator.foreach$(Iterator.scala:937)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1425)
	at scala.collection.IterableLike.foreach(IterableLike.scala:70)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:69)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at kafka.log.Log.$anonfun$close$3(Log.scala:714)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12)
	at kafka.log.Log.maybeHandleIOException(Log.scala:1927)
	at kafka.log.Log.close(Log.scala:709)
	at kafka.log.LogManager.$anonfun$shutdown$8(LogManager.scala:456)
	at kafka.utils.CoreUtils$$anon$1.run(CoreUtils.scala:63)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[2019-02-12 13:35:57,267] WARN Invalid argument (kafka.utils.CoreUtils$)
java.io.IOException: Invalid argument
	at java.io.RandomAccessFile.setLength(Native Method)
	at kafka.log.AbstractIndex.$anonfun$resize$1(AbstractIndex.scala:186)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:12)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.resize(AbstractIndex.scala:173)
	at kafka.log.AbstractIndex.$anonfun$trimToValidSize$1(AbstractIndex.scala:242)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:12)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.trimToValidSize(AbstractIndex.scala:242)
	at kafka.log.AbstractIndex.close(AbstractIndex.scala:253)
	at kafka.log.LogSegment.$anonfun$close$2(LogSegment.scala:565)
	at kafka.utils.CoreUtils$.swallow(CoreUtils.scala:86)
	at kafka.log.LogSegment.close(LogSegment.scala:565)
	at kafka.log.Log.$anonfun$close$4(Log.scala:714)
	at kafka.log.Log.$anonfun$close$4$adapted(Log.scala:714)
	at scala.collection.Iterator.foreach(Iterator.scala:937)
	at scala.collection.Iterator.foreach$(Iterator.scala:937)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1425)
	at scala.collection.IterableLike.foreach(IterableLike.scala:70)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:69)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at kafka.log.Log.$anonfun$close$3(Log.scala:714)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12)
	at kafka.log.Log.maybeHandleIOException(Log.scala:1927)
	at kafka.log.Log.close(Log.scala:709)
	at kafka.log.LogManager.$anonfun$shutdown$8(LogManager.scala:456)
	at kafka.utils.CoreUtils$$anon$1.run(CoreUtils.scala:63)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[2019-02-12 13:35:57,273] WARN Invalid argument (kafka.utils.CoreUtils$)
java.io.IOException: Invalid argument
	at java.io.RandomAccessFile.setLength(Native Method)
	at kafka.log.AbstractIndex.$anonfun$resize$1(AbstractIndex.scala:186)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:12)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.resize(AbstractIndex.scala:173)
	at kafka.log.TimeIndex.super$resize(TimeIndex.scala:189)
	at kafka.log.TimeIndex.$anonfun$resize$1(TimeIndex.scala:189)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:12)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.TimeIndex.resize(TimeIndex.scala:189)
	at kafka.log.AbstractIndex.$anonfun$trimToValidSize$1(AbstractIndex.scala:242)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:12)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.trimToValidSize(AbstractIndex.scala:242)
	at kafka.log.AbstractIndex.close(AbstractIndex.scala:253)
	at kafka.log.LogSegment.$anonfun$close$3(LogSegment.scala:566)
	at kafka.utils.CoreUtils$.swallow(CoreUtils.scala:86)
	at kafka.log.LogSegment.close(LogSegment.scala:566)
	at kafka.log.Log.$anonfun$close$4(Log.scala:714)
	at kafka.log.Log.$anonfun$close$4$adapted(Log.scala:714)
	at scala.collection.Iterator.foreach(Iterator.scala:937)
	at scala.collection.Iterator.foreach$(Iterator.scala:937)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1425)
	at scala.collection.IterableLike.foreach(IterableLike.scala:70)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:69)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at kafka.log.Log.$anonfun$close$3(Log.scala:714)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12)
	at kafka.log.Log.maybeHandleIOException(Log.scala:1927)
	at kafka.log.Log.close(Log.scala:709)
	at kafka.log.LogManager.$anonfun$shutdown$8(LogManager.scala:456)
	at kafka.utils.CoreUtils$$anon$1.run(CoreUtils.scala:63)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[2019-02-12 13:35:57,280] WARN Invalid argument (kafka.utils.CoreUtils$)
java.io.IOException: Invalid argument
	at java.io.RandomAccessFile.setLength(Native Method)
	at kafka.log.AbstractIndex.$anonfun$resize$1(AbstractIndex.scala:186)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:12)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.resize(AbstractIndex.scala:173)
	at kafka.log.AbstractIndex.$anonfun$trimToValidSize$1(AbstractIndex.scala:242)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:12)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.trimToValidSize(AbstractIndex.scala:242)
	at kafka.log.AbstractIndex.close(AbstractIndex.scala:253)
	at kafka.log.LogSegment.$anonfun$close$2(LogSegment.scala:565)
	at kafka.utils.CoreUtils$.swallow(CoreUtils.scala:86)
	at kafka.log.LogSegment.close(LogSegment.scala:565)
	at kafka.log.Log.$anonfun$close$4(Log.scala:714)
	at kafka.log.Log.$anonfun$close$4$adapted(Log.scala:714)
	at scala.collection.Iterator.foreach(Iterator.scala:937)
	at scala.collection.Iterator.foreach$(Iterator.scala:937)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1425)
	at scala.collection.IterableLike.foreach(IterableLike.scala:70)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:69)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at kafka.log.Log.$anonfun$close$3(Log.scala:714)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12)
	at kafka.log.Log.maybeHandleIOException(Log.scala:1927)
	at kafka.log.Log.close(Log.scala:709)
	at kafka.log.LogManager.$anonfun$shutdown$8(LogManager.scala:456)
	at kafka.utils.CoreUtils$$anon$1.run(CoreUtils.scala:63)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[2019-02-12 13:35:57,286] WARN Invalid argument (kafka.utils.CoreUtils$)
java.io.IOException: Invalid argument
	at java.io.RandomAccessFile.setLength(Native Method)
	at kafka.log.AbstractIndex.$anonfun$resize$1(AbstractIndex.scala:186)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:12)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.resize(AbstractIndex.scala:173)
	at kafka.log.TimeIndex.super$resize(TimeIndex.scala:189)
	at kafka.log.TimeIndex.$anonfun$resize$1(TimeIndex.scala:189)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:12)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.TimeIndex.resize(TimeIndex.scala:189)
	at kafka.log.AbstractIndex.$anonfun$trimToValidSize$1(AbstractIndex.scala:242)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:12)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.trimToValidSize(AbstractIndex.scala:242)
	at kafka.log.AbstractIndex.close(AbstractIndex.scala:253)
	at kafka.log.LogSegment.$anonfun$close$3(LogSegment.scala:566)
	at kafka.utils.CoreUtils$.swallow(CoreUtils.scala:86)
	at kafka.log.LogSegment.close(LogSegment.scala:566)
	at kafka.log.Log.$anonfun$close$4(Log.scala:714)
	at kafka.log.Log.$anonfun$close$4$adapted(Log.scala:714)
	at scala.collection.Iterator.foreach(Iterator.scala:937)
	at scala.collection.Iterator.foreach$(Iterator.scala:937)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1425)
	at scala.collection.IterableLike.foreach(IterableLike.scala:70)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:69)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at kafka.log.Log.$anonfun$close$3(Log.scala:714)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12)
	at kafka.log.Log.maybeHandleIOException(Log.scala:1927)
	at kafka.log.Log.close(Log.scala:709)
	at kafka.log.LogManager.$anonfun$shutdown$8(LogManager.scala:456)
	at kafka.utils.CoreUtils$$anon$1.run(CoreUtils.scala:63)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[2019-02-12 13:35:57,293] WARN Invalid argument (kafka.utils.CoreUtils$)
java.io.IOException: Invalid argument
	at java.io.RandomAccessFile.setLength(Native Method)
	at kafka.log.AbstractIndex.$anonfun$resize$1(AbstractIndex.scala:186)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:12)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.resize(AbstractIndex.scala:173)
	at kafka.log.AbstractIndex.$anonfun$trimToValidSize$1(AbstractIndex.scala:242)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:12)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.trimToValidSize(AbstractIndex.scala:242)
	at kafka.log.AbstractIndex.close(AbstractIndex.scala:253)
	at kafka.log.LogSegment.$anonfun$close$2(LogSegment.scala:565)
	at kafka.utils.CoreUtils$.swallow(CoreUtils.scala:86)
	at kafka.log.LogSegment.close(LogSegment.scala:565)
	at kafka.log.Log.$anonfun$close$4(Log.scala:714)
	at kafka.log.Log.$anonfun$close$4$adapted(Log.scala:714)
	at scala.collection.Iterator.foreach(Iterator.scala:937)
	at scala.collection.Iterator.foreach$(Iterator.scala:937)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1425)
	at scala.collection.IterableLike.foreach(IterableLike.scala:70)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:69)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at kafka.log.Log.$anonfun$close$3(Log.scala:714)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12)
	at kafka.log.Log.maybeHandleIOException(Log.scala:1927)
	at kafka.log.Log.close(Log.scala:709)
	at kafka.log.LogManager.$anonfun$shutdown$8(LogManager.scala:456)
	at kafka.utils.CoreUtils$$anon$1.run(CoreUtils.scala:63)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[2019-02-12 13:35:57,300] WARN Invalid argument (kafka.utils.CoreUtils$)
java.io.IOException: Invalid argument
	at java.io.RandomAccessFile.setLength(Native Method)
	at kafka.log.AbstractIndex.$anonfun$resize$1(AbstractIndex.scala:186)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:12)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.resize(AbstractIndex.scala:173)
	at kafka.log.TimeIndex.super$resize(TimeIndex.scala:189)
	at kafka.log.TimeIndex.$anonfun$resize$1(TimeIndex.scala:189)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:12)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.TimeIndex.resize(TimeIndex.scala:189)
	at kafka.log.AbstractIndex.$anonfun$trimToValidSize$1(AbstractIndex.scala:242)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:12)
	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
	at kafka.log.AbstractIndex.trimToValidSize(AbstractIndex.scala:242)
	at kafka.log.AbstractIndex.close(AbstractIndex.scala:253)
	at kafka.log.LogSegment.$anonfun$close$3(LogSegment.scala:566)
	at kafka.utils.CoreUtils$.swallow(CoreUtils.scala:86)
	at kafka.log.LogSegment.close(LogSegment.scala:566)
	at kafka.log.Log.$anonfun$close$4(Log.scala:714)
	at kafka.log.Log.$anonfun$close$4$adapted(Log.scala:714)
	at scala.collection.Iterator.foreach(Iterator.scala:937)
	at scala.collection.Iterator.foreach$(Iterator.scala:937)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1425)
	at scala.collection.IterableLike.foreach(IterableLike.scala:70)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:69)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at kafka.log.Log.$anonfun$close$3(Log.scala:714)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12)
	at kafka.log.Log.maybeHandleIOException(Log.scala:1927)
	at kafka.log.Log.close(Log.scala:709)
	at kafka.log.LogManager.$anonfun$shutdown$8(LogManager.scala:456)
	at kafka.utils.CoreUtils$$anon$1.run(CoreUtils.scala:63)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[2019-02-12 13:35:57,344] INFO Shutdown complete. (kafka.log.LogManager)
[2019-02-12 13:35:57,987] WARN [Controller id=0, targetBrokerId=0] Connection to node 0 (DESKTOP-7K1FP8K.localdomain/127.0.1.1:9092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient)
[2019-02-12 13:35:57,991] INFO [ZooKeeperClient] Closing. (kafka.zookeeper.ZooKeeperClient)
[2019-02-12 13:35:58,001] INFO Session: 0x1000001b4720005 closed (org.apache.zookeeper.ZooKeeper)
[2019-02-12 13:35:58,002] INFO EventThread shut down for session: 0x1000001b4720005 (org.apache.zookeeper.ClientCnxn)
[2019-02-12 13:35:58,003] INFO [ZooKeeperClient] Closed. (kafka.zookeeper.ZooKeeperClient)
[2019-02-12 13:35:58,003] INFO [ThrottledChannelReaper-Fetch]: Shutting down (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2019-02-12 13:35:58,516] INFO [ThrottledChannelReaper-Fetch]: Stopped (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2019-02-12 13:35:58,516] INFO [ThrottledChannelReaper-Fetch]: Shutdown completed (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2019-02-12 13:35:58,518] INFO [ThrottledChannelReaper-Produce]: Shutting down (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2019-02-12 13:35:58,577] INFO [ThrottledChannelReaper-Produce]: Stopped (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2019-02-12 13:35:58,577] INFO [ThrottledChannelReaper-Produce]: Shutdown completed (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2019-02-12 13:35:58,579] INFO [ThrottledChannelReaper-Request]: Shutting down (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2019-02-12 13:35:59,579] INFO [ThrottledChannelReaper-Request]: Stopped (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2019-02-12 13:35:59,579] INFO [ThrottledChannelReaper-Request]: Shutdown completed (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2019-02-12 13:35:59,587] INFO [SocketServer brokerId=0] Shutting down socket server (kafka.network.SocketServer)
[2019-02-12 13:35:59,664] INFO [SocketServer brokerId=0] Shutdown completed (kafka.network.SocketServer)
[2019-02-12 13:35:59,668] INFO [KafkaServer id=0] shut down completed (kafka.server.KafkaServer)
